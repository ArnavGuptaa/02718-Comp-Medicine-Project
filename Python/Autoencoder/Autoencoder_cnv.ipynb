{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type = Ignore\n",
    "#Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from AEModel import AE, MyDataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "cnv_df= pd.read_csv(\"../../data/master_cnv_df.csv\",encoding = \"UTF-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OR4F5</th>\n",
       "      <th>OR4F29</th>\n",
       "      <th>OR4F16</th>\n",
       "      <th>SAMD11</th>\n",
       "      <th>NOC2L</th>\n",
       "      <th>KLHL17</th>\n",
       "      <th>PLEKHN1</th>\n",
       "      <th>PERM1</th>\n",
       "      <th>HES4</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>...</th>\n",
       "      <th>PRY</th>\n",
       "      <th>BPY2</th>\n",
       "      <th>DAZ1</th>\n",
       "      <th>DAZ2</th>\n",
       "      <th>CDY1B</th>\n",
       "      <th>BPY2B</th>\n",
       "      <th>DAZ3</th>\n",
       "      <th>DAZ4</th>\n",
       "      <th>BPY2C</th>\n",
       "      <th>CDY1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OR4F5  OR4F29  OR4F16  SAMD11  NOC2L  KLHL17  PLEKHN1  PERM1  HES4  ISG15  \\\n",
       "0    3.0     3.0     3.0     3.0    3.0     3.0      3.0    3.0   3.0    3.0   \n",
       "1    2.0     2.0     2.0     2.0    2.0     2.0      2.0    2.0   2.0    2.0   \n",
       "2    3.0     3.0     3.0     3.0    3.0     3.0      3.0    3.0   3.0    3.0   \n",
       "3    2.0     2.0     2.0     2.0    2.0     2.0      2.0    2.0   2.0    2.0   \n",
       "4    2.0     2.0     2.0     2.0    2.0     2.0      2.0    2.0   2.0    2.0   \n",
       "\n",
       "   ...  PRY  BPY2  DAZ1  DAZ2  CDY1B  BPY2B  DAZ3  DAZ4  BPY2C  CDY1  \n",
       "0  ...  1.0   1.0   1.0   1.0    1.0    1.0   1.0   1.0    1.0   1.0  \n",
       "1  ...  0.0   0.0   0.0   0.0    0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "2  ...  2.0   2.0   2.0   2.0    2.0    2.0   2.0   2.0    2.0   2.0  \n",
       "3  ...  0.0   0.0   0.0   0.0    0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "4  ...  0.0   0.0   0.0   0.0    0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 19148 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(gene_df.iloc[:,1:].shape[1])\n",
    "cnv_df.iloc[:,1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is used to load the dataset for training\n",
    "pd_train_dataset, pd_test_dataset = train_test_split(cnv_df.iloc[:,1:], test_size=0.2)\n",
    "\n",
    "X_train_sc = StandardScaler().fit_transform(pd_train_dataset)\n",
    "\n",
    "X_test_sc = StandardScaler().fit_transform(pd_test_dataset)\n",
    "\n",
    "X_full_sc = StandardScaler().fit_transform(cnv_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MyDataset(X_train_sc)\n",
    "data_test = MyDataset(X_test_sc)\n",
    "data_full = MyDataset(X_full_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(data_train, batch_size=50, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape = len(X_train_sc[0])).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/700, train_loss = 1.186182\n",
      "epoch : 1/700, val_loss = 1.443389\n",
      "epoch : 2/700, train_loss = 0.725171\n",
      "epoch : 2/700, val_loss = 0.909317\n",
      "epoch : 3/700, train_loss = 0.634373\n",
      "epoch : 3/700, val_loss = 0.617501\n",
      "epoch : 4/700, train_loss = 0.592686\n",
      "epoch : 4/700, val_loss = 0.539665\n",
      "epoch : 5/700, train_loss = 0.562802\n",
      "epoch : 5/700, val_loss = 0.519412\n",
      "epoch : 6/700, train_loss = 0.537065\n",
      "epoch : 6/700, val_loss = 0.503184\n",
      "epoch : 7/700, train_loss = 0.512035\n",
      "epoch : 7/700, val_loss = 0.489772\n",
      "epoch : 8/700, train_loss = 0.488999\n",
      "epoch : 8/700, val_loss = 0.474468\n",
      "epoch : 9/700, train_loss = 0.467107\n",
      "epoch : 9/700, val_loss = 0.458449\n",
      "epoch : 10/700, train_loss = 0.447448\n",
      "epoch : 10/700, val_loss = 0.444890\n",
      "epoch : 11/700, train_loss = 0.429307\n",
      "epoch : 11/700, val_loss = 0.431447\n",
      "epoch : 12/700, train_loss = 0.412091\n",
      "epoch : 12/700, val_loss = 0.417985\n",
      "epoch : 13/700, train_loss = 0.398531\n",
      "epoch : 13/700, val_loss = 0.412797\n",
      "epoch : 14/700, train_loss = 0.386821\n",
      "epoch : 14/700, val_loss = 0.404994\n",
      "epoch : 15/700, train_loss = 0.377828\n",
      "epoch : 15/700, val_loss = 0.400453\n",
      "epoch : 16/700, train_loss = 0.367690\n",
      "epoch : 16/700, val_loss = 0.387959\n",
      "epoch : 17/700, train_loss = 0.359472\n",
      "epoch : 17/700, val_loss = 0.378468\n",
      "epoch : 18/700, train_loss = 0.350929\n",
      "epoch : 18/700, val_loss = 0.379895\n",
      "epoch : 19/700, train_loss = 0.343640\n",
      "epoch : 19/700, val_loss = 0.365526\n",
      "epoch : 20/700, train_loss = 0.335317\n",
      "epoch : 20/700, val_loss = 0.363454\n",
      "epoch : 21/700, train_loss = 0.327939\n",
      "epoch : 21/700, val_loss = 0.353740\n",
      "epoch : 22/700, train_loss = 0.321133\n",
      "epoch : 22/700, val_loss = 0.352289\n",
      "epoch : 23/700, train_loss = 0.316190\n",
      "epoch : 23/700, val_loss = 0.356004\n",
      "epoch : 24/700, train_loss = 0.310811\n",
      "epoch : 24/700, val_loss = 0.347868\n",
      "epoch : 25/700, train_loss = 0.305597\n",
      "epoch : 25/700, val_loss = 0.341314\n",
      "epoch : 26/700, train_loss = 0.299702\n",
      "epoch : 26/700, val_loss = 0.339050\n",
      "epoch : 27/700, train_loss = 0.296843\n",
      "epoch : 27/700, val_loss = 0.337013\n",
      "epoch : 28/700, train_loss = 0.292522\n",
      "epoch : 28/700, val_loss = 0.331041\n",
      "epoch : 29/700, train_loss = 0.289364\n",
      "epoch : 29/700, val_loss = 0.334580\n",
      "epoch : 30/700, train_loss = 0.285740\n",
      "epoch : 30/700, val_loss = 0.337473\n",
      "epoch : 31/700, train_loss = 0.284020\n",
      "epoch : 31/700, val_loss = 0.328565\n",
      "epoch : 32/700, train_loss = 0.279203\n",
      "epoch : 32/700, val_loss = 0.334206\n",
      "epoch : 33/700, train_loss = 0.276977\n",
      "epoch : 33/700, val_loss = 0.331511\n",
      "epoch : 34/700, train_loss = 0.273141\n",
      "epoch : 34/700, val_loss = 0.329535\n",
      "epoch : 35/700, train_loss = 0.270691\n",
      "epoch : 35/700, val_loss = 0.327297\n",
      "epoch : 36/700, train_loss = 0.267589\n",
      "epoch : 36/700, val_loss = 0.323413\n",
      "epoch : 37/700, train_loss = 0.263744\n",
      "epoch : 37/700, val_loss = 0.324171\n",
      "epoch : 38/700, train_loss = 0.260562\n",
      "epoch : 38/700, val_loss = 0.316189\n",
      "epoch : 39/700, train_loss = 0.256797\n",
      "epoch : 39/700, val_loss = 0.315880\n",
      "epoch : 40/700, train_loss = 0.253816\n",
      "epoch : 40/700, val_loss = 0.311759\n",
      "epoch : 41/700, train_loss = 0.250352\n",
      "epoch : 41/700, val_loss = 0.315479\n",
      "epoch : 42/700, train_loss = 0.248241\n",
      "epoch : 42/700, val_loss = 0.309087\n",
      "epoch : 43/700, train_loss = 0.245370\n",
      "epoch : 43/700, val_loss = 0.311706\n",
      "epoch : 44/700, train_loss = 0.243679\n",
      "epoch : 44/700, val_loss = 0.313414\n",
      "epoch : 45/700, train_loss = 0.241422\n",
      "epoch : 45/700, val_loss = 0.312093\n",
      "epoch : 46/700, train_loss = 0.239988\n",
      "epoch : 46/700, val_loss = 0.309275\n",
      "epoch : 47/700, train_loss = 0.238483\n",
      "epoch : 47/700, val_loss = 0.309712\n",
      "epoch : 48/700, train_loss = 0.238922\n",
      "epoch : 48/700, val_loss = 0.313789\n",
      "epoch : 49/700, train_loss = 0.240216\n",
      "epoch : 49/700, val_loss = 0.309529\n",
      "epoch : 50/700, train_loss = 0.238384\n",
      "epoch : 50/700, val_loss = 0.308726\n",
      "epoch : 51/700, train_loss = 0.236548\n",
      "epoch : 51/700, val_loss = 0.306457\n",
      "epoch : 52/700, train_loss = 0.233239\n",
      "epoch : 52/700, val_loss = 0.307827\n",
      "epoch : 53/700, train_loss = 0.231767\n",
      "epoch : 53/700, val_loss = 0.308237\n",
      "epoch : 54/700, train_loss = 0.229846\n",
      "epoch : 54/700, val_loss = 0.303727\n",
      "epoch : 55/700, train_loss = 0.226978\n",
      "epoch : 55/700, val_loss = 0.307448\n",
      "epoch : 56/700, train_loss = 0.225322\n",
      "epoch : 56/700, val_loss = 0.301069\n",
      "epoch : 57/700, train_loss = 0.223090\n",
      "epoch : 57/700, val_loss = 0.299234\n",
      "epoch : 58/700, train_loss = 0.220891\n",
      "epoch : 58/700, val_loss = 0.300684\n",
      "epoch : 59/700, train_loss = 0.219182\n",
      "epoch : 59/700, val_loss = 0.301763\n",
      "epoch : 60/700, train_loss = 0.217302\n",
      "epoch : 60/700, val_loss = 0.305301\n",
      "epoch : 61/700, train_loss = 0.216372\n",
      "epoch : 61/700, val_loss = 0.306378\n",
      "epoch : 62/700, train_loss = 0.215182\n",
      "epoch : 62/700, val_loss = 0.303092\n",
      "epoch : 63/700, train_loss = 0.213503\n",
      "epoch : 63/700, val_loss = 0.300243\n",
      "epoch : 64/700, train_loss = 0.212926\n",
      "epoch : 64/700, val_loss = 0.297190\n",
      "epoch : 65/700, train_loss = 0.211832\n",
      "epoch : 65/700, val_loss = 0.302436\n",
      "epoch : 66/700, train_loss = 0.211229\n",
      "epoch : 66/700, val_loss = 0.305604\n",
      "epoch : 67/700, train_loss = 0.210352\n",
      "epoch : 67/700, val_loss = 0.299323\n",
      "epoch : 68/700, train_loss = 0.209169\n",
      "epoch : 68/700, val_loss = 0.303076\n",
      "epoch : 69/700, train_loss = 0.207628\n",
      "epoch : 69/700, val_loss = 0.295662\n",
      "epoch : 70/700, train_loss = 0.205415\n",
      "epoch : 70/700, val_loss = 0.296519\n",
      "epoch : 71/700, train_loss = 0.204205\n",
      "epoch : 71/700, val_loss = 0.297817\n",
      "epoch : 72/700, train_loss = 0.201939\n",
      "epoch : 72/700, val_loss = 0.295506\n",
      "epoch : 73/700, train_loss = 0.201668\n",
      "epoch : 73/700, val_loss = 0.298523\n",
      "epoch : 74/700, train_loss = 0.202059\n",
      "epoch : 74/700, val_loss = 0.296539\n",
      "epoch : 75/700, train_loss = 0.200978\n",
      "epoch : 75/700, val_loss = 0.296325\n",
      "epoch : 76/700, train_loss = 0.199479\n",
      "epoch : 76/700, val_loss = 0.292567\n",
      "epoch : 77/700, train_loss = 0.198597\n",
      "epoch : 77/700, val_loss = 0.292757\n",
      "epoch : 78/700, train_loss = 0.195653\n",
      "epoch : 78/700, val_loss = 0.296429\n",
      "epoch : 79/700, train_loss = 0.194489\n",
      "epoch : 79/700, val_loss = 0.291002\n",
      "epoch : 80/700, train_loss = 0.192210\n",
      "epoch : 80/700, val_loss = 0.297396\n",
      "epoch : 81/700, train_loss = 0.191222\n",
      "epoch : 81/700, val_loss = 0.291887\n",
      "epoch : 82/700, train_loss = 0.189648\n",
      "epoch : 82/700, val_loss = 0.290778\n",
      "epoch : 83/700, train_loss = 0.188202\n",
      "epoch : 83/700, val_loss = 0.293510\n",
      "epoch : 84/700, train_loss = 0.188459\n",
      "epoch : 84/700, val_loss = 0.293788\n",
      "epoch : 85/700, train_loss = 0.186681\n",
      "epoch : 85/700, val_loss = 0.299211\n",
      "epoch : 86/700, train_loss = 0.185835\n",
      "epoch : 86/700, val_loss = 0.296715\n",
      "epoch : 87/700, train_loss = 0.184384\n",
      "epoch : 87/700, val_loss = 0.298818\n",
      "epoch : 88/700, train_loss = 0.184008\n",
      "epoch : 88/700, val_loss = 0.300005\n",
      "epoch : 89/700, train_loss = 0.184882\n",
      "epoch : 89/700, val_loss = 0.295021\n",
      "epoch : 90/700, train_loss = 0.184473\n",
      "epoch : 90/700, val_loss = 0.297683\n",
      "epoch : 91/700, train_loss = 0.182673\n",
      "epoch : 91/700, val_loss = 0.297234\n",
      "epoch : 92/700, train_loss = 0.182050\n",
      "epoch : 92/700, val_loss = 0.295879\n",
      "epoch : 93/700, train_loss = 0.180575\n",
      "epoch : 93/700, val_loss = 0.296055\n",
      "epoch : 94/700, train_loss = 0.179814\n",
      "epoch : 94/700, val_loss = 0.296295\n",
      "epoch : 95/700, train_loss = 0.179624\n",
      "epoch : 95/700, val_loss = 0.304945\n",
      "epoch : 96/700, train_loss = 0.180068\n",
      "epoch : 96/700, val_loss = 0.313125\n",
      "epoch : 97/700, train_loss = 0.180578\n",
      "epoch : 97/700, val_loss = 0.351137\n",
      "epoch : 98/700, train_loss = 0.182392\n",
      "epoch : 98/700, val_loss = 0.305353\n",
      "epoch : 99/700, train_loss = 0.182106\n",
      "epoch : 99/700, val_loss = 0.300822\n",
      "epoch : 100/700, train_loss = 0.181292\n",
      "epoch : 100/700, val_loss = 0.296984\n",
      "epoch : 101/700, train_loss = 0.179552\n",
      "epoch : 101/700, val_loss = 0.292405\n",
      "epoch : 102/700, train_loss = 0.178778\n",
      "epoch : 102/700, val_loss = 0.297751\n",
      "epoch : 103/700, train_loss = 0.178704\n",
      "epoch : 103/700, val_loss = 0.290597\n",
      "epoch : 104/700, train_loss = 0.176966\n",
      "epoch : 104/700, val_loss = 0.295535\n",
      "epoch : 105/700, train_loss = 0.176078\n",
      "epoch : 105/700, val_loss = 0.290720\n",
      "epoch : 106/700, train_loss = 0.173857\n",
      "epoch : 106/700, val_loss = 0.287143\n",
      "epoch : 107/700, train_loss = 0.171427\n",
      "epoch : 107/700, val_loss = 0.290091\n",
      "epoch : 108/700, train_loss = 0.169388\n",
      "epoch : 108/700, val_loss = 0.292034\n",
      "epoch : 109/700, train_loss = 0.168917\n",
      "epoch : 109/700, val_loss = 0.289935\n",
      "epoch : 110/700, train_loss = 0.168925\n",
      "epoch : 110/700, val_loss = 0.287308\n",
      "epoch : 111/700, train_loss = 0.168934\n",
      "epoch : 111/700, val_loss = 0.292570\n",
      "epoch : 112/700, train_loss = 0.168747\n",
      "epoch : 112/700, val_loss = 0.300870\n",
      "epoch : 113/700, train_loss = 0.168973\n",
      "epoch : 113/700, val_loss = 0.295214\n",
      "epoch : 114/700, train_loss = 0.168151\n",
      "epoch : 114/700, val_loss = 0.290673\n",
      "epoch : 115/700, train_loss = 0.166351\n",
      "epoch : 115/700, val_loss = 0.287380\n",
      "epoch : 116/700, train_loss = 0.164782\n",
      "epoch : 116/700, val_loss = 0.286247\n",
      "epoch : 117/700, train_loss = 0.169192\n",
      "epoch : 117/700, val_loss = 0.316449\n",
      "epoch : 118/700, train_loss = 0.188359\n",
      "epoch : 118/700, val_loss = 0.312831\n",
      "epoch : 119/700, train_loss = 0.187949\n",
      "epoch : 119/700, val_loss = 0.297206\n",
      "epoch : 120/700, train_loss = 0.176385\n",
      "epoch : 120/700, val_loss = 0.281635\n",
      "epoch : 121/700, train_loss = 0.168835\n",
      "epoch : 121/700, val_loss = 0.281527\n",
      "epoch : 122/700, train_loss = 0.168776\n",
      "epoch : 122/700, val_loss = 0.282295\n",
      "epoch : 123/700, train_loss = 0.165590\n",
      "epoch : 123/700, val_loss = 0.283283\n",
      "epoch : 124/700, train_loss = 0.161918\n",
      "epoch : 124/700, val_loss = 0.281616\n",
      "epoch : 125/700, train_loss = 0.160896\n",
      "epoch : 125/700, val_loss = 0.283911\n",
      "epoch : 126/700, train_loss = 0.159504\n",
      "epoch : 126/700, val_loss = 0.284545\n",
      "epoch : 127/700, train_loss = 0.158132\n",
      "epoch : 127/700, val_loss = 0.284334\n",
      "epoch : 128/700, train_loss = 0.157213\n",
      "epoch : 128/700, val_loss = 0.279940\n",
      "epoch : 129/700, train_loss = 0.156052\n",
      "epoch : 129/700, val_loss = 0.282147\n",
      "epoch : 130/700, train_loss = 0.155421\n",
      "epoch : 130/700, val_loss = 0.283936\n",
      "epoch : 131/700, train_loss = 0.154716\n",
      "epoch : 131/700, val_loss = 0.292083\n",
      "epoch : 132/700, train_loss = 0.155005\n",
      "epoch : 132/700, val_loss = 0.289242\n",
      "epoch : 133/700, train_loss = 0.154794\n",
      "epoch : 133/700, val_loss = 0.285907\n",
      "epoch : 134/700, train_loss = 0.154285\n",
      "epoch : 134/700, val_loss = 0.288351\n",
      "epoch : 135/700, train_loss = 0.154379\n",
      "epoch : 135/700, val_loss = 0.286476\n",
      "epoch : 136/700, train_loss = 0.154740\n",
      "epoch : 136/700, val_loss = 0.284708\n",
      "epoch : 137/700, train_loss = 0.153977\n",
      "epoch : 137/700, val_loss = 0.288297\n",
      "epoch : 138/700, train_loss = 0.153807\n",
      "epoch : 138/700, val_loss = 0.285800\n",
      "epoch : 139/700, train_loss = 0.152867\n",
      "epoch : 139/700, val_loss = 0.285316\n",
      "epoch : 140/700, train_loss = 0.151640\n",
      "epoch : 140/700, val_loss = 0.287720\n",
      "epoch : 141/700, train_loss = 0.152107\n",
      "epoch : 141/700, val_loss = 0.284068\n",
      "epoch : 142/700, train_loss = 0.151301\n",
      "epoch : 142/700, val_loss = 0.287324\n",
      "epoch : 143/700, train_loss = 0.150699\n",
      "epoch : 143/700, val_loss = 0.285880\n",
      "epoch : 144/700, train_loss = 0.149999\n",
      "epoch : 144/700, val_loss = 0.285080\n",
      "epoch : 145/700, train_loss = 0.149670\n",
      "epoch : 145/700, val_loss = 0.285571\n",
      "epoch : 146/700, train_loss = 0.149280\n",
      "epoch : 146/700, val_loss = 0.284432\n",
      "epoch : 147/700, train_loss = 0.149296\n",
      "epoch : 147/700, val_loss = 0.280840\n",
      "epoch : 148/700, train_loss = 0.148383\n",
      "epoch : 148/700, val_loss = 0.281375\n",
      "epoch : 149/700, train_loss = 0.147207\n",
      "epoch : 149/700, val_loss = 0.288031\n",
      "epoch : 150/700, train_loss = 0.146579\n",
      "epoch : 150/700, val_loss = 0.285041\n",
      "epoch : 151/700, train_loss = 0.146220\n",
      "epoch : 151/700, val_loss = 0.282935\n",
      "epoch : 152/700, train_loss = 0.146065\n",
      "epoch : 152/700, val_loss = 0.281219\n",
      "epoch : 153/700, train_loss = 0.146309\n",
      "epoch : 153/700, val_loss = 0.285895\n",
      "epoch : 154/700, train_loss = 0.146185\n",
      "epoch : 154/700, val_loss = 0.286904\n",
      "epoch : 155/700, train_loss = 0.146344\n",
      "epoch : 155/700, val_loss = 0.287081\n",
      "epoch : 156/700, train_loss = 0.146760\n",
      "epoch : 156/700, val_loss = 0.289748\n",
      "epoch : 157/700, train_loss = 0.146913\n",
      "epoch : 157/700, val_loss = 0.284635\n",
      "epoch : 158/700, train_loss = 0.146845\n",
      "epoch : 158/700, val_loss = 0.286877\n",
      "epoch : 159/700, train_loss = 0.146359\n",
      "epoch : 159/700, val_loss = 0.292086\n",
      "epoch : 160/700, train_loss = 0.146592\n",
      "epoch : 160/700, val_loss = 0.294093\n",
      "epoch : 161/700, train_loss = 0.146027\n",
      "epoch : 161/700, val_loss = 0.294987\n",
      "epoch : 162/700, train_loss = 0.145515\n",
      "epoch : 162/700, val_loss = 0.291180\n",
      "epoch : 163/700, train_loss = 0.145193\n",
      "epoch : 163/700, val_loss = 0.285067\n",
      "epoch : 164/700, train_loss = 0.145045\n",
      "epoch : 164/700, val_loss = 0.287615\n",
      "epoch : 165/700, train_loss = 0.145070\n",
      "epoch : 165/700, val_loss = 0.286978\n",
      "epoch : 166/700, train_loss = 0.145435\n",
      "epoch : 166/700, val_loss = 0.287552\n",
      "epoch : 167/700, train_loss = 0.147998\n",
      "epoch : 167/700, val_loss = 0.290254\n",
      "epoch : 168/700, train_loss = 0.145318\n",
      "epoch : 168/700, val_loss = 0.281404\n",
      "epoch : 169/700, train_loss = 0.143198\n",
      "epoch : 169/700, val_loss = 0.280162\n",
      "epoch : 170/700, train_loss = 0.141823\n",
      "epoch : 170/700, val_loss = 0.281417\n",
      "epoch : 171/700, train_loss = 0.140925\n",
      "epoch : 171/700, val_loss = 0.283453\n",
      "epoch : 172/700, train_loss = 0.139542\n",
      "epoch : 172/700, val_loss = 0.298138\n",
      "epoch : 173/700, train_loss = 0.160161\n",
      "epoch : 173/700, val_loss = 0.297634\n",
      "epoch : 174/700, train_loss = 0.154959\n",
      "epoch : 174/700, val_loss = 0.282982\n",
      "epoch : 175/700, train_loss = 0.148623\n",
      "epoch : 175/700, val_loss = 0.283357\n",
      "epoch : 176/700, train_loss = 0.147434\n",
      "epoch : 176/700, val_loss = 0.282761\n",
      "epoch : 177/700, train_loss = 0.147466\n",
      "epoch : 177/700, val_loss = 0.280136\n",
      "epoch : 178/700, train_loss = 0.144302\n",
      "epoch : 178/700, val_loss = 0.277280\n",
      "epoch : 179/700, train_loss = 0.141298\n",
      "epoch : 179/700, val_loss = 0.277310\n",
      "epoch : 180/700, train_loss = 0.140722\n",
      "epoch : 180/700, val_loss = 0.277945\n",
      "epoch : 181/700, train_loss = 0.140680\n",
      "epoch : 181/700, val_loss = 0.281647\n",
      "epoch : 182/700, train_loss = 0.141016\n",
      "epoch : 182/700, val_loss = 0.284348\n",
      "epoch : 183/700, train_loss = 0.140546\n",
      "epoch : 183/700, val_loss = 0.290296\n",
      "epoch : 184/700, train_loss = 0.139740\n",
      "epoch : 184/700, val_loss = 0.285260\n",
      "epoch : 185/700, train_loss = 0.140086\n",
      "epoch : 185/700, val_loss = 0.282459\n",
      "epoch : 186/700, train_loss = 0.139330\n",
      "epoch : 186/700, val_loss = 0.278808\n",
      "epoch : 187/700, train_loss = 0.138356\n",
      "epoch : 187/700, val_loss = 0.276024\n",
      "epoch : 188/700, train_loss = 0.137132\n",
      "epoch : 188/700, val_loss = 0.278828\n",
      "epoch : 189/700, train_loss = 0.136401\n",
      "epoch : 189/700, val_loss = 0.286231\n",
      "epoch : 190/700, train_loss = 0.135735\n",
      "epoch : 190/700, val_loss = 0.283061\n",
      "epoch : 191/700, train_loss = 0.136146\n",
      "epoch : 191/700, val_loss = 0.282130\n",
      "epoch : 192/700, train_loss = 0.135677\n",
      "epoch : 192/700, val_loss = 0.287213\n",
      "epoch : 193/700, train_loss = 0.138601\n",
      "epoch : 193/700, val_loss = 0.285676\n",
      "epoch : 194/700, train_loss = 0.141165\n",
      "epoch : 194/700, val_loss = 0.283784\n",
      "epoch : 195/700, train_loss = 0.138527\n",
      "epoch : 195/700, val_loss = 0.286220\n",
      "epoch : 196/700, train_loss = 0.137310\n",
      "epoch : 196/700, val_loss = 0.293291\n",
      "epoch : 197/700, train_loss = 0.136918\n",
      "epoch : 197/700, val_loss = 0.291651\n",
      "epoch : 198/700, train_loss = 0.136934\n",
      "epoch : 198/700, val_loss = 0.288662\n",
      "epoch : 199/700, train_loss = 0.136659\n",
      "epoch : 199/700, val_loss = 0.283525\n",
      "epoch : 200/700, train_loss = 0.136885\n",
      "epoch : 200/700, val_loss = 0.283731\n",
      "epoch : 201/700, train_loss = 0.135959\n",
      "epoch : 201/700, val_loss = 0.280971\n",
      "epoch : 202/700, train_loss = 0.135693\n",
      "epoch : 202/700, val_loss = 0.281041\n",
      "epoch : 203/700, train_loss = 0.135298\n",
      "epoch : 203/700, val_loss = 0.280663\n",
      "epoch : 204/700, train_loss = 0.135150\n",
      "epoch : 204/700, val_loss = 0.280195\n",
      "epoch : 205/700, train_loss = 0.134090\n",
      "epoch : 205/700, val_loss = 0.287611\n",
      "epoch : 206/700, train_loss = 0.134044\n",
      "epoch : 206/700, val_loss = 0.289494\n",
      "epoch : 207/700, train_loss = 0.134020\n",
      "epoch : 207/700, val_loss = 0.286591\n",
      "epoch : 208/700, train_loss = 0.133091\n",
      "epoch : 208/700, val_loss = 0.281323\n",
      "epoch : 209/700, train_loss = 0.133468\n",
      "epoch : 209/700, val_loss = 0.279584\n",
      "epoch : 210/700, train_loss = 0.133812\n",
      "epoch : 210/700, val_loss = 0.285797\n",
      "epoch : 211/700, train_loss = 0.135515\n",
      "epoch : 211/700, val_loss = 0.288675\n",
      "epoch : 212/700, train_loss = 0.135366\n",
      "epoch : 212/700, val_loss = 0.281328\n",
      "epoch : 213/700, train_loss = 0.134468\n",
      "epoch : 213/700, val_loss = 0.284156\n",
      "epoch : 214/700, train_loss = 0.134196\n",
      "epoch : 214/700, val_loss = 0.288019\n",
      "epoch : 215/700, train_loss = 0.133651\n",
      "epoch : 215/700, val_loss = 0.290117\n",
      "epoch : 216/700, train_loss = 0.132786\n",
      "epoch : 216/700, val_loss = 0.286323\n",
      "epoch : 217/700, train_loss = 0.133025\n",
      "epoch : 217/700, val_loss = 0.287810\n",
      "epoch : 218/700, train_loss = 0.133834\n",
      "epoch : 218/700, val_loss = 0.289633\n",
      "epoch : 219/700, train_loss = 0.134244\n",
      "epoch : 219/700, val_loss = 0.290913\n",
      "epoch : 220/700, train_loss = 0.133961\n",
      "epoch : 220/700, val_loss = 0.291502\n",
      "epoch : 221/700, train_loss = 0.134367\n",
      "epoch : 221/700, val_loss = 0.289661\n",
      "epoch : 222/700, train_loss = 0.134339\n",
      "epoch : 222/700, val_loss = 0.289034\n",
      "epoch : 223/700, train_loss = 0.134335\n",
      "epoch : 223/700, val_loss = 0.284235\n",
      "epoch : 224/700, train_loss = 0.134563\n",
      "epoch : 224/700, val_loss = 0.286135\n",
      "epoch : 225/700, train_loss = 0.134266\n",
      "epoch : 225/700, val_loss = 0.285504\n",
      "epoch : 226/700, train_loss = 0.133281\n",
      "epoch : 226/700, val_loss = 0.285258\n",
      "epoch : 227/700, train_loss = 0.132347\n",
      "epoch : 227/700, val_loss = 0.284454\n",
      "epoch : 228/700, train_loss = 0.131358\n",
      "epoch : 228/700, val_loss = 0.283002\n",
      "epoch : 229/700, train_loss = 0.131331\n",
      "epoch : 229/700, val_loss = 0.279478\n",
      "epoch : 230/700, train_loss = 0.130867\n",
      "epoch : 230/700, val_loss = 0.282260\n",
      "epoch : 231/700, train_loss = 0.130594\n",
      "epoch : 231/700, val_loss = 0.284357\n",
      "epoch : 232/700, train_loss = 0.130163\n",
      "epoch : 232/700, val_loss = 0.289825\n",
      "epoch : 233/700, train_loss = 0.129833\n",
      "epoch : 233/700, val_loss = 0.287221\n",
      "epoch : 234/700, train_loss = 0.129849\n",
      "epoch : 234/700, val_loss = 0.287349\n",
      "epoch : 235/700, train_loss = 0.129209\n",
      "epoch : 235/700, val_loss = 0.286333\n",
      "epoch : 236/700, train_loss = 0.129047\n",
      "epoch : 236/700, val_loss = 0.291045\n",
      "epoch : 237/700, train_loss = 0.129104\n",
      "epoch : 237/700, val_loss = 0.290147\n",
      "epoch : 238/700, train_loss = 0.129813\n",
      "epoch : 238/700, val_loss = 0.287896\n",
      "epoch : 239/700, train_loss = 0.129815\n",
      "epoch : 239/700, val_loss = 0.285603\n",
      "epoch : 240/700, train_loss = 0.130427\n",
      "epoch : 240/700, val_loss = 0.285336\n",
      "epoch : 241/700, train_loss = 0.130008\n",
      "epoch : 241/700, val_loss = 0.284588\n",
      "epoch : 242/700, train_loss = 0.129834\n",
      "epoch : 242/700, val_loss = 0.283948\n",
      "epoch : 243/700, train_loss = 0.129167\n",
      "epoch : 243/700, val_loss = 0.284596\n",
      "epoch : 244/700, train_loss = 0.128514\n",
      "epoch : 244/700, val_loss = 0.289069\n",
      "epoch : 245/700, train_loss = 0.128536\n",
      "epoch : 245/700, val_loss = 0.285020\n",
      "epoch : 246/700, train_loss = 0.127620\n",
      "epoch : 246/700, val_loss = 0.284032\n",
      "epoch : 247/700, train_loss = 0.128049\n",
      "epoch : 247/700, val_loss = 0.282200\n",
      "epoch : 248/700, train_loss = 0.128570\n",
      "epoch : 248/700, val_loss = 0.287230\n",
      "epoch : 249/700, train_loss = 0.128774\n",
      "epoch : 249/700, val_loss = 0.285530\n",
      "epoch : 250/700, train_loss = 0.128812\n",
      "epoch : 250/700, val_loss = 0.282137\n",
      "epoch : 251/700, train_loss = 0.129479\n",
      "epoch : 251/700, val_loss = 0.281031\n",
      "epoch : 252/700, train_loss = 0.129706\n",
      "epoch : 252/700, val_loss = 0.286903\n",
      "epoch : 253/700, train_loss = 0.129285\n",
      "epoch : 253/700, val_loss = 0.289577\n",
      "epoch : 254/700, train_loss = 0.128391\n",
      "epoch : 254/700, val_loss = 0.286865\n",
      "epoch : 255/700, train_loss = 0.128229\n",
      "epoch : 255/700, val_loss = 0.287351\n",
      "epoch : 256/700, train_loss = 0.127467\n",
      "epoch : 256/700, val_loss = 0.286111\n",
      "epoch : 257/700, train_loss = 0.126882\n",
      "epoch : 257/700, val_loss = 0.280717\n",
      "epoch : 258/700, train_loss = 0.126220\n",
      "epoch : 258/700, val_loss = 0.283030\n",
      "epoch : 259/700, train_loss = 0.126394\n",
      "epoch : 259/700, val_loss = 0.282060\n",
      "epoch : 260/700, train_loss = 0.126186\n",
      "epoch : 260/700, val_loss = 0.282659\n",
      "epoch : 261/700, train_loss = 0.125885\n",
      "epoch : 261/700, val_loss = 0.282513\n",
      "epoch : 262/700, train_loss = 0.125700\n",
      "epoch : 262/700, val_loss = 0.285033\n",
      "epoch : 263/700, train_loss = 0.126446\n",
      "epoch : 263/700, val_loss = 0.285093\n",
      "epoch : 264/700, train_loss = 0.125704\n",
      "epoch : 264/700, val_loss = 0.286971\n",
      "epoch : 265/700, train_loss = 0.126292\n",
      "epoch : 265/700, val_loss = 0.282183\n",
      "epoch : 266/700, train_loss = 0.125742\n",
      "epoch : 266/700, val_loss = 0.282801\n",
      "epoch : 267/700, train_loss = 0.125839\n",
      "epoch : 267/700, val_loss = 0.282273\n",
      "epoch : 268/700, train_loss = 0.125511\n",
      "epoch : 268/700, val_loss = 0.282489\n",
      "epoch : 269/700, train_loss = 0.126007\n",
      "epoch : 269/700, val_loss = 0.285309\n",
      "epoch : 270/700, train_loss = 0.126158\n",
      "epoch : 270/700, val_loss = 0.285506\n",
      "epoch : 271/700, train_loss = 0.127005\n",
      "epoch : 271/700, val_loss = 0.282316\n",
      "epoch : 272/700, train_loss = 0.126456\n",
      "epoch : 272/700, val_loss = 0.286984\n",
      "epoch : 273/700, train_loss = 0.126227\n",
      "epoch : 273/700, val_loss = 0.288238\n",
      "epoch : 274/700, train_loss = 0.125983\n",
      "epoch : 274/700, val_loss = 0.289944\n",
      "epoch : 275/700, train_loss = 0.125610\n",
      "epoch : 275/700, val_loss = 0.284613\n",
      "epoch : 276/700, train_loss = 0.125494\n",
      "epoch : 276/700, val_loss = 0.280479\n",
      "epoch : 277/700, train_loss = 0.126302\n",
      "epoch : 277/700, val_loss = 0.280730\n",
      "epoch : 278/700, train_loss = 0.125766\n",
      "epoch : 278/700, val_loss = 0.283742\n",
      "epoch : 279/700, train_loss = 0.125743\n",
      "epoch : 279/700, val_loss = 0.281129\n",
      "epoch : 280/700, train_loss = 0.124871\n",
      "epoch : 280/700, val_loss = 0.288091\n",
      "epoch : 281/700, train_loss = 0.125169\n",
      "epoch : 281/700, val_loss = 0.287025\n",
      "epoch : 282/700, train_loss = 0.125283\n",
      "epoch : 282/700, val_loss = 0.289070\n",
      "epoch : 283/700, train_loss = 0.125547\n",
      "epoch : 283/700, val_loss = 0.290240\n",
      "epoch : 284/700, train_loss = 0.126025\n",
      "epoch : 284/700, val_loss = 0.285649\n",
      "epoch : 285/700, train_loss = 0.125843\n",
      "epoch : 285/700, val_loss = 0.289322\n",
      "epoch : 286/700, train_loss = 0.126664\n",
      "epoch : 286/700, val_loss = 0.288709\n",
      "epoch : 287/700, train_loss = 0.126796\n",
      "epoch : 287/700, val_loss = 0.286414\n",
      "epoch : 288/700, train_loss = 0.127056\n",
      "epoch : 288/700, val_loss = 0.286051\n",
      "epoch : 289/700, train_loss = 0.127620\n",
      "epoch : 289/700, val_loss = 0.285752\n",
      "epoch : 290/700, train_loss = 0.127975\n",
      "epoch : 290/700, val_loss = 0.286049\n",
      "epoch : 291/700, train_loss = 0.128344\n",
      "epoch : 291/700, val_loss = 0.285032\n",
      "epoch : 292/700, train_loss = 0.128221\n",
      "epoch : 292/700, val_loss = 0.293285\n",
      "epoch : 293/700, train_loss = 0.128233\n",
      "epoch : 293/700, val_loss = 0.297350\n",
      "epoch : 294/700, train_loss = 0.128794\n",
      "epoch : 294/700, val_loss = 0.296007\n",
      "epoch : 295/700, train_loss = 0.131180\n",
      "epoch : 295/700, val_loss = 0.286258\n",
      "epoch : 296/700, train_loss = 0.131804\n",
      "epoch : 296/700, val_loss = 0.281382\n",
      "epoch : 297/700, train_loss = 0.154226\n",
      "epoch : 297/700, val_loss = 0.331288\n",
      "epoch : 298/700, train_loss = 0.165707\n",
      "epoch : 298/700, val_loss = 0.296019\n",
      "epoch : 299/700, train_loss = 0.174770\n",
      "epoch : 299/700, val_loss = 0.303877\n",
      "epoch : 300/700, train_loss = 0.200742\n",
      "epoch : 300/700, val_loss = 0.304902\n",
      "epoch : 301/700, train_loss = 0.190205\n",
      "epoch : 301/700, val_loss = 0.293035\n",
      "epoch : 302/700, train_loss = 0.166592\n",
      "epoch : 302/700, val_loss = 0.279630\n",
      "epoch : 303/700, train_loss = 0.150819\n",
      "epoch : 303/700, val_loss = 0.275703\n",
      "epoch : 304/700, train_loss = 0.142690\n",
      "epoch : 304/700, val_loss = 0.280608\n",
      "epoch : 305/700, train_loss = 0.137815\n",
      "epoch : 305/700, val_loss = 0.279612\n",
      "epoch : 306/700, train_loss = 0.134172\n",
      "epoch : 306/700, val_loss = 0.279223\n",
      "epoch : 307/700, train_loss = 0.131921\n",
      "epoch : 307/700, val_loss = 0.280091\n",
      "epoch : 308/700, train_loss = 0.130979\n",
      "epoch : 308/700, val_loss = 0.280043\n",
      "epoch : 309/700, train_loss = 0.130095\n",
      "epoch : 309/700, val_loss = 0.277276\n",
      "epoch : 310/700, train_loss = 0.129401\n",
      "epoch : 310/700, val_loss = 0.282301\n",
      "epoch : 311/700, train_loss = 0.129809\n",
      "epoch : 311/700, val_loss = 0.281470\n",
      "epoch : 312/700, train_loss = 0.129577\n",
      "epoch : 312/700, val_loss = 0.286720\n",
      "epoch : 313/700, train_loss = 0.128772\n",
      "epoch : 313/700, val_loss = 0.289282\n",
      "epoch : 314/700, train_loss = 0.128280\n",
      "epoch : 314/700, val_loss = 0.287185\n",
      "epoch : 315/700, train_loss = 0.130141\n",
      "epoch : 315/700, val_loss = 0.287815\n",
      "epoch : 316/700, train_loss = 0.132003\n",
      "epoch : 316/700, val_loss = 0.281720\n",
      "epoch : 317/700, train_loss = 0.134385\n",
      "epoch : 317/700, val_loss = 0.279481\n",
      "epoch : 318/700, train_loss = 0.130814\n",
      "epoch : 318/700, val_loss = 0.284001\n",
      "epoch : 319/700, train_loss = 0.128488\n",
      "epoch : 319/700, val_loss = 0.280945\n",
      "epoch : 320/700, train_loss = 0.126559\n",
      "epoch : 320/700, val_loss = 0.282274\n",
      "epoch : 321/700, train_loss = 0.125117\n",
      "epoch : 321/700, val_loss = 0.279205\n",
      "epoch : 322/700, train_loss = 0.124134\n",
      "epoch : 322/700, val_loss = 0.280282\n",
      "epoch : 323/700, train_loss = 0.123431\n",
      "epoch : 323/700, val_loss = 0.280977\n",
      "epoch : 324/700, train_loss = 0.123039\n",
      "epoch : 324/700, val_loss = 0.285648\n",
      "epoch : 325/700, train_loss = 0.122899\n",
      "epoch : 325/700, val_loss = 0.284512\n",
      "epoch : 326/700, train_loss = 0.122706\n",
      "epoch : 326/700, val_loss = 0.281200\n",
      "epoch : 327/700, train_loss = 0.122511\n",
      "epoch : 327/700, val_loss = 0.280635\n",
      "epoch : 328/700, train_loss = 0.122689\n",
      "epoch : 328/700, val_loss = 0.282128\n",
      "epoch : 329/700, train_loss = 0.122270\n",
      "epoch : 329/700, val_loss = 0.288759\n",
      "epoch : 330/700, train_loss = 0.123199\n",
      "epoch : 330/700, val_loss = 0.286263\n",
      "epoch : 331/700, train_loss = 0.123117\n",
      "epoch : 331/700, val_loss = 0.285847\n",
      "epoch : 332/700, train_loss = 0.123702\n",
      "epoch : 332/700, val_loss = 0.283768\n",
      "epoch : 333/700, train_loss = 0.140762\n",
      "epoch : 333/700, val_loss = 0.302088\n",
      "epoch : 334/700, train_loss = 0.156624\n",
      "epoch : 334/700, val_loss = 0.288148\n",
      "epoch : 335/700, train_loss = 0.142209\n",
      "epoch : 335/700, val_loss = 0.279240\n",
      "epoch : 336/700, train_loss = 0.133608\n",
      "epoch : 336/700, val_loss = 0.279283\n",
      "epoch : 337/700, train_loss = 0.129691\n",
      "epoch : 337/700, val_loss = 0.278708\n",
      "epoch : 338/700, train_loss = 0.127897\n",
      "epoch : 338/700, val_loss = 0.281896\n",
      "epoch : 339/700, train_loss = 0.126980\n",
      "epoch : 339/700, val_loss = 0.283197\n",
      "epoch : 340/700, train_loss = 0.125952\n",
      "epoch : 340/700, val_loss = 0.284426\n",
      "epoch : 341/700, train_loss = 0.125598\n",
      "epoch : 341/700, val_loss = 0.282249\n",
      "epoch : 342/700, train_loss = 0.124853\n",
      "epoch : 342/700, val_loss = 0.283656\n",
      "epoch : 343/700, train_loss = 0.124165\n",
      "epoch : 343/700, val_loss = 0.286394\n",
      "epoch : 344/700, train_loss = 0.123919\n",
      "epoch : 344/700, val_loss = 0.283213\n",
      "epoch : 345/700, train_loss = 0.123266\n",
      "epoch : 345/700, val_loss = 0.282138\n",
      "epoch : 346/700, train_loss = 0.122947\n",
      "epoch : 346/700, val_loss = 0.282187\n",
      "epoch : 347/700, train_loss = 0.122895\n",
      "epoch : 347/700, val_loss = 0.282524\n",
      "epoch : 348/700, train_loss = 0.122274\n",
      "epoch : 348/700, val_loss = 0.287967\n",
      "epoch : 349/700, train_loss = 0.122476\n",
      "epoch : 349/700, val_loss = 0.285303\n",
      "epoch : 350/700, train_loss = 0.121986\n",
      "epoch : 350/700, val_loss = 0.286966\n",
      "epoch : 351/700, train_loss = 0.121590\n",
      "epoch : 351/700, val_loss = 0.286917\n",
      "epoch : 352/700, train_loss = 0.121424\n",
      "epoch : 352/700, val_loss = 0.283983\n",
      "epoch : 353/700, train_loss = 0.121699\n",
      "epoch : 353/700, val_loss = 0.280915\n",
      "epoch : 354/700, train_loss = 0.121809\n",
      "epoch : 354/700, val_loss = 0.279823\n",
      "epoch : 355/700, train_loss = 0.121616\n",
      "epoch : 355/700, val_loss = 0.283140\n",
      "epoch : 356/700, train_loss = 0.121747\n",
      "epoch : 356/700, val_loss = 0.282990\n",
      "epoch : 357/700, train_loss = 0.121829\n",
      "epoch : 357/700, val_loss = 0.284421\n",
      "epoch : 358/700, train_loss = 0.122143\n",
      "epoch : 358/700, val_loss = 0.282150\n",
      "epoch : 359/700, train_loss = 0.121669\n",
      "epoch : 359/700, val_loss = 0.286198\n",
      "epoch : 360/700, train_loss = 0.121873\n",
      "epoch : 360/700, val_loss = 0.291824\n",
      "epoch : 361/700, train_loss = 0.122170\n",
      "epoch : 361/700, val_loss = 0.287192\n",
      "epoch : 362/700, train_loss = 0.121994\n",
      "epoch : 362/700, val_loss = 0.281780\n",
      "epoch : 363/700, train_loss = 0.121960\n",
      "epoch : 363/700, val_loss = 0.280307\n",
      "epoch : 364/700, train_loss = 0.121871\n",
      "epoch : 364/700, val_loss = 0.282260\n",
      "epoch : 365/700, train_loss = 0.121999\n",
      "epoch : 365/700, val_loss = 0.287014\n",
      "epoch : 366/700, train_loss = 0.121473\n",
      "epoch : 366/700, val_loss = 0.292680\n",
      "epoch : 367/700, train_loss = 0.121804\n",
      "epoch : 367/700, val_loss = 0.288267\n",
      "epoch : 368/700, train_loss = 0.122065\n",
      "epoch : 368/700, val_loss = 0.286523\n",
      "epoch : 369/700, train_loss = 0.122174\n",
      "epoch : 369/700, val_loss = 0.281739\n",
      "epoch : 370/700, train_loss = 0.122270\n",
      "epoch : 370/700, val_loss = 0.284808\n",
      "epoch : 371/700, train_loss = 0.121988\n",
      "epoch : 371/700, val_loss = 0.287770\n",
      "epoch : 372/700, train_loss = 0.122386\n",
      "epoch : 372/700, val_loss = 0.288061\n",
      "epoch : 373/700, train_loss = 0.124085\n",
      "epoch : 373/700, val_loss = 0.281329\n",
      "epoch : 374/700, train_loss = 0.123123\n",
      "epoch : 374/700, val_loss = 0.289359\n",
      "epoch : 375/700, train_loss = 0.123696\n",
      "epoch : 375/700, val_loss = 0.287045\n",
      "epoch : 376/700, train_loss = 0.123791\n",
      "epoch : 376/700, val_loss = 0.285813\n",
      "epoch : 377/700, train_loss = 0.121568\n",
      "epoch : 377/700, val_loss = 0.281578\n",
      "epoch : 378/700, train_loss = 0.123008\n",
      "epoch : 378/700, val_loss = 0.283564\n",
      "epoch : 379/700, train_loss = 0.121858\n",
      "epoch : 379/700, val_loss = 0.284353\n",
      "epoch : 380/700, train_loss = 0.120548\n",
      "epoch : 380/700, val_loss = 0.283712\n",
      "epoch : 381/700, train_loss = 0.119937\n",
      "epoch : 381/700, val_loss = 0.283631\n",
      "epoch : 382/700, train_loss = 0.119633\n",
      "epoch : 382/700, val_loss = 0.283723\n",
      "epoch : 383/700, train_loss = 0.122712\n",
      "epoch : 383/700, val_loss = 0.287324\n",
      "epoch : 384/700, train_loss = 0.145401\n",
      "epoch : 384/700, val_loss = 0.298737\n",
      "epoch : 385/700, train_loss = 0.164765\n",
      "epoch : 385/700, val_loss = 0.289270\n",
      "epoch : 386/700, train_loss = 0.147817\n",
      "epoch : 386/700, val_loss = 0.282226\n",
      "epoch : 387/700, train_loss = 0.134288\n",
      "epoch : 387/700, val_loss = 0.277563\n",
      "epoch : 388/700, train_loss = 0.127267\n",
      "epoch : 388/700, val_loss = 0.280699\n",
      "epoch : 389/700, train_loss = 0.123538\n",
      "epoch : 389/700, val_loss = 0.279187\n",
      "epoch : 390/700, train_loss = 0.124542\n",
      "epoch : 390/700, val_loss = 0.277265\n",
      "epoch : 391/700, train_loss = 0.122042\n",
      "epoch : 391/700, val_loss = 0.278443\n",
      "epoch : 392/700, train_loss = 0.120589\n",
      "epoch : 392/700, val_loss = 0.278995\n",
      "epoch : 393/700, train_loss = 0.119866\n",
      "epoch : 393/700, val_loss = 0.280121\n",
      "epoch : 394/700, train_loss = 0.126884\n",
      "epoch : 394/700, val_loss = 0.284045\n",
      "epoch : 395/700, train_loss = 0.123301\n",
      "epoch : 395/700, val_loss = 0.283646\n",
      "epoch : 396/700, train_loss = 0.126142\n",
      "epoch : 396/700, val_loss = 0.287062\n",
      "epoch : 397/700, train_loss = 0.137858\n",
      "epoch : 397/700, val_loss = 0.285034\n",
      "epoch : 398/700, train_loss = 0.131113\n",
      "epoch : 398/700, val_loss = 0.280171\n",
      "epoch : 399/700, train_loss = 0.124445\n",
      "epoch : 399/700, val_loss = 0.274834\n",
      "epoch : 400/700, train_loss = 0.121154\n",
      "epoch : 400/700, val_loss = 0.274939\n",
      "epoch : 401/700, train_loss = 0.119212\n",
      "epoch : 401/700, val_loss = 0.276293\n",
      "epoch : 402/700, train_loss = 0.118000\n",
      "epoch : 402/700, val_loss = 0.277432\n",
      "epoch : 403/700, train_loss = 0.117335\n",
      "epoch : 403/700, val_loss = 0.276764\n",
      "epoch : 404/700, train_loss = 0.116963\n",
      "epoch : 404/700, val_loss = 0.275870\n",
      "epoch : 405/700, train_loss = 0.116734\n",
      "epoch : 405/700, val_loss = 0.275956\n",
      "epoch : 406/700, train_loss = 0.116804\n",
      "epoch : 406/700, val_loss = 0.278611\n",
      "epoch : 407/700, train_loss = 0.117006\n",
      "epoch : 407/700, val_loss = 0.282041\n",
      "epoch : 408/700, train_loss = 0.117096\n",
      "epoch : 408/700, val_loss = 0.286291\n",
      "epoch : 409/700, train_loss = 0.117658\n",
      "epoch : 409/700, val_loss = 0.283296\n",
      "epoch : 410/700, train_loss = 0.117674\n",
      "epoch : 410/700, val_loss = 0.279664\n",
      "epoch : 411/700, train_loss = 0.117397\n",
      "epoch : 411/700, val_loss = 0.279014\n",
      "epoch : 412/700, train_loss = 0.117480\n",
      "epoch : 412/700, val_loss = 0.277729\n",
      "epoch : 413/700, train_loss = 0.117402\n",
      "epoch : 413/700, val_loss = 0.279910\n",
      "epoch : 414/700, train_loss = 0.117366\n",
      "epoch : 414/700, val_loss = 0.282933\n",
      "epoch : 415/700, train_loss = 0.117397\n",
      "epoch : 415/700, val_loss = 0.285246\n",
      "epoch : 416/700, train_loss = 0.117363\n",
      "epoch : 416/700, val_loss = 0.282642\n",
      "epoch : 417/700, train_loss = 0.117059\n",
      "epoch : 417/700, val_loss = 0.279838\n",
      "epoch : 418/700, train_loss = 0.117180\n",
      "epoch : 418/700, val_loss = 0.279533\n",
      "epoch : 419/700, train_loss = 0.116978\n",
      "epoch : 419/700, val_loss = 0.285047\n",
      "epoch : 420/700, train_loss = 0.117195\n",
      "epoch : 420/700, val_loss = 0.285229\n",
      "epoch : 421/700, train_loss = 0.117244\n",
      "epoch : 421/700, val_loss = 0.281900\n",
      "epoch : 422/700, train_loss = 0.116939\n",
      "epoch : 422/700, val_loss = 0.280103\n",
      "epoch : 423/700, train_loss = 0.116956\n",
      "epoch : 423/700, val_loss = 0.280061\n",
      "epoch : 424/700, train_loss = 0.116658\n",
      "epoch : 424/700, val_loss = 0.280062\n",
      "epoch : 425/700, train_loss = 0.116225\n",
      "epoch : 425/700, val_loss = 0.283790\n",
      "epoch : 426/700, train_loss = 0.116169\n",
      "epoch : 426/700, val_loss = 0.283802\n",
      "epoch : 427/700, train_loss = 0.116299\n",
      "epoch : 427/700, val_loss = 0.281082\n",
      "epoch : 428/700, train_loss = 0.115806\n",
      "epoch : 428/700, val_loss = 0.280952\n",
      "epoch : 429/700, train_loss = 0.116045\n",
      "epoch : 429/700, val_loss = 0.283068\n",
      "epoch : 430/700, train_loss = 0.116079\n",
      "epoch : 430/700, val_loss = 0.282937\n",
      "epoch : 431/700, train_loss = 0.116310\n",
      "epoch : 431/700, val_loss = 0.279607\n",
      "epoch : 432/700, train_loss = 0.116376\n",
      "epoch : 432/700, val_loss = 0.283200\n",
      "epoch : 433/700, train_loss = 0.116696\n",
      "epoch : 433/700, val_loss = 0.286124\n",
      "epoch : 434/700, train_loss = 0.117222\n",
      "epoch : 434/700, val_loss = 0.287382\n",
      "epoch : 435/700, train_loss = 0.117995\n",
      "epoch : 435/700, val_loss = 0.285314\n",
      "epoch : 436/700, train_loss = 0.117640\n",
      "epoch : 436/700, val_loss = 0.285218\n",
      "epoch : 437/700, train_loss = 0.118012\n",
      "epoch : 437/700, val_loss = 0.280726\n",
      "epoch : 438/700, train_loss = 0.117695\n",
      "epoch : 438/700, val_loss = 0.281658\n",
      "epoch : 439/700, train_loss = 0.117677\n",
      "epoch : 439/700, val_loss = 0.282340\n",
      "epoch : 440/700, train_loss = 0.116971\n",
      "epoch : 440/700, val_loss = 0.288928\n",
      "epoch : 441/700, train_loss = 0.117088\n",
      "epoch : 441/700, val_loss = 0.286105\n",
      "epoch : 442/700, train_loss = 0.116741\n",
      "epoch : 442/700, val_loss = 0.286479\n",
      "epoch : 443/700, train_loss = 0.116926\n",
      "epoch : 443/700, val_loss = 0.285347\n",
      "epoch : 444/700, train_loss = 0.117131\n",
      "epoch : 444/700, val_loss = 0.282903\n",
      "epoch : 445/700, train_loss = 0.116920\n",
      "epoch : 445/700, val_loss = 0.282153\n",
      "epoch : 446/700, train_loss = 0.117060\n",
      "epoch : 446/700, val_loss = 0.280809\n",
      "epoch : 447/700, train_loss = 0.116871\n",
      "epoch : 447/700, val_loss = 0.283242\n",
      "epoch : 448/700, train_loss = 0.117176\n",
      "epoch : 448/700, val_loss = 0.286510\n",
      "epoch : 449/700, train_loss = 0.116990\n",
      "epoch : 449/700, val_loss = 0.286123\n",
      "epoch : 450/700, train_loss = 0.117309\n",
      "epoch : 450/700, val_loss = 0.282873\n",
      "epoch : 451/700, train_loss = 0.116940\n",
      "epoch : 451/700, val_loss = 0.280710\n",
      "epoch : 452/700, train_loss = 0.117280\n",
      "epoch : 452/700, val_loss = 0.280355\n",
      "epoch : 453/700, train_loss = 0.117069\n",
      "epoch : 453/700, val_loss = 0.284919\n",
      "epoch : 454/700, train_loss = 0.117088\n",
      "epoch : 454/700, val_loss = 0.290152\n",
      "epoch : 455/700, train_loss = 0.116407\n",
      "epoch : 455/700, val_loss = 0.287802\n",
      "epoch : 456/700, train_loss = 0.116513\n",
      "epoch : 456/700, val_loss = 0.285168\n",
      "epoch : 457/700, train_loss = 0.116472\n",
      "epoch : 457/700, val_loss = 0.280111\n",
      "epoch : 458/700, train_loss = 0.116782\n",
      "epoch : 458/700, val_loss = 0.292227\n",
      "epoch : 459/700, train_loss = 0.180942\n",
      "epoch : 459/700, val_loss = 0.308829\n",
      "epoch : 460/700, train_loss = 0.176959\n",
      "epoch : 460/700, val_loss = 0.278693\n",
      "epoch : 461/700, train_loss = 0.150877\n",
      "epoch : 461/700, val_loss = 0.278688\n",
      "epoch : 462/700, train_loss = 0.136073\n",
      "epoch : 462/700, val_loss = 0.277633\n",
      "epoch : 463/700, train_loss = 0.128980\n",
      "epoch : 463/700, val_loss = 0.278847\n",
      "epoch : 464/700, train_loss = 0.125154\n",
      "epoch : 464/700, val_loss = 0.277863\n",
      "epoch : 465/700, train_loss = 0.122574\n",
      "epoch : 465/700, val_loss = 0.277000\n",
      "epoch : 466/700, train_loss = 0.120865\n",
      "epoch : 466/700, val_loss = 0.279435\n",
      "epoch : 467/700, train_loss = 0.119774\n",
      "epoch : 467/700, val_loss = 0.282543\n",
      "epoch : 468/700, train_loss = 0.119298\n",
      "epoch : 468/700, val_loss = 0.279461\n",
      "epoch : 469/700, train_loss = 0.119116\n",
      "epoch : 469/700, val_loss = 0.278122\n",
      "epoch : 470/700, train_loss = 0.118825\n",
      "epoch : 470/700, val_loss = 0.282368\n",
      "epoch : 471/700, train_loss = 0.118963\n",
      "epoch : 471/700, val_loss = 0.282756\n",
      "epoch : 472/700, train_loss = 0.118860\n",
      "epoch : 472/700, val_loss = 0.280834\n",
      "epoch : 473/700, train_loss = 0.118392\n",
      "epoch : 473/700, val_loss = 0.277764\n",
      "epoch : 474/700, train_loss = 0.118246\n",
      "epoch : 474/700, val_loss = 0.278569\n",
      "epoch : 475/700, train_loss = 0.118153\n",
      "epoch : 475/700, val_loss = 0.282287\n",
      "epoch : 476/700, train_loss = 0.117929\n",
      "epoch : 476/700, val_loss = 0.283336\n",
      "epoch : 477/700, train_loss = 0.117499\n",
      "epoch : 477/700, val_loss = 0.282916\n",
      "epoch : 478/700, train_loss = 0.117118\n",
      "epoch : 478/700, val_loss = 0.283924\n",
      "epoch : 479/700, train_loss = 0.116314\n",
      "epoch : 479/700, val_loss = 0.282592\n",
      "epoch : 480/700, train_loss = 0.116563\n",
      "epoch : 480/700, val_loss = 0.292828\n",
      "epoch : 481/700, train_loss = 0.161483\n",
      "epoch : 481/700, val_loss = 0.285644\n",
      "epoch : 482/700, train_loss = 0.146953\n",
      "epoch : 482/700, val_loss = 0.282147\n",
      "epoch : 483/700, train_loss = 0.132401\n",
      "epoch : 483/700, val_loss = 0.278753\n",
      "epoch : 484/700, train_loss = 0.124606\n",
      "epoch : 484/700, val_loss = 0.276142\n",
      "epoch : 485/700, train_loss = 0.120542\n",
      "epoch : 485/700, val_loss = 0.279243\n",
      "epoch : 486/700, train_loss = 0.128199\n",
      "epoch : 486/700, val_loss = 0.277339\n",
      "epoch : 487/700, train_loss = 0.124107\n",
      "epoch : 487/700, val_loss = 0.272513\n",
      "epoch : 488/700, train_loss = 0.120357\n",
      "epoch : 488/700, val_loss = 0.271976\n",
      "epoch : 489/700, train_loss = 0.117998\n",
      "epoch : 489/700, val_loss = 0.275134\n",
      "epoch : 490/700, train_loss = 0.116653\n",
      "epoch : 490/700, val_loss = 0.276837\n",
      "epoch : 491/700, train_loss = 0.115583\n",
      "epoch : 491/700, val_loss = 0.278880\n",
      "epoch : 492/700, train_loss = 0.114910\n",
      "epoch : 492/700, val_loss = 0.280196\n",
      "epoch : 493/700, train_loss = 0.114622\n",
      "epoch : 493/700, val_loss = 0.279407\n",
      "epoch : 494/700, train_loss = 0.114503\n",
      "epoch : 494/700, val_loss = 0.278774\n",
      "epoch : 495/700, train_loss = 0.114525\n",
      "epoch : 495/700, val_loss = 0.275689\n",
      "epoch : 496/700, train_loss = 0.114635\n",
      "epoch : 496/700, val_loss = 0.274324\n",
      "epoch : 497/700, train_loss = 0.114720\n",
      "epoch : 497/700, val_loss = 0.276018\n",
      "epoch : 498/700, train_loss = 0.114893\n",
      "epoch : 498/700, val_loss = 0.282268\n",
      "epoch : 499/700, train_loss = 0.115589\n",
      "epoch : 499/700, val_loss = 0.283113\n",
      "epoch : 500/700, train_loss = 0.116298\n",
      "epoch : 500/700, val_loss = 0.281476\n",
      "epoch : 501/700, train_loss = 0.116479\n",
      "epoch : 501/700, val_loss = 0.282897\n",
      "epoch : 502/700, train_loss = 0.117099\n",
      "epoch : 502/700, val_loss = 0.284132\n",
      "epoch : 503/700, train_loss = 0.117020\n",
      "epoch : 503/700, val_loss = 0.287899\n",
      "epoch : 504/700, train_loss = 0.116518\n",
      "epoch : 504/700, val_loss = 0.282535\n",
      "epoch : 505/700, train_loss = 0.115942\n",
      "epoch : 505/700, val_loss = 0.277078\n",
      "epoch : 506/700, train_loss = 0.115522\n",
      "epoch : 506/700, val_loss = 0.277846\n",
      "epoch : 507/700, train_loss = 0.115097\n",
      "epoch : 507/700, val_loss = 0.279402\n",
      "epoch : 508/700, train_loss = 0.114433\n",
      "epoch : 508/700, val_loss = 0.282527\n",
      "epoch : 509/700, train_loss = 0.114185\n",
      "epoch : 509/700, val_loss = 0.281049\n",
      "epoch : 510/700, train_loss = 0.114060\n",
      "epoch : 510/700, val_loss = 0.278704\n",
      "epoch : 511/700, train_loss = 0.113748\n",
      "epoch : 511/700, val_loss = 0.279280\n",
      "epoch : 512/700, train_loss = 0.114006\n",
      "epoch : 512/700, val_loss = 0.280128\n",
      "epoch : 513/700, train_loss = 0.113925\n",
      "epoch : 513/700, val_loss = 0.280398\n",
      "epoch : 514/700, train_loss = 0.114164\n",
      "epoch : 514/700, val_loss = 0.279566\n",
      "epoch : 515/700, train_loss = 0.114360\n",
      "epoch : 515/700, val_loss = 0.279667\n",
      "epoch : 516/700, train_loss = 0.114253\n",
      "epoch : 516/700, val_loss = 0.280996\n",
      "epoch : 517/700, train_loss = 0.114698\n",
      "epoch : 517/700, val_loss = 0.278874\n",
      "epoch : 518/700, train_loss = 0.114751\n",
      "epoch : 518/700, val_loss = 0.280397\n",
      "epoch : 519/700, train_loss = 0.114951\n",
      "epoch : 519/700, val_loss = 0.288274\n",
      "epoch : 520/700, train_loss = 0.119405\n",
      "epoch : 520/700, val_loss = 0.291141\n",
      "epoch : 521/700, train_loss = 0.118400\n",
      "epoch : 521/700, val_loss = 0.284852\n",
      "epoch : 522/700, train_loss = 0.117069\n",
      "epoch : 522/700, val_loss = 0.281320\n",
      "epoch : 523/700, train_loss = 0.115794\n",
      "epoch : 523/700, val_loss = 0.278576\n",
      "epoch : 524/700, train_loss = 0.115291\n",
      "epoch : 524/700, val_loss = 0.279895\n",
      "epoch : 525/700, train_loss = 0.115650\n",
      "epoch : 525/700, val_loss = 0.282152\n",
      "epoch : 526/700, train_loss = 0.116624\n",
      "epoch : 526/700, val_loss = 0.283680\n",
      "epoch : 527/700, train_loss = 0.116229\n",
      "epoch : 527/700, val_loss = 0.283346\n",
      "epoch : 528/700, train_loss = 0.115587\n",
      "epoch : 528/700, val_loss = 0.285115\n",
      "epoch : 529/700, train_loss = 0.114962\n",
      "epoch : 529/700, val_loss = 0.284358\n",
      "epoch : 530/700, train_loss = 0.114495\n",
      "epoch : 530/700, val_loss = 0.281422\n",
      "epoch : 531/700, train_loss = 0.114166\n",
      "epoch : 531/700, val_loss = 0.278844\n",
      "epoch : 532/700, train_loss = 0.113978\n",
      "epoch : 532/700, val_loss = 0.277728\n",
      "epoch : 533/700, train_loss = 0.113829\n",
      "epoch : 533/700, val_loss = 0.278335\n",
      "epoch : 534/700, train_loss = 0.113289\n",
      "epoch : 534/700, val_loss = 0.281912\n",
      "epoch : 535/700, train_loss = 0.113450\n",
      "epoch : 535/700, val_loss = 0.283243\n",
      "epoch : 536/700, train_loss = 0.113542\n",
      "epoch : 536/700, val_loss = 0.280676\n",
      "epoch : 537/700, train_loss = 0.113131\n",
      "epoch : 537/700, val_loss = 0.282595\n",
      "epoch : 538/700, train_loss = 0.113170\n",
      "epoch : 538/700, val_loss = 0.281583\n",
      "epoch : 539/700, train_loss = 0.112953\n",
      "epoch : 539/700, val_loss = 0.282308\n",
      "epoch : 540/700, train_loss = 0.112931\n",
      "epoch : 540/700, val_loss = 0.282105\n",
      "epoch : 541/700, train_loss = 0.112776\n",
      "epoch : 541/700, val_loss = 0.280498\n",
      "epoch : 542/700, train_loss = 0.112751\n",
      "epoch : 542/700, val_loss = 0.279395\n",
      "epoch : 543/700, train_loss = 0.112512\n",
      "epoch : 543/700, val_loss = 0.280958\n",
      "epoch : 544/700, train_loss = 0.112503\n",
      "epoch : 544/700, val_loss = 0.281657\n",
      "epoch : 545/700, train_loss = 0.112472\n",
      "epoch : 545/700, val_loss = 0.284582\n",
      "epoch : 546/700, train_loss = 0.112574\n",
      "epoch : 546/700, val_loss = 0.286679\n",
      "epoch : 547/700, train_loss = 0.112536\n",
      "epoch : 547/700, val_loss = 0.286347\n",
      "epoch : 548/700, train_loss = 0.112839\n",
      "epoch : 548/700, val_loss = 0.283930\n",
      "epoch : 549/700, train_loss = 0.113221\n",
      "epoch : 549/700, val_loss = 0.279728\n",
      "epoch : 550/700, train_loss = 0.113708\n",
      "epoch : 550/700, val_loss = 0.278347\n",
      "epoch : 551/700, train_loss = 0.114163\n",
      "epoch : 551/700, val_loss = 0.279934\n",
      "epoch : 552/700, train_loss = 0.114559\n",
      "epoch : 552/700, val_loss = 0.283295\n",
      "epoch : 553/700, train_loss = 0.115274\n",
      "epoch : 553/700, val_loss = 0.285560\n",
      "epoch : 554/700, train_loss = 0.115129\n",
      "epoch : 554/700, val_loss = 0.290667\n",
      "epoch : 555/700, train_loss = 0.115464\n",
      "epoch : 555/700, val_loss = 0.289707\n",
      "epoch : 556/700, train_loss = 0.115622\n",
      "epoch : 556/700, val_loss = 0.285007\n",
      "epoch : 557/700, train_loss = 0.115495\n",
      "epoch : 557/700, val_loss = 0.280565\n",
      "epoch : 558/700, train_loss = 0.115201\n",
      "epoch : 558/700, val_loss = 0.281317\n",
      "epoch : 559/700, train_loss = 0.115052\n",
      "epoch : 559/700, val_loss = 0.282681\n",
      "epoch : 560/700, train_loss = 0.114592\n",
      "epoch : 560/700, val_loss = 0.289319\n",
      "epoch : 561/700, train_loss = 0.114302\n",
      "epoch : 561/700, val_loss = 0.289316\n",
      "epoch : 562/700, train_loss = 0.113872\n",
      "epoch : 562/700, val_loss = 0.283649\n",
      "epoch : 563/700, train_loss = 0.113297\n",
      "epoch : 563/700, val_loss = 0.279758\n",
      "epoch : 564/700, train_loss = 0.113037\n",
      "epoch : 564/700, val_loss = 0.278874\n",
      "epoch : 565/700, train_loss = 0.112925\n",
      "epoch : 565/700, val_loss = 0.280174\n",
      "epoch : 566/700, train_loss = 0.112413\n",
      "epoch : 566/700, val_loss = 0.285625\n",
      "epoch : 567/700, train_loss = 0.112548\n",
      "epoch : 567/700, val_loss = 0.282841\n",
      "epoch : 568/700, train_loss = 0.112895\n",
      "epoch : 568/700, val_loss = 0.280913\n",
      "epoch : 569/700, train_loss = 0.113140\n",
      "epoch : 569/700, val_loss = 0.282919\n",
      "epoch : 570/700, train_loss = 0.113552\n",
      "epoch : 570/700, val_loss = 0.285488\n",
      "epoch : 571/700, train_loss = 0.114023\n",
      "epoch : 571/700, val_loss = 0.287093\n",
      "epoch : 572/700, train_loss = 0.114442\n",
      "epoch : 572/700, val_loss = 0.289197\n",
      "epoch : 573/700, train_loss = 0.114850\n",
      "epoch : 573/700, val_loss = 0.284593\n",
      "epoch : 574/700, train_loss = 0.114480\n",
      "epoch : 574/700, val_loss = 0.284779\n",
      "epoch : 575/700, train_loss = 0.114311\n",
      "epoch : 575/700, val_loss = 0.280956\n",
      "epoch : 576/700, train_loss = 0.114264\n",
      "epoch : 576/700, val_loss = 0.282557\n",
      "epoch : 577/700, train_loss = 0.114317\n",
      "epoch : 577/700, val_loss = 0.289363\n",
      "epoch : 578/700, train_loss = 0.114650\n",
      "epoch : 578/700, val_loss = 0.289341\n",
      "epoch : 579/700, train_loss = 0.114744\n",
      "epoch : 579/700, val_loss = 0.286417\n",
      "epoch : 580/700, train_loss = 0.114676\n",
      "epoch : 580/700, val_loss = 0.283213\n",
      "epoch : 581/700, train_loss = 0.114123\n",
      "epoch : 581/700, val_loss = 0.283685\n",
      "epoch : 582/700, train_loss = 0.113976\n",
      "epoch : 582/700, val_loss = 0.280594\n",
      "epoch : 583/700, train_loss = 0.113782\n",
      "epoch : 583/700, val_loss = 0.282384\n",
      "epoch : 584/700, train_loss = 0.114060\n",
      "epoch : 584/700, val_loss = 0.282302\n",
      "epoch : 585/700, train_loss = 0.113939\n",
      "epoch : 585/700, val_loss = 0.286465\n",
      "epoch : 586/700, train_loss = 0.114087\n",
      "epoch : 586/700, val_loss = 0.284951\n",
      "epoch : 587/700, train_loss = 0.113523\n",
      "epoch : 587/700, val_loss = 0.286604\n",
      "epoch : 588/700, train_loss = 0.113559\n",
      "epoch : 588/700, val_loss = 0.286505\n",
      "epoch : 589/700, train_loss = 0.113872\n",
      "epoch : 589/700, val_loss = 0.282301\n",
      "epoch : 590/700, train_loss = 0.113494\n",
      "epoch : 590/700, val_loss = 0.280752\n",
      "epoch : 591/700, train_loss = 0.113332\n",
      "epoch : 591/700, val_loss = 0.284930\n",
      "epoch : 592/700, train_loss = 0.120569\n",
      "epoch : 592/700, val_loss = 0.291695\n",
      "epoch : 593/700, train_loss = 0.121415\n",
      "epoch : 593/700, val_loss = 0.290964\n",
      "epoch : 594/700, train_loss = 0.118701\n",
      "epoch : 594/700, val_loss = 0.284181\n",
      "epoch : 595/700, train_loss = 0.116271\n",
      "epoch : 595/700, val_loss = 0.283633\n",
      "epoch : 596/700, train_loss = 0.114956\n",
      "epoch : 596/700, val_loss = 0.283692\n",
      "epoch : 597/700, train_loss = 0.114001\n",
      "epoch : 597/700, val_loss = 0.286177\n",
      "epoch : 598/700, train_loss = 0.113415\n",
      "epoch : 598/700, val_loss = 0.287954\n",
      "epoch : 599/700, train_loss = 0.113769\n",
      "epoch : 599/700, val_loss = 0.285813\n",
      "epoch : 600/700, train_loss = 0.113838\n",
      "epoch : 600/700, val_loss = 0.287154\n",
      "epoch : 601/700, train_loss = 0.113446\n",
      "epoch : 601/700, val_loss = 0.289185\n",
      "epoch : 602/700, train_loss = 0.113293\n",
      "epoch : 602/700, val_loss = 0.286637\n",
      "epoch : 603/700, train_loss = 0.113094\n",
      "epoch : 603/700, val_loss = 0.283537\n",
      "epoch : 604/700, train_loss = 0.112997\n",
      "epoch : 604/700, val_loss = 0.280310\n",
      "epoch : 605/700, train_loss = 0.112714\n",
      "epoch : 605/700, val_loss = 0.283823\n",
      "epoch : 606/700, train_loss = 0.112575\n",
      "epoch : 606/700, val_loss = 0.288286\n",
      "epoch : 607/700, train_loss = 0.112662\n",
      "epoch : 607/700, val_loss = 0.288164\n",
      "epoch : 608/700, train_loss = 0.112534\n",
      "epoch : 608/700, val_loss = 0.287039\n",
      "epoch : 609/700, train_loss = 0.112253\n",
      "epoch : 609/700, val_loss = 0.284460\n",
      "epoch : 610/700, train_loss = 0.112071\n",
      "epoch : 610/700, val_loss = 0.285002\n",
      "epoch : 611/700, train_loss = 0.112043\n",
      "epoch : 611/700, val_loss = 0.284880\n",
      "epoch : 612/700, train_loss = 0.111920\n",
      "epoch : 612/700, val_loss = 0.286454\n",
      "epoch : 613/700, train_loss = 0.112283\n",
      "epoch : 613/700, val_loss = 0.285089\n",
      "epoch : 614/700, train_loss = 0.112218\n",
      "epoch : 614/700, val_loss = 0.284853\n",
      "epoch : 615/700, train_loss = 0.112088\n",
      "epoch : 615/700, val_loss = 0.280988\n",
      "epoch : 616/700, train_loss = 0.111718\n",
      "epoch : 616/700, val_loss = 0.282304\n",
      "epoch : 617/700, train_loss = 0.111988\n",
      "epoch : 617/700, val_loss = 0.282823\n",
      "epoch : 618/700, train_loss = 0.111996\n",
      "epoch : 618/700, val_loss = 0.286196\n",
      "epoch : 619/700, train_loss = 0.112254\n",
      "epoch : 619/700, val_loss = 0.285375\n",
      "epoch : 620/700, train_loss = 0.112512\n",
      "epoch : 620/700, val_loss = 0.287155\n",
      "epoch : 621/700, train_loss = 0.112957\n",
      "epoch : 621/700, val_loss = 0.288075\n",
      "epoch : 622/700, train_loss = 0.113432\n",
      "epoch : 622/700, val_loss = 0.287542\n",
      "epoch : 623/700, train_loss = 0.114187\n",
      "epoch : 623/700, val_loss = 0.285232\n",
      "epoch : 624/700, train_loss = 0.113917\n",
      "epoch : 624/700, val_loss = 0.289696\n",
      "epoch : 625/700, train_loss = 0.114305\n",
      "epoch : 625/700, val_loss = 0.289156\n",
      "epoch : 626/700, train_loss = 0.115146\n",
      "epoch : 626/700, val_loss = 0.287994\n",
      "epoch : 627/700, train_loss = 0.115436\n",
      "epoch : 627/700, val_loss = 0.283300\n",
      "epoch : 628/700, train_loss = 0.114582\n",
      "epoch : 628/700, val_loss = 0.287655\n",
      "epoch : 629/700, train_loss = 0.114771\n",
      "epoch : 629/700, val_loss = 0.284188\n",
      "epoch : 630/700, train_loss = 0.114125\n",
      "epoch : 630/700, val_loss = 0.283940\n",
      "epoch : 631/700, train_loss = 0.113739\n",
      "epoch : 631/700, val_loss = 0.280846\n",
      "epoch : 632/700, train_loss = 0.113013\n",
      "epoch : 632/700, val_loss = 0.285121\n",
      "epoch : 633/700, train_loss = 0.112931\n",
      "epoch : 633/700, val_loss = 0.286359\n",
      "epoch : 634/700, train_loss = 0.112294\n",
      "epoch : 634/700, val_loss = 0.283997\n",
      "epoch : 635/700, train_loss = 0.112368\n",
      "epoch : 635/700, val_loss = 0.282130\n",
      "epoch : 636/700, train_loss = 0.111989\n",
      "epoch : 636/700, val_loss = 0.283734\n",
      "epoch : 637/700, train_loss = 0.111675\n",
      "epoch : 637/700, val_loss = 0.283153\n",
      "epoch : 638/700, train_loss = 0.111662\n",
      "epoch : 638/700, val_loss = 0.280922\n",
      "epoch : 639/700, train_loss = 0.111219\n",
      "epoch : 639/700, val_loss = 0.282317\n",
      "epoch : 640/700, train_loss = 0.111274\n",
      "epoch : 640/700, val_loss = 0.285126\n",
      "epoch : 641/700, train_loss = 0.111164\n",
      "epoch : 641/700, val_loss = 0.283597\n",
      "epoch : 642/700, train_loss = 0.111042\n",
      "epoch : 642/700, val_loss = 0.280803\n",
      "epoch : 643/700, train_loss = 0.110824\n",
      "epoch : 643/700, val_loss = 0.280496\n",
      "epoch : 644/700, train_loss = 0.110987\n",
      "epoch : 644/700, val_loss = 0.283228\n",
      "epoch : 645/700, train_loss = 0.110847\n",
      "epoch : 645/700, val_loss = 0.285744\n",
      "epoch : 646/700, train_loss = 0.111128\n",
      "epoch : 646/700, val_loss = 0.281362\n",
      "epoch : 647/700, train_loss = 0.110891\n",
      "epoch : 647/700, val_loss = 0.283584\n",
      "epoch : 648/700, train_loss = 0.111266\n",
      "epoch : 648/700, val_loss = 0.283822\n",
      "epoch : 649/700, train_loss = 0.111649\n",
      "epoch : 649/700, val_loss = 0.287286\n",
      "epoch : 650/700, train_loss = 0.111954\n",
      "epoch : 650/700, val_loss = 0.289232\n",
      "epoch : 651/700, train_loss = 0.112290\n",
      "epoch : 651/700, val_loss = 0.289463\n",
      "epoch : 652/700, train_loss = 0.112625\n",
      "epoch : 652/700, val_loss = 0.286745\n",
      "epoch : 653/700, train_loss = 0.112567\n",
      "epoch : 653/700, val_loss = 0.286072\n",
      "epoch : 654/700, train_loss = 0.112887\n",
      "epoch : 654/700, val_loss = 0.279401\n",
      "epoch : 655/700, train_loss = 0.112894\n",
      "epoch : 655/700, val_loss = 0.282338\n",
      "epoch : 656/700, train_loss = 0.112629\n",
      "epoch : 656/700, val_loss = 0.289091\n",
      "epoch : 657/700, train_loss = 0.112726\n",
      "epoch : 657/700, val_loss = 0.290681\n",
      "epoch : 658/700, train_loss = 0.112578\n",
      "epoch : 658/700, val_loss = 0.285429\n",
      "epoch : 659/700, train_loss = 0.112291\n",
      "epoch : 659/700, val_loss = 0.280695\n",
      "epoch : 660/700, train_loss = 0.112454\n",
      "epoch : 660/700, val_loss = 0.281836\n",
      "epoch : 661/700, train_loss = 0.112081\n",
      "epoch : 661/700, val_loss = 0.284836\n",
      "epoch : 662/700, train_loss = 0.112282\n",
      "epoch : 662/700, val_loss = 0.283983\n",
      "epoch : 663/700, train_loss = 0.112059\n",
      "epoch : 663/700, val_loss = 0.281213\n",
      "epoch : 664/700, train_loss = 0.112196\n",
      "epoch : 664/700, val_loss = 0.284802\n",
      "epoch : 665/700, train_loss = 0.112335\n",
      "epoch : 665/700, val_loss = 0.287991\n",
      "epoch : 666/700, train_loss = 0.114343\n",
      "epoch : 666/700, val_loss = 0.289116\n",
      "epoch : 667/700, train_loss = 0.114236\n",
      "epoch : 667/700, val_loss = 0.289496\n",
      "epoch : 668/700, train_loss = 0.113672\n",
      "epoch : 668/700, val_loss = 0.290383\n",
      "epoch : 669/700, train_loss = 0.114079\n",
      "epoch : 669/700, val_loss = 0.286308\n",
      "epoch : 670/700, train_loss = 0.114226\n",
      "epoch : 670/700, val_loss = 0.285163\n",
      "epoch : 671/700, train_loss = 0.114449\n",
      "epoch : 671/700, val_loss = 0.286870\n",
      "epoch : 672/700, train_loss = 0.113563\n",
      "epoch : 672/700, val_loss = 0.290950\n",
      "epoch : 673/700, train_loss = 0.113833\n",
      "epoch : 673/700, val_loss = 0.286589\n",
      "epoch : 674/700, train_loss = 0.113760\n",
      "epoch : 674/700, val_loss = 0.281070\n",
      "epoch : 675/700, train_loss = 0.112817\n",
      "epoch : 675/700, val_loss = 0.283089\n",
      "epoch : 676/700, train_loss = 0.112334\n",
      "epoch : 676/700, val_loss = 0.285060\n",
      "epoch : 677/700, train_loss = 0.111937\n",
      "epoch : 677/700, val_loss = 0.285644\n",
      "epoch : 678/700, train_loss = 0.111590\n",
      "epoch : 678/700, val_loss = 0.283074\n",
      "epoch : 679/700, train_loss = 0.110893\n",
      "epoch : 679/700, val_loss = 0.283954\n",
      "epoch : 680/700, train_loss = 0.110933\n",
      "epoch : 680/700, val_loss = 0.280867\n",
      "epoch : 681/700, train_loss = 0.111077\n",
      "epoch : 681/700, val_loss = 0.286496\n",
      "epoch : 682/700, train_loss = 0.137115\n",
      "epoch : 682/700, val_loss = 0.286856\n",
      "epoch : 683/700, train_loss = 0.129686\n",
      "epoch : 683/700, val_loss = 0.282147\n",
      "epoch : 684/700, train_loss = 0.121740\n",
      "epoch : 684/700, val_loss = 0.285019\n",
      "epoch : 685/700, train_loss = 0.116595\n",
      "epoch : 685/700, val_loss = 0.282409\n",
      "epoch : 686/700, train_loss = 0.114368\n",
      "epoch : 686/700, val_loss = 0.281374\n",
      "epoch : 687/700, train_loss = 0.112940\n",
      "epoch : 687/700, val_loss = 0.280327\n",
      "epoch : 688/700, train_loss = 0.112368\n",
      "epoch : 688/700, val_loss = 0.280348\n",
      "epoch : 689/700, train_loss = 0.112052\n",
      "epoch : 689/700, val_loss = 0.281035\n",
      "epoch : 690/700, train_loss = 0.111584\n",
      "epoch : 690/700, val_loss = 0.285724\n",
      "epoch : 691/700, train_loss = 0.111751\n",
      "epoch : 691/700, val_loss = 0.282033\n",
      "epoch : 692/700, train_loss = 0.111922\n",
      "epoch : 692/700, val_loss = 0.278208\n",
      "epoch : 693/700, train_loss = 0.111580\n",
      "epoch : 693/700, val_loss = 0.279574\n",
      "epoch : 694/700, train_loss = 0.111630\n",
      "epoch : 694/700, val_loss = 0.280958\n",
      "epoch : 695/700, train_loss = 0.111631\n",
      "epoch : 695/700, val_loss = 0.283546\n",
      "epoch : 696/700, train_loss = 0.111373\n",
      "epoch : 696/700, val_loss = 0.285855\n",
      "epoch : 697/700, train_loss = 0.111154\n",
      "epoch : 697/700, val_loss = 0.284880\n",
      "epoch : 698/700, train_loss = 0.110906\n",
      "epoch : 698/700, val_loss = 0.283917\n",
      "epoch : 699/700, train_loss = 0.110624\n",
      "epoch : 699/700, val_loss = 0.282705\n",
      "epoch : 700/700, train_loss = 0.110333\n",
      "epoch : 700/700, val_loss = 0.280644\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "epochs=700\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        lol, outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train.append(loss)\n",
    "\n",
    "\n",
    "    #For Valid Loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            code, outputs = model(batch)\n",
    "            loss_val =criterion(outputs, batch)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    losses_val.append(val_loss)\n",
    "\n",
    "\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    print(\"epoch : {}/{}, val_loss = {:.6f}\".format(epoch + 1, epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train, label = 'train_loss')\n",
    "plt.plot(losses_val, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"../../Figures/AE_cnv.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(X_full_sc,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Gene Autoencoder Taining\n",
    "\n",
    "data_full = MyDataset(X_full_sc)\n",
    "full_loader = DataLoader(data_full, batch_size=50)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE(input_shape = len(X_full_sc[0])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/100, train_loss = 1.350110\n",
      "epoch : 2/100, train_loss = 0.824939\n",
      "epoch : 3/100, train_loss = 0.749920\n",
      "epoch : 4/100, train_loss = 0.702631\n",
      "epoch : 5/100, train_loss = 0.662834\n",
      "epoch : 6/100, train_loss = 0.626659\n",
      "epoch : 7/100, train_loss = 0.593370\n",
      "epoch : 8/100, train_loss = 0.561592\n",
      "epoch : 9/100, train_loss = 0.532784\n",
      "epoch : 10/100, train_loss = 0.508384\n",
      "epoch : 11/100, train_loss = 0.488393\n",
      "epoch : 12/100, train_loss = 0.471875\n",
      "epoch : 13/100, train_loss = 0.457809\n",
      "epoch : 14/100, train_loss = 0.444228\n",
      "epoch : 15/100, train_loss = 0.430415\n",
      "epoch : 16/100, train_loss = 0.420101\n",
      "epoch : 17/100, train_loss = 0.411300\n",
      "epoch : 18/100, train_loss = 0.404620\n",
      "epoch : 19/100, train_loss = 0.396524\n",
      "epoch : 20/100, train_loss = 0.387928\n",
      "epoch : 21/100, train_loss = 0.380161\n",
      "epoch : 22/100, train_loss = 0.374433\n",
      "epoch : 23/100, train_loss = 0.370264\n",
      "epoch : 24/100, train_loss = 0.365195\n",
      "epoch : 25/100, train_loss = 0.361014\n",
      "epoch : 26/100, train_loss = 0.356363\n",
      "epoch : 27/100, train_loss = 0.351127\n",
      "epoch : 28/100, train_loss = 0.346425\n",
      "epoch : 29/100, train_loss = 0.341874\n",
      "epoch : 30/100, train_loss = 0.338267\n",
      "epoch : 31/100, train_loss = 0.334871\n",
      "epoch : 32/100, train_loss = 0.330782\n",
      "epoch : 33/100, train_loss = 0.327594\n",
      "epoch : 34/100, train_loss = 0.325019\n",
      "epoch : 35/100, train_loss = 0.322207\n",
      "epoch : 36/100, train_loss = 0.319455\n",
      "epoch : 37/100, train_loss = 0.318072\n",
      "epoch : 38/100, train_loss = 0.315458\n",
      "epoch : 39/100, train_loss = 0.312924\n",
      "epoch : 40/100, train_loss = 0.309662\n",
      "epoch : 41/100, train_loss = 0.307487\n",
      "epoch : 42/100, train_loss = 0.305356\n",
      "epoch : 43/100, train_loss = 0.302598\n",
      "epoch : 44/100, train_loss = 0.300264\n",
      "epoch : 45/100, train_loss = 0.297779\n",
      "epoch : 46/100, train_loss = 0.295872\n",
      "epoch : 47/100, train_loss = 0.294086\n",
      "epoch : 48/100, train_loss = 0.293191\n",
      "epoch : 49/100, train_loss = 0.292446\n",
      "epoch : 50/100, train_loss = 0.289377\n",
      "epoch : 51/100, train_loss = 0.286789\n",
      "epoch : 52/100, train_loss = 0.284612\n",
      "epoch : 53/100, train_loss = 0.282291\n",
      "epoch : 54/100, train_loss = 0.281320\n",
      "epoch : 55/100, train_loss = 0.279556\n",
      "epoch : 56/100, train_loss = 0.279576\n",
      "epoch : 57/100, train_loss = 0.278858\n",
      "epoch : 58/100, train_loss = 0.277627\n",
      "epoch : 59/100, train_loss = 0.276456\n",
      "epoch : 60/100, train_loss = 0.274395\n",
      "epoch : 61/100, train_loss = 0.272353\n",
      "epoch : 62/100, train_loss = 0.268943\n",
      "epoch : 63/100, train_loss = 0.265480\n",
      "epoch : 64/100, train_loss = 0.263157\n",
      "epoch : 65/100, train_loss = 0.263056\n",
      "epoch : 66/100, train_loss = 0.260972\n",
      "epoch : 67/100, train_loss = 0.258670\n",
      "epoch : 68/100, train_loss = 0.257329\n",
      "epoch : 69/100, train_loss = 0.256714\n",
      "epoch : 70/100, train_loss = 0.256579\n",
      "epoch : 71/100, train_loss = 0.256855\n",
      "epoch : 72/100, train_loss = 0.255708\n",
      "epoch : 73/100, train_loss = 0.253217\n",
      "epoch : 74/100, train_loss = 0.251841\n",
      "epoch : 75/100, train_loss = 0.250338\n",
      "epoch : 76/100, train_loss = 0.247903\n",
      "epoch : 77/100, train_loss = 0.245845\n",
      "epoch : 78/100, train_loss = 0.244651\n",
      "epoch : 79/100, train_loss = 0.247340\n",
      "epoch : 80/100, train_loss = 0.247306\n",
      "epoch : 81/100, train_loss = 0.244460\n",
      "epoch : 82/100, train_loss = 0.243602\n",
      "epoch : 83/100, train_loss = 0.242451\n",
      "epoch : 84/100, train_loss = 0.240875\n",
      "epoch : 85/100, train_loss = 0.241900\n",
      "epoch : 86/100, train_loss = 0.241943\n",
      "epoch : 87/100, train_loss = 0.240553\n",
      "epoch : 88/100, train_loss = 0.239957\n",
      "epoch : 89/100, train_loss = 0.247945\n",
      "epoch : 90/100, train_loss = 0.266603\n",
      "epoch : 91/100, train_loss = 0.262873\n",
      "epoch : 92/100, train_loss = 0.246482\n",
      "epoch : 93/100, train_loss = 0.237888\n",
      "epoch : 94/100, train_loss = 0.233738\n",
      "epoch : 95/100, train_loss = 0.231146\n",
      "epoch : 96/100, train_loss = 0.229808\n",
      "epoch : 97/100, train_loss = 0.234581\n",
      "epoch : 98/100, train_loss = 0.233822\n",
      "epoch : 99/100, train_loss = 0.232945\n",
      "epoch : 100/100, train_loss = 0.230087\n"
     ]
    }
   ],
   "source": [
    "losses_train_final = []\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in full_loader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        code, outputs = model(batch_features)\n",
    "        \n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train_final.append(loss)\n",
    "\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out,out2 = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41fc6849-724a-4bac-8775-5e703fe74184</td>\n",
       "      <td>-0.974156</td>\n",
       "      <td>0.640870</td>\n",
       "      <td>0.954011</td>\n",
       "      <td>-0.290834</td>\n",
       "      <td>-0.895645</td>\n",
       "      <td>0.305227</td>\n",
       "      <td>-0.743481</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>0.743691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779438</td>\n",
       "      <td>-0.282916</td>\n",
       "      <td>1.046016</td>\n",
       "      <td>-0.897457</td>\n",
       "      <td>-1.595210</td>\n",
       "      <td>-0.421787</td>\n",
       "      <td>0.261736</td>\n",
       "      <td>-0.015702</td>\n",
       "      <td>-0.602377</td>\n",
       "      <td>-1.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0f6b347c-30da-495b-b103-36af30df77d7</td>\n",
       "      <td>-1.076514</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>0.847657</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>-0.743481</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>-0.757991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.606827</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>0.462093</td>\n",
       "      <td>-0.897457</td>\n",
       "      <td>-0.223368</td>\n",
       "      <td>-1.191027</td>\n",
       "      <td>2.811174</td>\n",
       "      <td>0.781487</td>\n",
       "      <td>2.521043</td>\n",
       "      <td>1.981540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6568ed2b-0018-4750-9b3b-6c414dba60ae</td>\n",
       "      <td>0.795264</td>\n",
       "      <td>-0.361459</td>\n",
       "      <td>0.818655</td>\n",
       "      <td>0.453412</td>\n",
       "      <td>-0.448078</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>-0.743481</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>1.411721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779438</td>\n",
       "      <td>-0.036675</td>\n",
       "      <td>0.525036</td>\n",
       "      <td>-0.897457</td>\n",
       "      <td>-0.325024</td>\n",
       "      <td>0.273792</td>\n",
       "      <td>1.294912</td>\n",
       "      <td>-0.440448</td>\n",
       "      <td>-0.602377</td>\n",
       "      <td>1.385615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e18b7869-0049-4be7-9611-d08db28df33d</td>\n",
       "      <td>-1.096501</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>-0.602058</td>\n",
       "      <td>-0.959978</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>0.493545</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>-0.564996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227359</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>-0.307242</td>\n",
       "      <td>-0.132606</td>\n",
       "      <td>-0.015206</td>\n",
       "      <td>-0.577474</td>\n",
       "      <td>-0.834728</td>\n",
       "      <td>-0.509095</td>\n",
       "      <td>0.683812</td>\n",
       "      <td>-1.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>361cc367-f85d-402a-a19d-999d33f7667a</td>\n",
       "      <td>-1.246765</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>-0.798074</td>\n",
       "      <td>-1.634691</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>-0.743481</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>-0.937965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427165</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>-1.114877</td>\n",
       "      <td>0.265582</td>\n",
       "      <td>-0.279157</td>\n",
       "      <td>-0.565246</td>\n",
       "      <td>-0.834728</td>\n",
       "      <td>-0.509095</td>\n",
       "      <td>-0.602377</td>\n",
       "      <td>-1.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>41811e19-d6bb-4f1c-ac67-9df06ad9f85b</td>\n",
       "      <td>-1.117414</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>-0.407473</td>\n",
       "      <td>-0.175871</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>0.208899</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>-0.937965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511187</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>-1.337603</td>\n",
       "      <td>0.698206</td>\n",
       "      <td>-0.373926</td>\n",
       "      <td>-0.383705</td>\n",
       "      <td>-0.834728</td>\n",
       "      <td>-0.509095</td>\n",
       "      <td>-0.232455</td>\n",
       "      <td>-1.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>409bd257-5ce4-4a03-a7a8-c9c8e697fb39</td>\n",
       "      <td>-0.518656</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>-0.802269</td>\n",
       "      <td>-0.282307</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>-0.004733</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>-0.683462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301082</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>-0.208951</td>\n",
       "      <td>-0.897457</td>\n",
       "      <td>-0.307497</td>\n",
       "      <td>0.202201</td>\n",
       "      <td>-0.834728</td>\n",
       "      <td>-0.509095</td>\n",
       "      <td>0.101116</td>\n",
       "      <td>-0.276592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>d9e3e564-e9c8-416c-8a25-91e6744b1472</td>\n",
       "      <td>-0.360635</td>\n",
       "      <td>0.209288</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>-0.802269</td>\n",
       "      <td>-0.998645</td>\n",
       "      <td>-0.793644</td>\n",
       "      <td>-0.743481</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779438</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>0.116199</td>\n",
       "      <td>-0.897457</td>\n",
       "      <td>0.394974</td>\n",
       "      <td>0.463602</td>\n",
       "      <td>-0.834728</td>\n",
       "      <td>0.030162</td>\n",
       "      <td>-0.602377</td>\n",
       "      <td>0.484565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>fc1de880-73b1-4a71-a1e9-09c0591bfc8f</td>\n",
       "      <td>2.197401</td>\n",
       "      <td>1.283121</td>\n",
       "      <td>1.496613</td>\n",
       "      <td>1.100107</td>\n",
       "      <td>1.182862</td>\n",
       "      <td>1.571816</td>\n",
       "      <td>-0.743481</td>\n",
       "      <td>-0.722786</td>\n",
       "      <td>0.987760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779438</td>\n",
       "      <td>1.848925</td>\n",
       "      <td>0.523951</td>\n",
       "      <td>-0.897457</td>\n",
       "      <td>-0.794214</td>\n",
       "      <td>1.821550</td>\n",
       "      <td>-0.321065</td>\n",
       "      <td>0.848350</td>\n",
       "      <td>-0.602377</td>\n",
       "      <td>-1.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>f48ea6d2-0ea2-4bdc-8ac7-2ad5b986892f</td>\n",
       "      <td>-1.246765</td>\n",
       "      <td>-0.841759</td>\n",
       "      <td>-0.723073</td>\n",
       "      <td>-0.802269</td>\n",
       "      <td>-0.756075</td>\n",
       "      <td>-0.830574</td>\n",
       "      <td>2.380940</td>\n",
       "      <td>3.325788</td>\n",
       "      <td>-0.937965</td>\n",
       "      <td>...</td>\n",
       "      <td>1.244015</td>\n",
       "      <td>-0.673909</td>\n",
       "      <td>-1.337603</td>\n",
       "      <td>2.570595</td>\n",
       "      <td>-1.595210</td>\n",
       "      <td>-1.854316</td>\n",
       "      <td>-0.834728</td>\n",
       "      <td>-0.509095</td>\n",
       "      <td>0.249588</td>\n",
       "      <td>-1.009496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file_name         0         1         2  \\\n",
       "0    41fc6849-724a-4bac-8775-5e703fe74184 -0.974156  0.640870  0.954011   \n",
       "1    0f6b347c-30da-495b-b103-36af30df77d7 -1.076514 -0.841759 -0.723073   \n",
       "2    6568ed2b-0018-4750-9b3b-6c414dba60ae  0.795264 -0.361459  0.818655   \n",
       "3    e18b7869-0049-4be7-9611-d08db28df33d -1.096501 -0.841759 -0.723073   \n",
       "4    361cc367-f85d-402a-a19d-999d33f7667a -1.246765 -0.841759 -0.723073   \n",
       "..                                    ...       ...       ...       ...   \n",
       "949  41811e19-d6bb-4f1c-ac67-9df06ad9f85b -1.117414 -0.841759 -0.723073   \n",
       "950  409bd257-5ce4-4a03-a7a8-c9c8e697fb39 -0.518656 -0.841759 -0.723073   \n",
       "951  d9e3e564-e9c8-416c-8a25-91e6744b1472 -0.360635  0.209288 -0.723073   \n",
       "952  fc1de880-73b1-4a71-a1e9-09c0591bfc8f  2.197401  1.283121  1.496613   \n",
       "953  f48ea6d2-0ea2-4bdc-8ac7-2ad5b986892f -1.246765 -0.841759 -0.723073   \n",
       "\n",
       "            3         4         5         6         7         8  ...  \\\n",
       "0   -0.290834 -0.895645  0.305227 -0.743481 -0.722786  0.743691  ...   \n",
       "1    0.847657 -0.414650 -0.830574 -0.743481 -0.722786 -0.757991  ...   \n",
       "2    0.453412 -0.448078 -0.830574 -0.743481 -0.722786  1.411721  ...   \n",
       "3   -0.602058 -0.959978 -0.830574  0.493545 -0.722786 -0.564996  ...   \n",
       "4   -0.798074 -1.634691 -0.830574 -0.743481 -0.722786 -0.937965  ...   \n",
       "..        ...       ...       ...       ...       ...       ...  ...   \n",
       "949 -0.407473 -0.175871 -0.830574  0.208899 -0.722786 -0.937965  ...   \n",
       "950 -0.802269 -0.282307 -0.830574 -0.004733 -0.722786 -0.683462  ...   \n",
       "951 -0.802269 -0.998645 -0.793644 -0.743481 -0.722786  0.008906  ...   \n",
       "952  1.100107  1.182862  1.571816 -0.743481 -0.722786  0.987760  ...   \n",
       "953 -0.802269 -0.756075 -0.830574  2.380940  3.325788 -0.937965  ...   \n",
       "\n",
       "          118       119       120       121       122       123       124  \\\n",
       "0   -0.779438 -0.282916  1.046016 -0.897457 -1.595210 -0.421787  0.261736   \n",
       "1   -0.606827 -0.673909  0.462093 -0.897457 -0.223368 -1.191027  2.811174   \n",
       "2   -0.779438 -0.036675  0.525036 -0.897457 -0.325024  0.273792  1.294912   \n",
       "3   -0.227359 -0.673909 -0.307242 -0.132606 -0.015206 -0.577474 -0.834728   \n",
       "4   -0.427165 -0.673909 -1.114877  0.265582 -0.279157 -0.565246 -0.834728   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "949  0.511187 -0.673909 -1.337603  0.698206 -0.373926 -0.383705 -0.834728   \n",
       "950 -0.301082 -0.673909 -0.208951 -0.897457 -0.307497  0.202201 -0.834728   \n",
       "951 -0.779438 -0.673909  0.116199 -0.897457  0.394974  0.463602 -0.834728   \n",
       "952 -0.779438  1.848925  0.523951 -0.897457 -0.794214  1.821550 -0.321065   \n",
       "953  1.244015 -0.673909 -1.337603  2.570595 -1.595210 -1.854316 -0.834728   \n",
       "\n",
       "          125       126       127  \n",
       "0   -0.015702 -0.602377 -1.009496  \n",
       "1    0.781487  2.521043  1.981540  \n",
       "2   -0.440448 -0.602377  1.385615  \n",
       "3   -0.509095  0.683812 -1.009496  \n",
       "4   -0.509095 -0.602377 -1.009496  \n",
       "..        ...       ...       ...  \n",
       "949 -0.509095 -0.232455 -1.009496  \n",
       "950 -0.509095  0.101116 -0.276592  \n",
       "951  0.030162 -0.602377  0.484565  \n",
       "952  0.848350 -0.602377 -1.009496  \n",
       "953 -0.509095  0.249588 -1.009496  \n",
       "\n",
       "[954 rows x 129 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df = pd.DataFrame(out)\n",
    "latent_df.insert(0,'file_name',cnv_df['file_name'])\n",
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"../../data/cnv_df_128.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ef9037651b8fb300183737a1adf54e758a8413bef4becc8f06877b013d9a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
