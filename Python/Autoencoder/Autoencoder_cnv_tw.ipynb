{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type = Ignore\n",
    "#Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from AEModel import AE, MyDataset,AE_T\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "cnv_df= pd.read_csv(\"../../data/master_cnv_df.csv\",encoding = \"UTF-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OR4F5</th>\n",
       "      <th>OR4F29</th>\n",
       "      <th>OR4F16</th>\n",
       "      <th>SAMD11</th>\n",
       "      <th>NOC2L</th>\n",
       "      <th>KLHL17</th>\n",
       "      <th>PLEKHN1</th>\n",
       "      <th>PERM1</th>\n",
       "      <th>HES4</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>...</th>\n",
       "      <th>PRY</th>\n",
       "      <th>BPY2</th>\n",
       "      <th>DAZ1</th>\n",
       "      <th>DAZ2</th>\n",
       "      <th>CDY1B</th>\n",
       "      <th>BPY2B</th>\n",
       "      <th>DAZ3</th>\n",
       "      <th>DAZ4</th>\n",
       "      <th>BPY2C</th>\n",
       "      <th>CDY1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OR4F5  OR4F29  OR4F16  SAMD11  NOC2L  KLHL17  PLEKHN1  PERM1  HES4  ISG15  \\\n",
       "0    3.0     3.0     3.0     3.0    3.0     3.0      3.0    3.0   3.0    3.0   \n",
       "1    2.0     2.0     2.0     2.0    2.0     2.0      2.0    2.0   2.0    2.0   \n",
       "2    3.0     3.0     3.0     3.0    3.0     3.0      3.0    3.0   3.0    3.0   \n",
       "3    2.0     2.0     2.0     2.0    2.0     2.0      2.0    2.0   2.0    2.0   \n",
       "4    2.0     2.0     2.0     2.0    2.0     2.0      2.0    2.0   2.0    2.0   \n",
       "\n",
       "   ...  PRY  BPY2  DAZ1  DAZ2  CDY1B  BPY2B  DAZ3  DAZ4  BPY2C  CDY1  \n",
       "0  ...  1.0   1.0   1.0   1.0    1.0    1.0   1.0   1.0    1.0   1.0  \n",
       "1  ...  0.0   0.0   0.0   0.0    0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "2  ...  2.0   2.0   2.0   2.0    2.0    2.0   2.0   2.0    2.0   2.0  \n",
       "3  ...  0.0   0.0   0.0   0.0    0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "4  ...  0.0   0.0   0.0   0.0    0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 19148 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(gene_df.iloc[:,1:].shape[1])\n",
    "cnv_df.iloc[:,1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is used to load the dataset for training\n",
    "pd_train_dataset, pd_test_dataset = train_test_split(cnv_df.iloc[:,1:], test_size=0.2)\n",
    "\n",
    "X_train_sc = StandardScaler().fit_transform(pd_train_dataset)\n",
    "\n",
    "X_test_sc = StandardScaler().fit_transform(pd_test_dataset)\n",
    "\n",
    "X_full_sc = StandardScaler().fit_transform(cnv_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MyDataset(X_train_sc)\n",
    "data_test = MyDataset(X_test_sc)\n",
    "data_full = MyDataset(X_full_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(data_train, batch_size=50, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE_T(input_shape = len(X_train_sc[0])).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/200, train_loss = 0.209061\n",
      "epoch : 1/200, val_loss = 0.249155\n",
      "epoch : 2/200, train_loss = 0.208816\n",
      "epoch : 2/200, val_loss = 0.248947\n",
      "epoch : 3/200, train_loss = 0.208577\n",
      "epoch : 3/200, val_loss = 0.248623\n",
      "epoch : 4/200, train_loss = 0.208332\n",
      "epoch : 4/200, val_loss = 0.248410\n",
      "epoch : 5/200, train_loss = 0.208095\n",
      "epoch : 5/200, val_loss = 0.248125\n",
      "epoch : 6/200, train_loss = 0.207860\n",
      "epoch : 6/200, val_loss = 0.248021\n",
      "epoch : 7/200, train_loss = 0.207622\n",
      "epoch : 7/200, val_loss = 0.247998\n",
      "epoch : 8/200, train_loss = 0.207398\n",
      "epoch : 8/200, val_loss = 0.248052\n",
      "epoch : 9/200, train_loss = 0.207180\n",
      "epoch : 9/200, val_loss = 0.247917\n",
      "epoch : 10/200, train_loss = 0.206976\n",
      "epoch : 10/200, val_loss = 0.247949\n",
      "epoch : 11/200, train_loss = 0.206744\n",
      "epoch : 11/200, val_loss = 0.248103\n",
      "epoch : 12/200, train_loss = 0.206655\n",
      "epoch : 12/200, val_loss = 0.247525\n",
      "epoch : 13/200, train_loss = 0.206339\n",
      "epoch : 13/200, val_loss = 0.247143\n",
      "epoch : 14/200, train_loss = 0.206100\n",
      "epoch : 14/200, val_loss = 0.247029\n",
      "epoch : 15/200, train_loss = 0.205914\n",
      "epoch : 15/200, val_loss = 0.246651\n",
      "epoch : 16/200, train_loss = 0.205672\n",
      "epoch : 16/200, val_loss = 0.246506\n",
      "epoch : 17/200, train_loss = 0.205446\n",
      "epoch : 17/200, val_loss = 0.246512\n",
      "epoch : 18/200, train_loss = 0.205220\n",
      "epoch : 18/200, val_loss = 0.246327\n",
      "epoch : 19/200, train_loss = 0.204998\n",
      "epoch : 19/200, val_loss = 0.246298\n",
      "epoch : 20/200, train_loss = 0.204766\n",
      "epoch : 20/200, val_loss = 0.246350\n",
      "epoch : 21/200, train_loss = 0.204549\n",
      "epoch : 21/200, val_loss = 0.246238\n",
      "epoch : 22/200, train_loss = 0.204343\n",
      "epoch : 22/200, val_loss = 0.246189\n",
      "epoch : 23/200, train_loss = 0.204128\n",
      "epoch : 23/200, val_loss = 0.245848\n",
      "epoch : 24/200, train_loss = 0.203905\n",
      "epoch : 24/200, val_loss = 0.245432\n",
      "epoch : 25/200, train_loss = 0.203685\n",
      "epoch : 25/200, val_loss = 0.245108\n",
      "epoch : 26/200, train_loss = 0.203479\n",
      "epoch : 26/200, val_loss = 0.245079\n",
      "epoch : 27/200, train_loss = 0.203275\n",
      "epoch : 27/200, val_loss = 0.244918\n",
      "epoch : 28/200, train_loss = 0.203060\n",
      "epoch : 28/200, val_loss = 0.244881\n",
      "epoch : 29/200, train_loss = 0.202853\n",
      "epoch : 29/200, val_loss = 0.244884\n",
      "epoch : 30/200, train_loss = 0.202656\n",
      "epoch : 30/200, val_loss = 0.244948\n",
      "epoch : 31/200, train_loss = 0.202485\n",
      "epoch : 31/200, val_loss = 0.245438\n",
      "epoch : 32/200, train_loss = 0.202428\n",
      "epoch : 32/200, val_loss = 0.244965\n",
      "epoch : 33/200, train_loss = 0.202201\n",
      "epoch : 33/200, val_loss = 0.244545\n",
      "epoch : 34/200, train_loss = 0.201923\n",
      "epoch : 34/200, val_loss = 0.244279\n",
      "epoch : 35/200, train_loss = 0.201687\n",
      "epoch : 35/200, val_loss = 0.244037\n",
      "epoch : 36/200, train_loss = 0.201474\n",
      "epoch : 36/200, val_loss = 0.243779\n",
      "epoch : 37/200, train_loss = 0.201253\n",
      "epoch : 37/200, val_loss = 0.243678\n",
      "epoch : 38/200, train_loss = 0.201045\n",
      "epoch : 38/200, val_loss = 0.243612\n",
      "epoch : 39/200, train_loss = 0.200843\n",
      "epoch : 39/200, val_loss = 0.243724\n",
      "epoch : 40/200, train_loss = 0.200687\n",
      "epoch : 40/200, val_loss = 0.243296\n",
      "epoch : 41/200, train_loss = 0.200478\n",
      "epoch : 41/200, val_loss = 0.243118\n",
      "epoch : 42/200, train_loss = 0.200285\n",
      "epoch : 42/200, val_loss = 0.243235\n",
      "epoch : 43/200, train_loss = 0.200071\n",
      "epoch : 43/200, val_loss = 0.243210\n",
      "epoch : 44/200, train_loss = 0.199875\n",
      "epoch : 44/200, val_loss = 0.243269\n",
      "epoch : 45/200, train_loss = 0.199675\n",
      "epoch : 45/200, val_loss = 0.243100\n",
      "epoch : 46/200, train_loss = 0.199477\n",
      "epoch : 46/200, val_loss = 0.242872\n",
      "epoch : 47/200, train_loss = 0.199287\n",
      "epoch : 47/200, val_loss = 0.242678\n",
      "epoch : 48/200, train_loss = 0.199105\n",
      "epoch : 48/200, val_loss = 0.242453\n",
      "epoch : 49/200, train_loss = 0.198959\n",
      "epoch : 49/200, val_loss = 0.242483\n",
      "epoch : 50/200, train_loss = 0.198821\n",
      "epoch : 50/200, val_loss = 0.242221\n",
      "epoch : 51/200, train_loss = 0.198573\n",
      "epoch : 51/200, val_loss = 0.241912\n",
      "epoch : 52/200, train_loss = 0.198437\n",
      "epoch : 52/200, val_loss = 0.241901\n",
      "epoch : 53/200, train_loss = 0.198293\n",
      "epoch : 53/200, val_loss = 0.241862\n",
      "epoch : 54/200, train_loss = 0.198038\n",
      "epoch : 54/200, val_loss = 0.241635\n",
      "epoch : 55/200, train_loss = 0.197891\n",
      "epoch : 55/200, val_loss = 0.241939\n",
      "epoch : 56/200, train_loss = 0.197668\n",
      "epoch : 56/200, val_loss = 0.241853\n",
      "epoch : 57/200, train_loss = 0.197478\n",
      "epoch : 57/200, val_loss = 0.241718\n",
      "epoch : 58/200, train_loss = 0.197280\n",
      "epoch : 58/200, val_loss = 0.241582\n",
      "epoch : 59/200, train_loss = 0.197107\n",
      "epoch : 59/200, val_loss = 0.241624\n",
      "epoch : 60/200, train_loss = 0.196925\n",
      "epoch : 60/200, val_loss = 0.241639\n",
      "epoch : 61/200, train_loss = 0.196738\n",
      "epoch : 61/200, val_loss = 0.241702\n",
      "epoch : 62/200, train_loss = 0.196572\n",
      "epoch : 62/200, val_loss = 0.241811\n",
      "epoch : 63/200, train_loss = 0.196401\n",
      "epoch : 63/200, val_loss = 0.240927\n",
      "epoch : 64/200, train_loss = 0.196204\n",
      "epoch : 64/200, val_loss = 0.240476\n",
      "epoch : 65/200, train_loss = 0.196023\n",
      "epoch : 65/200, val_loss = 0.240197\n",
      "epoch : 66/200, train_loss = 0.195843\n",
      "epoch : 66/200, val_loss = 0.240074\n",
      "epoch : 67/200, train_loss = 0.195661\n",
      "epoch : 67/200, val_loss = 0.240161\n",
      "epoch : 68/200, train_loss = 0.195495\n",
      "epoch : 68/200, val_loss = 0.240236\n",
      "epoch : 69/200, train_loss = 0.195468\n",
      "epoch : 69/200, val_loss = 0.240501\n",
      "epoch : 70/200, train_loss = 0.195221\n",
      "epoch : 70/200, val_loss = 0.240393\n",
      "epoch : 71/200, train_loss = 0.195019\n",
      "epoch : 71/200, val_loss = 0.240716\n",
      "epoch : 72/200, train_loss = 0.194842\n",
      "epoch : 72/200, val_loss = 0.240743\n",
      "epoch : 73/200, train_loss = 0.194662\n",
      "epoch : 73/200, val_loss = 0.240704\n",
      "epoch : 74/200, train_loss = 0.194492\n",
      "epoch : 74/200, val_loss = 0.240370\n",
      "epoch : 75/200, train_loss = 0.194318\n",
      "epoch : 75/200, val_loss = 0.239845\n",
      "epoch : 76/200, train_loss = 0.194138\n",
      "epoch : 76/200, val_loss = 0.239282\n",
      "epoch : 77/200, train_loss = 0.193960\n",
      "epoch : 77/200, val_loss = 0.239078\n",
      "epoch : 78/200, train_loss = 0.193797\n",
      "epoch : 78/200, val_loss = 0.238971\n",
      "epoch : 79/200, train_loss = 0.193620\n",
      "epoch : 79/200, val_loss = 0.239060\n",
      "epoch : 80/200, train_loss = 0.193474\n",
      "epoch : 80/200, val_loss = 0.239082\n",
      "epoch : 81/200, train_loss = 0.193319\n",
      "epoch : 81/200, val_loss = 0.238827\n",
      "epoch : 82/200, train_loss = 0.193144\n",
      "epoch : 82/200, val_loss = 0.238940\n",
      "epoch : 83/200, train_loss = 0.192972\n",
      "epoch : 83/200, val_loss = 0.239298\n",
      "epoch : 84/200, train_loss = 0.192814\n",
      "epoch : 84/200, val_loss = 0.239604\n",
      "epoch : 85/200, train_loss = 0.192642\n",
      "epoch : 85/200, val_loss = 0.239682\n",
      "epoch : 86/200, train_loss = 0.192511\n",
      "epoch : 86/200, val_loss = 0.239625\n",
      "epoch : 87/200, train_loss = 0.192360\n",
      "epoch : 87/200, val_loss = 0.239165\n",
      "epoch : 88/200, train_loss = 0.192256\n",
      "epoch : 88/200, val_loss = 0.239167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/arnavgupta/Desktop/Comp Medicine Project/CompMed/Python/Autoencoder/Autoencoder_cnv_tw.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arnavgupta/Desktop/Comp%20Medicine%20Project/CompMed/Python/Autoencoder/Autoencoder_cnv_tw.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_loss \u001b[39m=\u001b[39m criterion(outputs, batch_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arnavgupta/Desktop/Comp%20Medicine%20Project/CompMed/Python/Autoencoder/Autoencoder_cnv_tw.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# compute accumulated gradients\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arnavgupta/Desktop/Comp%20Medicine%20Project/CompMed/Python/Autoencoder/Autoencoder_cnv_tw.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arnavgupta/Desktop/Comp%20Medicine%20Project/CompMed/Python/Autoencoder/Autoencoder_cnv_tw.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# perform parameter update based on current gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arnavgupta/Desktop/Comp%20Medicine%20Project/CompMed/Python/Autoencoder/Autoencoder_cnv_tw.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:401\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    394\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    395\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    400\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 401\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:191\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    188\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    193\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "epochs=200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        lol, outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train.append(loss)\n",
    "\n",
    "\n",
    "    #For Valid Loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            code, outputs = model(batch)\n",
    "            loss_val =criterion(outputs, batch)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    losses_val.append(val_loss)\n",
    "\n",
    "\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    print(\"epoch : {}/{}, val_loss = {:.6f}\".format(epoch + 1, epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x147b038b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train, label = 'train_loss')\n",
    "plt.plot(losses_val, label = 'val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(X_full_sc,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Gene Autoencoder Taining\n",
    "\n",
    "data_full = MyDataset(X_full_sc)\n",
    "full_loader = DataLoader(data_full, batch_size=50)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE_T(input_shape = len(X_full_sc[0])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/550, train_loss = 2.119835\n",
      "epoch : 2/550, train_loss = 1.940203\n",
      "epoch : 3/550, train_loss = 1.824407\n",
      "epoch : 4/550, train_loss = 1.721721\n",
      "epoch : 5/550, train_loss = 1.629092\n",
      "epoch : 6/550, train_loss = 1.544775\n",
      "epoch : 7/550, train_loss = 1.465876\n",
      "epoch : 8/550, train_loss = 1.392595\n",
      "epoch : 9/550, train_loss = 1.325414\n",
      "epoch : 10/550, train_loss = 1.263482\n",
      "epoch : 11/550, train_loss = 1.205320\n",
      "epoch : 12/550, train_loss = 1.152598\n",
      "epoch : 13/550, train_loss = 1.104545\n",
      "epoch : 14/550, train_loss = 1.061457\n",
      "epoch : 15/550, train_loss = 1.023130\n",
      "epoch : 16/550, train_loss = 0.989158\n",
      "epoch : 17/550, train_loss = 0.958668\n",
      "epoch : 18/550, train_loss = 0.931087\n",
      "epoch : 19/550, train_loss = 0.905874\n",
      "epoch : 20/550, train_loss = 0.883030\n",
      "epoch : 21/550, train_loss = 0.862261\n",
      "epoch : 22/550, train_loss = 0.843371\n",
      "epoch : 23/550, train_loss = 0.826222\n",
      "epoch : 24/550, train_loss = 0.810659\n",
      "epoch : 25/550, train_loss = 0.796637\n",
      "epoch : 26/550, train_loss = 0.784134\n",
      "epoch : 27/550, train_loss = 0.772908\n",
      "epoch : 28/550, train_loss = 0.762957\n",
      "epoch : 29/550, train_loss = 0.753860\n",
      "epoch : 30/550, train_loss = 0.745661\n",
      "epoch : 31/550, train_loss = 0.738317\n",
      "epoch : 32/550, train_loss = 0.731708\n",
      "epoch : 33/550, train_loss = 0.725718\n",
      "epoch : 34/550, train_loss = 0.720242\n",
      "epoch : 35/550, train_loss = 0.715182\n",
      "epoch : 36/550, train_loss = 0.710548\n",
      "epoch : 37/550, train_loss = 0.706307\n",
      "epoch : 38/550, train_loss = 0.702364\n",
      "epoch : 39/550, train_loss = 0.698732\n",
      "epoch : 40/550, train_loss = 0.695323\n",
      "epoch : 41/550, train_loss = 0.692092\n",
      "epoch : 42/550, train_loss = 0.689052\n",
      "epoch : 43/550, train_loss = 0.686207\n",
      "epoch : 44/550, train_loss = 0.683535\n",
      "epoch : 45/550, train_loss = 0.681021\n",
      "epoch : 46/550, train_loss = 0.678662\n",
      "epoch : 47/550, train_loss = 0.676440\n",
      "epoch : 48/550, train_loss = 0.674326\n",
      "epoch : 49/550, train_loss = 0.672309\n",
      "epoch : 50/550, train_loss = 0.670378\n",
      "epoch : 51/550, train_loss = 0.668527\n",
      "epoch : 52/550, train_loss = 0.666746\n",
      "epoch : 53/550, train_loss = 0.665025\n",
      "epoch : 54/550, train_loss = 0.663345\n",
      "epoch : 55/550, train_loss = 0.661719\n",
      "epoch : 56/550, train_loss = 0.660130\n",
      "epoch : 57/550, train_loss = 0.658564\n",
      "epoch : 58/550, train_loss = 0.657019\n",
      "epoch : 59/550, train_loss = 0.655497\n",
      "epoch : 60/550, train_loss = 0.653990\n",
      "epoch : 61/550, train_loss = 0.652505\n",
      "epoch : 62/550, train_loss = 0.651050\n",
      "epoch : 63/550, train_loss = 0.649647\n",
      "epoch : 64/550, train_loss = 0.648250\n",
      "epoch : 65/550, train_loss = 0.646835\n",
      "epoch : 66/550, train_loss = 0.645425\n",
      "epoch : 67/550, train_loss = 0.644045\n",
      "epoch : 68/550, train_loss = 0.642686\n",
      "epoch : 69/550, train_loss = 0.641349\n",
      "epoch : 70/550, train_loss = 0.640030\n",
      "epoch : 71/550, train_loss = 0.638728\n",
      "epoch : 72/550, train_loss = 0.637440\n",
      "epoch : 73/550, train_loss = 0.636165\n",
      "epoch : 74/550, train_loss = 0.634903\n",
      "epoch : 75/550, train_loss = 0.633653\n",
      "epoch : 76/550, train_loss = 0.632416\n",
      "epoch : 77/550, train_loss = 0.631203\n",
      "epoch : 78/550, train_loss = 0.629987\n",
      "epoch : 79/550, train_loss = 0.628783\n",
      "epoch : 80/550, train_loss = 0.627584\n",
      "epoch : 81/550, train_loss = 0.626391\n",
      "epoch : 82/550, train_loss = 0.625202\n",
      "epoch : 83/550, train_loss = 0.624014\n",
      "epoch : 84/550, train_loss = 0.622828\n",
      "epoch : 85/550, train_loss = 0.621643\n",
      "epoch : 86/550, train_loss = 0.620459\n",
      "epoch : 87/550, train_loss = 0.619278\n",
      "epoch : 88/550, train_loss = 0.618096\n",
      "epoch : 89/550, train_loss = 0.616912\n",
      "epoch : 90/550, train_loss = 0.615727\n",
      "epoch : 91/550, train_loss = 0.614536\n",
      "epoch : 92/550, train_loss = 0.613340\n",
      "epoch : 93/550, train_loss = 0.612142\n",
      "epoch : 94/550, train_loss = 0.610935\n",
      "epoch : 95/550, train_loss = 0.609724\n",
      "epoch : 96/550, train_loss = 0.608512\n",
      "epoch : 97/550, train_loss = 0.607287\n",
      "epoch : 98/550, train_loss = 0.606069\n",
      "epoch : 99/550, train_loss = 0.604832\n",
      "epoch : 100/550, train_loss = 0.603593\n",
      "epoch : 101/550, train_loss = 0.602346\n",
      "epoch : 102/550, train_loss = 0.601091\n",
      "epoch : 103/550, train_loss = 0.599829\n",
      "epoch : 104/550, train_loss = 0.598619\n",
      "epoch : 105/550, train_loss = 0.597330\n",
      "epoch : 106/550, train_loss = 0.596037\n",
      "epoch : 107/550, train_loss = 0.594735\n",
      "epoch : 108/550, train_loss = 0.593424\n",
      "epoch : 109/550, train_loss = 0.592104\n",
      "epoch : 110/550, train_loss = 0.590773\n",
      "epoch : 111/550, train_loss = 0.589430\n",
      "epoch : 112/550, train_loss = 0.588092\n",
      "epoch : 113/550, train_loss = 0.586723\n",
      "epoch : 114/550, train_loss = 0.585343\n",
      "epoch : 115/550, train_loss = 0.583991\n",
      "epoch : 116/550, train_loss = 0.582577\n",
      "epoch : 117/550, train_loss = 0.581158\n",
      "epoch : 118/550, train_loss = 0.579729\n",
      "epoch : 119/550, train_loss = 0.578293\n",
      "epoch : 120/550, train_loss = 0.576849\n",
      "epoch : 121/550, train_loss = 0.575395\n",
      "epoch : 122/550, train_loss = 0.573931\n",
      "epoch : 123/550, train_loss = 0.572460\n",
      "epoch : 124/550, train_loss = 0.570982\n",
      "epoch : 125/550, train_loss = 0.569495\n",
      "epoch : 126/550, train_loss = 0.568001\n",
      "epoch : 127/550, train_loss = 0.566500\n",
      "epoch : 128/550, train_loss = 0.564989\n",
      "epoch : 129/550, train_loss = 0.563472\n",
      "epoch : 130/550, train_loss = 0.561942\n",
      "epoch : 131/550, train_loss = 0.560403\n",
      "epoch : 132/550, train_loss = 0.558855\n",
      "epoch : 133/550, train_loss = 0.557301\n",
      "epoch : 134/550, train_loss = 0.555736\n",
      "epoch : 135/550, train_loss = 0.554161\n",
      "epoch : 136/550, train_loss = 0.552579\n",
      "epoch : 137/550, train_loss = 0.550992\n",
      "epoch : 138/550, train_loss = 0.549398\n",
      "epoch : 139/550, train_loss = 0.547799\n",
      "epoch : 140/550, train_loss = 0.546194\n",
      "epoch : 141/550, train_loss = 0.544587\n",
      "epoch : 142/550, train_loss = 0.542975\n",
      "epoch : 143/550, train_loss = 0.541358\n",
      "epoch : 144/550, train_loss = 0.539734\n",
      "epoch : 145/550, train_loss = 0.538114\n",
      "epoch : 146/550, train_loss = 0.536519\n",
      "epoch : 147/550, train_loss = 0.534879\n",
      "epoch : 148/550, train_loss = 0.533246\n",
      "epoch : 149/550, train_loss = 0.531606\n",
      "epoch : 150/550, train_loss = 0.529965\n",
      "epoch : 151/550, train_loss = 0.528330\n",
      "epoch : 152/550, train_loss = 0.526693\n",
      "epoch : 153/550, train_loss = 0.525060\n",
      "epoch : 154/550, train_loss = 0.523428\n",
      "epoch : 155/550, train_loss = 0.521802\n",
      "epoch : 156/550, train_loss = 0.520189\n",
      "epoch : 157/550, train_loss = 0.518569\n",
      "epoch : 158/550, train_loss = 0.516954\n",
      "epoch : 159/550, train_loss = 0.515342\n",
      "epoch : 160/550, train_loss = 0.513732\n",
      "epoch : 161/550, train_loss = 0.512130\n",
      "epoch : 162/550, train_loss = 0.510532\n",
      "epoch : 163/550, train_loss = 0.508939\n",
      "epoch : 164/550, train_loss = 0.507353\n",
      "epoch : 165/550, train_loss = 0.505772\n",
      "epoch : 166/550, train_loss = 0.504198\n",
      "epoch : 167/550, train_loss = 0.502630\n",
      "epoch : 168/550, train_loss = 0.501070\n",
      "epoch : 169/550, train_loss = 0.499516\n",
      "epoch : 170/550, train_loss = 0.497969\n",
      "epoch : 171/550, train_loss = 0.496428\n",
      "epoch : 172/550, train_loss = 0.494895\n",
      "epoch : 173/550, train_loss = 0.493369\n",
      "epoch : 174/550, train_loss = 0.491850\n",
      "epoch : 175/550, train_loss = 0.490341\n",
      "epoch : 176/550, train_loss = 0.488837\n",
      "epoch : 177/550, train_loss = 0.487341\n",
      "epoch : 178/550, train_loss = 0.485850\n",
      "epoch : 179/550, train_loss = 0.484363\n",
      "epoch : 180/550, train_loss = 0.482881\n",
      "epoch : 181/550, train_loss = 0.481406\n",
      "epoch : 182/550, train_loss = 0.479933\n",
      "epoch : 183/550, train_loss = 0.478465\n",
      "epoch : 184/550, train_loss = 0.476999\n",
      "epoch : 185/550, train_loss = 0.475542\n",
      "epoch : 186/550, train_loss = 0.474087\n",
      "epoch : 187/550, train_loss = 0.472642\n",
      "epoch : 188/550, train_loss = 0.471200\n",
      "epoch : 189/550, train_loss = 0.469769\n",
      "epoch : 190/550, train_loss = 0.468344\n",
      "epoch : 191/550, train_loss = 0.466924\n",
      "epoch : 192/550, train_loss = 0.465509\n",
      "epoch : 193/550, train_loss = 0.464103\n",
      "epoch : 194/550, train_loss = 0.462701\n",
      "epoch : 195/550, train_loss = 0.461308\n",
      "epoch : 196/550, train_loss = 0.459920\n",
      "epoch : 197/550, train_loss = 0.458543\n",
      "epoch : 198/550, train_loss = 0.457170\n",
      "epoch : 199/550, train_loss = 0.455807\n",
      "epoch : 200/550, train_loss = 0.454443\n",
      "epoch : 201/550, train_loss = 0.453090\n",
      "epoch : 202/550, train_loss = 0.451740\n",
      "epoch : 203/550, train_loss = 0.450401\n",
      "epoch : 204/550, train_loss = 0.449060\n",
      "epoch : 205/550, train_loss = 0.447739\n",
      "epoch : 206/550, train_loss = 0.446410\n",
      "epoch : 207/550, train_loss = 0.445089\n",
      "epoch : 208/550, train_loss = 0.443769\n",
      "epoch : 209/550, train_loss = 0.442458\n",
      "epoch : 210/550, train_loss = 0.441144\n",
      "epoch : 211/550, train_loss = 0.439842\n",
      "epoch : 212/550, train_loss = 0.438539\n",
      "epoch : 213/550, train_loss = 0.437243\n",
      "epoch : 214/550, train_loss = 0.435947\n",
      "epoch : 215/550, train_loss = 0.434663\n",
      "epoch : 216/550, train_loss = 0.433377\n",
      "epoch : 217/550, train_loss = 0.432104\n",
      "epoch : 218/550, train_loss = 0.430830\n",
      "epoch : 219/550, train_loss = 0.429566\n",
      "epoch : 220/550, train_loss = 0.428301\n",
      "epoch : 221/550, train_loss = 0.427046\n",
      "epoch : 222/550, train_loss = 0.425790\n",
      "epoch : 223/550, train_loss = 0.424542\n",
      "epoch : 224/550, train_loss = 0.423295\n",
      "epoch : 225/550, train_loss = 0.422051\n",
      "epoch : 226/550, train_loss = 0.420804\n",
      "epoch : 227/550, train_loss = 0.419580\n",
      "epoch : 228/550, train_loss = 0.418333\n",
      "epoch : 229/550, train_loss = 0.417096\n",
      "epoch : 230/550, train_loss = 0.415860\n",
      "epoch : 231/550, train_loss = 0.414631\n",
      "epoch : 232/550, train_loss = 0.413411\n",
      "epoch : 233/550, train_loss = 0.412187\n",
      "epoch : 234/550, train_loss = 0.410967\n",
      "epoch : 235/550, train_loss = 0.409755\n",
      "epoch : 236/550, train_loss = 0.408541\n",
      "epoch : 237/550, train_loss = 0.407334\n",
      "epoch : 238/550, train_loss = 0.406127\n",
      "epoch : 239/550, train_loss = 0.404926\n",
      "epoch : 240/550, train_loss = 0.403727\n",
      "epoch : 241/550, train_loss = 0.402534\n",
      "epoch : 242/550, train_loss = 0.401345\n",
      "epoch : 243/550, train_loss = 0.400163\n",
      "epoch : 244/550, train_loss = 0.398982\n",
      "epoch : 245/550, train_loss = 0.397808\n",
      "epoch : 246/550, train_loss = 0.396640\n",
      "epoch : 247/550, train_loss = 0.395473\n",
      "epoch : 248/550, train_loss = 0.394310\n",
      "epoch : 249/550, train_loss = 0.393184\n",
      "epoch : 250/550, train_loss = 0.392016\n",
      "epoch : 251/550, train_loss = 0.390867\n",
      "epoch : 252/550, train_loss = 0.389726\n",
      "epoch : 253/550, train_loss = 0.388593\n",
      "epoch : 254/550, train_loss = 0.387471\n",
      "epoch : 255/550, train_loss = 0.386357\n",
      "epoch : 256/550, train_loss = 0.385253\n",
      "epoch : 257/550, train_loss = 0.384155\n",
      "epoch : 258/550, train_loss = 0.383065\n",
      "epoch : 259/550, train_loss = 0.381980\n",
      "epoch : 260/550, train_loss = 0.380905\n",
      "epoch : 261/550, train_loss = 0.379854\n",
      "epoch : 262/550, train_loss = 0.378794\n",
      "epoch : 263/550, train_loss = 0.377737\n",
      "epoch : 264/550, train_loss = 0.376693\n",
      "epoch : 265/550, train_loss = 0.375661\n",
      "epoch : 266/550, train_loss = 0.374637\n",
      "epoch : 267/550, train_loss = 0.373617\n",
      "epoch : 268/550, train_loss = 0.372608\n",
      "epoch : 269/550, train_loss = 0.371611\n",
      "epoch : 270/550, train_loss = 0.370623\n",
      "epoch : 271/550, train_loss = 0.369646\n",
      "epoch : 272/550, train_loss = 0.368678\n",
      "epoch : 273/550, train_loss = 0.367717\n",
      "epoch : 274/550, train_loss = 0.366766\n",
      "epoch : 275/550, train_loss = 0.365825\n",
      "epoch : 276/550, train_loss = 0.364893\n",
      "epoch : 277/550, train_loss = 0.363968\n",
      "epoch : 278/550, train_loss = 0.363084\n",
      "epoch : 279/550, train_loss = 0.362192\n",
      "epoch : 280/550, train_loss = 0.361250\n",
      "epoch : 281/550, train_loss = 0.360351\n",
      "epoch : 282/550, train_loss = 0.359469\n",
      "epoch : 283/550, train_loss = 0.358598\n",
      "epoch : 284/550, train_loss = 0.357738\n",
      "epoch : 285/550, train_loss = 0.356885\n",
      "epoch : 286/550, train_loss = 0.356039\n",
      "epoch : 287/550, train_loss = 0.355202\n",
      "epoch : 288/550, train_loss = 0.354372\n",
      "epoch : 289/550, train_loss = 0.353552\n",
      "epoch : 290/550, train_loss = 0.352737\n",
      "epoch : 291/550, train_loss = 0.351932\n",
      "epoch : 292/550, train_loss = 0.351130\n",
      "epoch : 293/550, train_loss = 0.350338\n",
      "epoch : 294/550, train_loss = 0.349551\n",
      "epoch : 295/550, train_loss = 0.348772\n",
      "epoch : 296/550, train_loss = 0.347999\n",
      "epoch : 297/550, train_loss = 0.347234\n",
      "epoch : 298/550, train_loss = 0.346471\n",
      "epoch : 299/550, train_loss = 0.345722\n",
      "epoch : 300/550, train_loss = 0.344977\n",
      "epoch : 301/550, train_loss = 0.344240\n",
      "epoch : 302/550, train_loss = 0.343508\n",
      "epoch : 303/550, train_loss = 0.342784\n",
      "epoch : 304/550, train_loss = 0.342065\n",
      "epoch : 305/550, train_loss = 0.341360\n",
      "epoch : 306/550, train_loss = 0.340664\n",
      "epoch : 307/550, train_loss = 0.339974\n",
      "epoch : 308/550, train_loss = 0.339283\n",
      "epoch : 309/550, train_loss = 0.338603\n",
      "epoch : 310/550, train_loss = 0.337927\n",
      "epoch : 311/550, train_loss = 0.337262\n",
      "epoch : 312/550, train_loss = 0.336600\n",
      "epoch : 313/550, train_loss = 0.335949\n",
      "epoch : 314/550, train_loss = 0.335457\n",
      "epoch : 315/550, train_loss = 0.334734\n",
      "epoch : 316/550, train_loss = 0.334052\n",
      "epoch : 317/550, train_loss = 0.333419\n",
      "epoch : 318/550, train_loss = 0.332789\n",
      "epoch : 319/550, train_loss = 0.332177\n",
      "epoch : 320/550, train_loss = 0.331566\n",
      "epoch : 321/550, train_loss = 0.330964\n",
      "epoch : 322/550, train_loss = 0.330365\n",
      "epoch : 323/550, train_loss = 0.329776\n",
      "epoch : 324/550, train_loss = 0.329190\n",
      "epoch : 325/550, train_loss = 0.328610\n",
      "epoch : 326/550, train_loss = 0.328034\n",
      "epoch : 327/550, train_loss = 0.327464\n",
      "epoch : 328/550, train_loss = 0.326895\n",
      "epoch : 329/550, train_loss = 0.326335\n",
      "epoch : 330/550, train_loss = 0.325774\n",
      "epoch : 331/550, train_loss = 0.325222\n",
      "epoch : 332/550, train_loss = 0.324670\n",
      "epoch : 333/550, train_loss = 0.324128\n",
      "epoch : 334/550, train_loss = 0.323598\n",
      "epoch : 335/550, train_loss = 0.323061\n",
      "epoch : 336/550, train_loss = 0.322525\n",
      "epoch : 337/550, train_loss = 0.321995\n",
      "epoch : 338/550, train_loss = 0.321470\n",
      "epoch : 339/550, train_loss = 0.320945\n",
      "epoch : 340/550, train_loss = 0.320422\n",
      "epoch : 341/550, train_loss = 0.319902\n",
      "epoch : 342/550, train_loss = 0.319383\n",
      "epoch : 343/550, train_loss = 0.318876\n",
      "epoch : 344/550, train_loss = 0.318363\n",
      "epoch : 345/550, train_loss = 0.317854\n",
      "epoch : 346/550, train_loss = 0.317347\n",
      "epoch : 347/550, train_loss = 0.316847\n",
      "epoch : 348/550, train_loss = 0.316359\n",
      "epoch : 349/550, train_loss = 0.315861\n",
      "epoch : 350/550, train_loss = 0.315359\n",
      "epoch : 351/550, train_loss = 0.314903\n",
      "epoch : 352/550, train_loss = 0.314447\n",
      "epoch : 353/550, train_loss = 0.313975\n",
      "epoch : 354/550, train_loss = 0.313453\n",
      "epoch : 355/550, train_loss = 0.312954\n",
      "epoch : 356/550, train_loss = 0.312464\n",
      "epoch : 357/550, train_loss = 0.311987\n",
      "epoch : 358/550, train_loss = 0.311507\n",
      "epoch : 359/550, train_loss = 0.311037\n",
      "epoch : 360/550, train_loss = 0.310565\n",
      "epoch : 361/550, train_loss = 0.310101\n",
      "epoch : 362/550, train_loss = 0.309635\n",
      "epoch : 363/550, train_loss = 0.309174\n",
      "epoch : 364/550, train_loss = 0.308716\n",
      "epoch : 365/550, train_loss = 0.308261\n",
      "epoch : 366/550, train_loss = 0.307804\n",
      "epoch : 367/550, train_loss = 0.307348\n",
      "epoch : 368/550, train_loss = 0.306896\n",
      "epoch : 369/550, train_loss = 0.306445\n",
      "epoch : 370/550, train_loss = 0.305997\n",
      "epoch : 371/550, train_loss = 0.305551\n",
      "epoch : 372/550, train_loss = 0.305111\n",
      "epoch : 373/550, train_loss = 0.304666\n",
      "epoch : 374/550, train_loss = 0.304226\n",
      "epoch : 375/550, train_loss = 0.303789\n",
      "epoch : 376/550, train_loss = 0.303350\n",
      "epoch : 377/550, train_loss = 0.302914\n",
      "epoch : 378/550, train_loss = 0.302480\n",
      "epoch : 379/550, train_loss = 0.302258\n",
      "epoch : 380/550, train_loss = 0.301688\n",
      "epoch : 381/550, train_loss = 0.301217\n",
      "epoch : 382/550, train_loss = 0.300781\n",
      "epoch : 383/550, train_loss = 0.300353\n",
      "epoch : 384/550, train_loss = 0.299930\n",
      "epoch : 385/550, train_loss = 0.299509\n",
      "epoch : 386/550, train_loss = 0.299097\n",
      "epoch : 387/550, train_loss = 0.298683\n",
      "epoch : 388/550, train_loss = 0.298273\n",
      "epoch : 389/550, train_loss = 0.297862\n",
      "epoch : 390/550, train_loss = 0.297447\n",
      "epoch : 391/550, train_loss = 0.297043\n",
      "epoch : 392/550, train_loss = 0.296637\n",
      "epoch : 393/550, train_loss = 0.296242\n",
      "epoch : 394/550, train_loss = 0.295840\n",
      "epoch : 395/550, train_loss = 0.295445\n",
      "epoch : 396/550, train_loss = 0.295047\n",
      "epoch : 397/550, train_loss = 0.294657\n",
      "epoch : 398/550, train_loss = 0.294354\n",
      "epoch : 399/550, train_loss = 0.293961\n",
      "epoch : 400/550, train_loss = 0.293534\n",
      "epoch : 401/550, train_loss = 0.293137\n",
      "epoch : 402/550, train_loss = 0.292747\n",
      "epoch : 403/550, train_loss = 0.292366\n",
      "epoch : 404/550, train_loss = 0.291990\n",
      "epoch : 405/550, train_loss = 0.291612\n",
      "epoch : 406/550, train_loss = 0.291239\n",
      "epoch : 407/550, train_loss = 0.290863\n",
      "epoch : 408/550, train_loss = 0.290495\n",
      "epoch : 409/550, train_loss = 0.290120\n",
      "epoch : 410/550, train_loss = 0.289752\n",
      "epoch : 411/550, train_loss = 0.289408\n",
      "epoch : 412/550, train_loss = 0.289047\n",
      "epoch : 413/550, train_loss = 0.288682\n",
      "epoch : 414/550, train_loss = 0.288313\n",
      "epoch : 415/550, train_loss = 0.287955\n",
      "epoch : 416/550, train_loss = 0.287596\n",
      "epoch : 417/550, train_loss = 0.287239\n",
      "epoch : 418/550, train_loss = 0.286888\n",
      "epoch : 419/550, train_loss = 0.286539\n",
      "epoch : 420/550, train_loss = 0.286190\n",
      "epoch : 421/550, train_loss = 0.285840\n",
      "epoch : 422/550, train_loss = 0.285507\n",
      "epoch : 423/550, train_loss = 0.285160\n",
      "epoch : 424/550, train_loss = 0.284835\n",
      "epoch : 425/550, train_loss = 0.284515\n",
      "epoch : 426/550, train_loss = 0.284179\n",
      "epoch : 427/550, train_loss = 0.283842\n",
      "epoch : 428/550, train_loss = 0.283517\n",
      "epoch : 429/550, train_loss = 0.283188\n",
      "epoch : 430/550, train_loss = 0.282870\n",
      "epoch : 431/550, train_loss = 0.282549\n",
      "epoch : 432/550, train_loss = 0.282225\n",
      "epoch : 433/550, train_loss = 0.281912\n",
      "epoch : 434/550, train_loss = 0.281595\n",
      "epoch : 435/550, train_loss = 0.281361\n",
      "epoch : 436/550, train_loss = 0.280997\n",
      "epoch : 437/550, train_loss = 0.280662\n",
      "epoch : 438/550, train_loss = 0.280355\n",
      "epoch : 439/550, train_loss = 0.280041\n",
      "epoch : 440/550, train_loss = 0.279730\n",
      "epoch : 441/550, train_loss = 0.279415\n",
      "epoch : 442/550, train_loss = 0.279104\n",
      "epoch : 443/550, train_loss = 0.278794\n",
      "epoch : 444/550, train_loss = 0.278488\n",
      "epoch : 445/550, train_loss = 0.278189\n",
      "epoch : 446/550, train_loss = 0.277888\n",
      "epoch : 447/550, train_loss = 0.277804\n",
      "epoch : 448/550, train_loss = 0.277492\n",
      "epoch : 449/550, train_loss = 0.277348\n",
      "epoch : 450/550, train_loss = 0.276962\n",
      "epoch : 451/550, train_loss = 0.276564\n",
      "epoch : 452/550, train_loss = 0.276230\n",
      "epoch : 453/550, train_loss = 0.275951\n",
      "epoch : 454/550, train_loss = 0.275658\n",
      "epoch : 455/550, train_loss = 0.275371\n",
      "epoch : 456/550, train_loss = 0.275086\n",
      "epoch : 457/550, train_loss = 0.274806\n",
      "epoch : 458/550, train_loss = 0.274545\n",
      "epoch : 459/550, train_loss = 0.274278\n",
      "epoch : 460/550, train_loss = 0.273980\n",
      "epoch : 461/550, train_loss = 0.273698\n",
      "epoch : 462/550, train_loss = 0.273449\n",
      "epoch : 463/550, train_loss = 0.273178\n",
      "epoch : 464/550, train_loss = 0.272891\n",
      "epoch : 465/550, train_loss = 0.272600\n",
      "epoch : 466/550, train_loss = 0.272336\n",
      "epoch : 467/550, train_loss = 0.272061\n",
      "epoch : 468/550, train_loss = 0.271804\n",
      "epoch : 469/550, train_loss = 0.271548\n",
      "epoch : 470/550, train_loss = 0.271306\n",
      "epoch : 471/550, train_loss = 0.271057\n",
      "epoch : 472/550, train_loss = 0.270810\n",
      "epoch : 473/550, train_loss = 0.270561\n",
      "epoch : 474/550, train_loss = 0.270315\n",
      "epoch : 475/550, train_loss = 0.270042\n",
      "epoch : 476/550, train_loss = 0.269789\n",
      "epoch : 477/550, train_loss = 0.269534\n",
      "epoch : 478/550, train_loss = 0.269292\n",
      "epoch : 479/550, train_loss = 0.269025\n",
      "epoch : 480/550, train_loss = 0.268784\n",
      "epoch : 481/550, train_loss = 0.268528\n",
      "epoch : 482/550, train_loss = 0.268288\n",
      "epoch : 483/550, train_loss = 0.268041\n",
      "epoch : 484/550, train_loss = 0.267806\n",
      "epoch : 485/550, train_loss = 0.267564\n",
      "epoch : 486/550, train_loss = 0.267324\n",
      "epoch : 487/550, train_loss = 0.267145\n",
      "epoch : 488/550, train_loss = 0.267256\n",
      "epoch : 489/550, train_loss = 0.267052\n",
      "epoch : 490/550, train_loss = 0.267077\n",
      "epoch : 491/550, train_loss = 0.266580\n",
      "epoch : 492/550, train_loss = 0.266782\n",
      "epoch : 493/550, train_loss = 0.266144\n",
      "epoch : 494/550, train_loss = 0.265715\n",
      "epoch : 495/550, train_loss = 0.265471\n",
      "epoch : 496/550, train_loss = 0.265235\n",
      "epoch : 497/550, train_loss = 0.265006\n",
      "epoch : 498/550, train_loss = 0.264745\n",
      "epoch : 499/550, train_loss = 0.264504\n",
      "epoch : 500/550, train_loss = 0.264278\n",
      "epoch : 501/550, train_loss = 0.264053\n",
      "epoch : 502/550, train_loss = 0.263843\n",
      "epoch : 503/550, train_loss = 0.263621\n",
      "epoch : 504/550, train_loss = 0.263407\n",
      "epoch : 505/550, train_loss = 0.263255\n",
      "epoch : 506/550, train_loss = 0.263007\n",
      "epoch : 507/550, train_loss = 0.262777\n",
      "epoch : 508/550, train_loss = 0.262556\n",
      "epoch : 509/550, train_loss = 0.262332\n",
      "epoch : 510/550, train_loss = 0.262183\n",
      "epoch : 511/550, train_loss = 0.261953\n",
      "epoch : 512/550, train_loss = 0.261742\n",
      "epoch : 513/550, train_loss = 0.261544\n",
      "epoch : 514/550, train_loss = 0.261368\n",
      "epoch : 515/550, train_loss = 0.261189\n",
      "epoch : 516/550, train_loss = 0.261020\n",
      "epoch : 517/550, train_loss = 0.260819\n",
      "epoch : 518/550, train_loss = 0.260592\n",
      "epoch : 519/550, train_loss = 0.260363\n",
      "epoch : 520/550, train_loss = 0.260175\n",
      "epoch : 521/550, train_loss = 0.259963\n",
      "epoch : 522/550, train_loss = 0.259773\n",
      "epoch : 523/550, train_loss = 0.259644\n",
      "epoch : 524/550, train_loss = 0.259548\n",
      "epoch : 525/550, train_loss = 0.259294\n",
      "epoch : 526/550, train_loss = 0.259038\n",
      "epoch : 527/550, train_loss = 0.258812\n",
      "epoch : 528/550, train_loss = 0.258589\n",
      "epoch : 529/550, train_loss = 0.258395\n",
      "epoch : 530/550, train_loss = 0.258190\n",
      "epoch : 531/550, train_loss = 0.257991\n",
      "epoch : 532/550, train_loss = 0.258016\n",
      "epoch : 533/550, train_loss = 0.258273\n",
      "epoch : 534/550, train_loss = 0.257930\n",
      "epoch : 535/550, train_loss = 0.257716\n",
      "epoch : 536/550, train_loss = 0.257369\n",
      "epoch : 537/550, train_loss = 0.257025\n",
      "epoch : 538/550, train_loss = 0.256772\n",
      "epoch : 539/550, train_loss = 0.256555\n",
      "epoch : 540/550, train_loss = 0.256350\n",
      "epoch : 541/550, train_loss = 0.256184\n",
      "epoch : 542/550, train_loss = 0.256063\n",
      "epoch : 543/550, train_loss = 0.255881\n",
      "epoch : 544/550, train_loss = 0.256778\n",
      "epoch : 545/550, train_loss = 0.256328\n",
      "epoch : 546/550, train_loss = 0.255661\n",
      "epoch : 547/550, train_loss = 0.255322\n",
      "epoch : 548/550, train_loss = 0.255060\n",
      "epoch : 549/550, train_loss = 0.254839\n",
      "epoch : 550/550, train_loss = 0.254637\n"
     ]
    }
   ],
   "source": [
    "losses_train_final = []\n",
    "\n",
    "epochs=550\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in full_loader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        code, outputs = model(batch_features)\n",
    "        \n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train_final.append(loss)\n",
    "\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out,out2 = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41fc6849-724a-4bac-8775-5e703fe74184</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>1.709592</td>\n",
       "      <td>1.802925</td>\n",
       "      <td>0.864535</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>1.251312</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>0.529081</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063729</td>\n",
       "      <td>-0.070066</td>\n",
       "      <td>-0.001569</td>\n",
       "      <td>-1.816005</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>-2.394009</td>\n",
       "      <td>1.257136</td>\n",
       "      <td>0.394306</td>\n",
       "      <td>-1.513282</td>\n",
       "      <td>2.038277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0f6b347c-30da-495b-b103-36af30df77d7</td>\n",
       "      <td>0.701019</td>\n",
       "      <td>1.634090</td>\n",
       "      <td>1.713905</td>\n",
       "      <td>1.874578</td>\n",
       "      <td>0.944521</td>\n",
       "      <td>-1.083194</td>\n",
       "      <td>0.259510</td>\n",
       "      <td>0.460175</td>\n",
       "      <td>0.286948</td>\n",
       "      <td>...</td>\n",
       "      <td>1.075076</td>\n",
       "      <td>-0.890278</td>\n",
       "      <td>-0.679742</td>\n",
       "      <td>-1.816005</td>\n",
       "      <td>-0.602386</td>\n",
       "      <td>-2.748668</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>0.989450</td>\n",
       "      <td>-1.030964</td>\n",
       "      <td>2.194542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6568ed2b-0018-4750-9b3b-6c414dba60ae</td>\n",
       "      <td>0.482779</td>\n",
       "      <td>0.834642</td>\n",
       "      <td>1.762489</td>\n",
       "      <td>1.899293</td>\n",
       "      <td>0.802337</td>\n",
       "      <td>-0.077112</td>\n",
       "      <td>-0.252213</td>\n",
       "      <td>0.776146</td>\n",
       "      <td>0.537715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965852</td>\n",
       "      <td>0.545567</td>\n",
       "      <td>1.027019</td>\n",
       "      <td>-1.816005</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>-2.467926</td>\n",
       "      <td>1.354587</td>\n",
       "      <td>1.037515</td>\n",
       "      <td>-1.744227</td>\n",
       "      <td>1.812896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e18b7869-0049-4be7-9611-d08db28df33d</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>-0.117965</td>\n",
       "      <td>1.499648</td>\n",
       "      <td>1.796704</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>-0.707842</td>\n",
       "      <td>0.717963</td>\n",
       "      <td>-0.450479</td>\n",
       "      <td>-0.127403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703502</td>\n",
       "      <td>-0.826764</td>\n",
       "      <td>-0.335318</td>\n",
       "      <td>-0.642262</td>\n",
       "      <td>-0.153610</td>\n",
       "      <td>-2.529420</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>0.609665</td>\n",
       "      <td>-1.030876</td>\n",
       "      <td>2.052207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>361cc367-f85d-402a-a19d-999d33f7667a</td>\n",
       "      <td>-0.230597</td>\n",
       "      <td>0.136987</td>\n",
       "      <td>1.499648</td>\n",
       "      <td>1.796704</td>\n",
       "      <td>0.142297</td>\n",
       "      <td>-0.392801</td>\n",
       "      <td>0.524221</td>\n",
       "      <td>0.057456</td>\n",
       "      <td>-0.127403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753804</td>\n",
       "      <td>-1.121887</td>\n",
       "      <td>0.059129</td>\n",
       "      <td>-1.010980</td>\n",
       "      <td>-0.244771</td>\n",
       "      <td>-2.726314</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>-0.026577</td>\n",
       "      <td>-0.617281</td>\n",
       "      <td>2.280982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>41811e19-d6bb-4f1c-ac67-9df06ad9f85b</td>\n",
       "      <td>0.104792</td>\n",
       "      <td>0.337378</td>\n",
       "      <td>1.499648</td>\n",
       "      <td>1.796704</td>\n",
       "      <td>-0.351197</td>\n",
       "      <td>-0.798211</td>\n",
       "      <td>-0.298172</td>\n",
       "      <td>-0.230740</td>\n",
       "      <td>-0.127403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574850</td>\n",
       "      <td>-1.200839</td>\n",
       "      <td>0.263114</td>\n",
       "      <td>-0.682506</td>\n",
       "      <td>-0.243005</td>\n",
       "      <td>-2.387605</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>0.188272</td>\n",
       "      <td>-0.485411</td>\n",
       "      <td>2.426005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>409bd257-5ce4-4a03-a7a8-c9c8e697fb39</td>\n",
       "      <td>0.054778</td>\n",
       "      <td>-0.184974</td>\n",
       "      <td>1.499648</td>\n",
       "      <td>1.796704</td>\n",
       "      <td>-0.171608</td>\n",
       "      <td>-0.950108</td>\n",
       "      <td>0.357520</td>\n",
       "      <td>-0.224926</td>\n",
       "      <td>-0.127403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667416</td>\n",
       "      <td>-1.235167</td>\n",
       "      <td>-0.071146</td>\n",
       "      <td>-0.727271</td>\n",
       "      <td>0.443232</td>\n",
       "      <td>-2.434762</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>-0.457758</td>\n",
       "      <td>-0.794980</td>\n",
       "      <td>2.277326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>d9e3e564-e9c8-416c-8a25-91e6744b1472</td>\n",
       "      <td>-0.080958</td>\n",
       "      <td>-0.737993</td>\n",
       "      <td>1.620272</td>\n",
       "      <td>1.800267</td>\n",
       "      <td>-0.261851</td>\n",
       "      <td>-0.587495</td>\n",
       "      <td>0.535072</td>\n",
       "      <td>-0.229857</td>\n",
       "      <td>0.078975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967174</td>\n",
       "      <td>-0.578166</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>-0.754880</td>\n",
       "      <td>0.795938</td>\n",
       "      <td>-2.518502</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>-0.697180</td>\n",
       "      <td>-1.034985</td>\n",
       "      <td>2.054563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>fc1de880-73b1-4a71-a1e9-09c0591bfc8f</td>\n",
       "      <td>1.256258</td>\n",
       "      <td>1.991867</td>\n",
       "      <td>2.139649</td>\n",
       "      <td>2.798089</td>\n",
       "      <td>1.641920</td>\n",
       "      <td>1.495566</td>\n",
       "      <td>2.351040</td>\n",
       "      <td>2.038052</td>\n",
       "      <td>1.044466</td>\n",
       "      <td>...</td>\n",
       "      <td>1.062476</td>\n",
       "      <td>0.344642</td>\n",
       "      <td>1.344346</td>\n",
       "      <td>-1.816005</td>\n",
       "      <td>2.062183</td>\n",
       "      <td>-2.748668</td>\n",
       "      <td>2.164313</td>\n",
       "      <td>1.493201</td>\n",
       "      <td>-1.744227</td>\n",
       "      <td>3.016144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>f48ea6d2-0ea2-4bdc-8ac7-2ad5b986892f</td>\n",
       "      <td>1.153343</td>\n",
       "      <td>0.014663</td>\n",
       "      <td>1.557922</td>\n",
       "      <td>1.796704</td>\n",
       "      <td>0.533686</td>\n",
       "      <td>-1.334220</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>0.066865</td>\n",
       "      <td>-0.127403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775974</td>\n",
       "      <td>-1.466030</td>\n",
       "      <td>0.157946</td>\n",
       "      <td>-0.142386</td>\n",
       "      <td>-0.179383</td>\n",
       "      <td>-2.175599</td>\n",
       "      <td>1.140180</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>-0.063337</td>\n",
       "      <td>2.616640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file_name         0         1         2  \\\n",
       "0    41fc6849-724a-4bac-8775-5e703fe74184  0.617700  0.011123  1.709592   \n",
       "1    0f6b347c-30da-495b-b103-36af30df77d7  0.701019  1.634090  1.713905   \n",
       "2    6568ed2b-0018-4750-9b3b-6c414dba60ae  0.482779  0.834642  1.762489   \n",
       "3    e18b7869-0049-4be7-9611-d08db28df33d  0.017044 -0.117965  1.499648   \n",
       "4    361cc367-f85d-402a-a19d-999d33f7667a -0.230597  0.136987  1.499648   \n",
       "..                                    ...       ...       ...       ...   \n",
       "949  41811e19-d6bb-4f1c-ac67-9df06ad9f85b  0.104792  0.337378  1.499648   \n",
       "950  409bd257-5ce4-4a03-a7a8-c9c8e697fb39  0.054778 -0.184974  1.499648   \n",
       "951  d9e3e564-e9c8-416c-8a25-91e6744b1472 -0.080958 -0.737993  1.620272   \n",
       "952  fc1de880-73b1-4a71-a1e9-09c0591bfc8f  1.256258  1.991867  2.139649   \n",
       "953  f48ea6d2-0ea2-4bdc-8ac7-2ad5b986892f  1.153343  0.014663  1.557922   \n",
       "\n",
       "            3         4         5         6         7         8  ...  \\\n",
       "0    1.802925  0.864535  0.013604  1.251312  0.654158  0.529081  ...   \n",
       "1    1.874578  0.944521 -1.083194  0.259510  0.460175  0.286948  ...   \n",
       "2    1.899293  0.802337 -0.077112 -0.252213  0.776146  0.537715  ...   \n",
       "3    1.796704  0.022777 -0.707842  0.717963 -0.450479 -0.127403  ...   \n",
       "4    1.796704  0.142297 -0.392801  0.524221  0.057456 -0.127403  ...   \n",
       "..        ...       ...       ...       ...       ...       ...  ...   \n",
       "949  1.796704 -0.351197 -0.798211 -0.298172 -0.230740 -0.127403  ...   \n",
       "950  1.796704 -0.171608 -0.950108  0.357520 -0.224926 -0.127403  ...   \n",
       "951  1.800267 -0.261851 -0.587495  0.535072 -0.229857  0.078975  ...   \n",
       "952  2.798089  1.641920  1.495566  2.351040  2.038052  1.044466  ...   \n",
       "953  1.796704  0.533686 -1.334220 -0.262562  0.066865 -0.127403  ...   \n",
       "\n",
       "          118       119       120       121       122       123       124  \\\n",
       "0    1.063729 -0.070066 -0.001569 -1.816005  1.563665 -2.394009  1.257136   \n",
       "1    1.075076 -0.890278 -0.679742 -1.816005 -0.602386 -2.748668  1.140180   \n",
       "2    0.965852  0.545567  1.027019 -1.816005  0.069231 -2.467926  1.354587   \n",
       "3    0.703502 -0.826764 -0.335318 -0.642262 -0.153610 -2.529420  1.140180   \n",
       "4    0.753804 -1.121887  0.059129 -1.010980 -0.244771 -2.726314  1.140180   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "949  0.574850 -1.200839  0.263114 -0.682506 -0.243005 -2.387605  1.140180   \n",
       "950  0.667416 -1.235167 -0.071146 -0.727271  0.443232 -2.434762  1.140180   \n",
       "951  0.967174 -0.578166  0.023856 -0.754880  0.795938 -2.518502  1.140180   \n",
       "952  1.062476  0.344642  1.344346 -1.816005  2.062183 -2.748668  2.164313   \n",
       "953  0.775974 -1.466030  0.157946 -0.142386 -0.179383 -2.175599  1.140180   \n",
       "\n",
       "          125       126       127  \n",
       "0    0.394306 -1.513282  2.038277  \n",
       "1    0.989450 -1.030964  2.194542  \n",
       "2    1.037515 -1.744227  1.812896  \n",
       "3    0.609665 -1.030876  2.052207  \n",
       "4   -0.026577 -0.617281  2.280982  \n",
       "..        ...       ...       ...  \n",
       "949  0.188272 -0.485411  2.426005  \n",
       "950 -0.457758 -0.794980  2.277326  \n",
       "951 -0.697180 -1.034985  2.054563  \n",
       "952  1.493201 -1.744227  3.016144  \n",
       "953  0.005079 -0.063337  2.616640  \n",
       "\n",
       "[954 rows x 129 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df = pd.DataFrame(out)\n",
    "latent_df.insert(0,'file_name',cnv_df['file_name'])\n",
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"../../data/cnv_df_128_tw.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ef9037651b8fb300183737a1adf54e758a8413bef4becc8f06877b013d9a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
