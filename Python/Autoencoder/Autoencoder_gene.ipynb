{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type = Ignore\n",
    "#Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from AEModel import AE, MyDataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "gene_df= pd.read_csv(\"../../data/master_gene_df.csv\",encoding = \"UTF-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSPYL1</th>\n",
       "      <th>TRIM40</th>\n",
       "      <th>PCGF2</th>\n",
       "      <th>GLT8D2</th>\n",
       "      <th>MLKL</th>\n",
       "      <th>TRPC1</th>\n",
       "      <th>NOP14</th>\n",
       "      <th>PTK7</th>\n",
       "      <th>SAAL1</th>\n",
       "      <th>LRRC27</th>\n",
       "      <th>...</th>\n",
       "      <th>DHX34</th>\n",
       "      <th>XAGE1B</th>\n",
       "      <th>ARL8A</th>\n",
       "      <th>KCTD8</th>\n",
       "      <th>SLX1B</th>\n",
       "      <th>CPNE6</th>\n",
       "      <th>XYLB</th>\n",
       "      <th>PKN3</th>\n",
       "      <th>RGS1</th>\n",
       "      <th>PGK1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.4909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.6829</td>\n",
       "      <td>6.5735</td>\n",
       "      <td>11.8354</td>\n",
       "      <td>4.3273</td>\n",
       "      <td>17.0597</td>\n",
       "      <td>76.0102</td>\n",
       "      <td>13.5971</td>\n",
       "      <td>4.1564</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1412</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>114.5090</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.3251</td>\n",
       "      <td>12.6058</td>\n",
       "      <td>84.0652</td>\n",
       "      <td>515.9193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.3660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.3977</td>\n",
       "      <td>12.3956</td>\n",
       "      <td>6.7753</td>\n",
       "      <td>3.4088</td>\n",
       "      <td>26.2333</td>\n",
       "      <td>35.2884</td>\n",
       "      <td>11.8437</td>\n",
       "      <td>1.0874</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5863</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>93.0715</td>\n",
       "      <td>0.3294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>2.6886</td>\n",
       "      <td>11.6752</td>\n",
       "      <td>82.8888</td>\n",
       "      <td>1444.9458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.5962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0650</td>\n",
       "      <td>5.2642</td>\n",
       "      <td>4.2537</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>25.3952</td>\n",
       "      <td>64.0538</td>\n",
       "      <td>34.0402</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>...</td>\n",
       "      <td>28.4462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>82.7308</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.1222</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.2784</td>\n",
       "      <td>7.2688</td>\n",
       "      <td>39.6819</td>\n",
       "      <td>632.6572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.8982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.7856</td>\n",
       "      <td>6.6122</td>\n",
       "      <td>5.0003</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>30.4176</td>\n",
       "      <td>69.6955</td>\n",
       "      <td>33.3931</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>...</td>\n",
       "      <td>20.8353</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>168.9003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>7.0244</td>\n",
       "      <td>15.3454</td>\n",
       "      <td>26.6279</td>\n",
       "      <td>682.4480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.1942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5414</td>\n",
       "      <td>5.0653</td>\n",
       "      <td>9.2886</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>30.6175</td>\n",
       "      <td>83.9273</td>\n",
       "      <td>10.7675</td>\n",
       "      <td>2.1584</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3766</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>106.8909</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>1.1573</td>\n",
       "      <td>3.3286</td>\n",
       "      <td>154.2987</td>\n",
       "      <td>436.9842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TSPYL1  TRIM40    PCGF2   GLT8D2     MLKL   TRPC1    NOP14     PTK7  \\\n",
       "0  24.4909     0.0  21.6829   6.5735  11.8354  4.3273  17.0597  76.0102   \n",
       "1  12.3660     0.0  24.3977  12.3956   6.7753  3.4088  26.2333  35.2884   \n",
       "2  41.5962     0.0  22.0650   5.2642   4.2537  0.8289  25.3952  64.0538   \n",
       "3  30.8982     0.0  14.7856   6.6122   5.0003  0.9631  30.4176  69.6955   \n",
       "4  14.1942     0.0   6.5414   5.0653   9.2886  0.9809  30.6175  83.9273   \n",
       "\n",
       "     SAAL1  LRRC27  ...    DHX34  XAGE1B     ARL8A   KCTD8   SLX1B   CPNE6  \\\n",
       "0  13.5971  4.1564  ...  13.1412  0.0000  114.5090  0.0000  0.0000  0.0000   \n",
       "1  11.8437  1.0874  ...   7.5863  0.1099   93.0715  0.3294  0.0000  0.0065   \n",
       "2  34.0402  0.9742  ...  28.4462  0.0000   82.7308  0.0277  0.1222  0.0000   \n",
       "3  33.3931  0.7268  ...  20.8353  0.0000  168.9003  0.0000  0.0229  0.0471   \n",
       "4  10.7675  2.1584  ...  15.3766  0.0000  106.8909  0.0000  0.0000  0.0414   \n",
       "\n",
       "     XYLB     PKN3      RGS1       PGK1  \n",
       "0  3.3251  12.6058   84.0652   515.9193  \n",
       "1  2.6886  11.6752   82.8888  1444.9458  \n",
       "2  4.2784   7.2688   39.6819   632.6572  \n",
       "3  7.0244  15.3454   26.6279   682.4480  \n",
       "4  1.1573   3.3286  154.2987   436.9842  \n",
       "\n",
       "[5 rows x 19188 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(gene_df.iloc[:,1:].shape[1])\n",
    "gene_df.iloc[:,1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is used to load the dataset for training\n",
    "pd_train_dataset, pd_test_dataset = train_test_split(gene_df.iloc[:,1:], test_size=0.2)\n",
    "\n",
    "X_train_sc = StandardScaler().fit_transform(pd_train_dataset)\n",
    "\n",
    "X_test_sc = StandardScaler().fit_transform(pd_test_dataset)\n",
    "\n",
    "X_full_sc = StandardScaler().fit_transform(gene_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MyDataset(X_train_sc)\n",
    "data_test = MyDataset(X_test_sc)\n",
    "data_full = MyDataset(X_full_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(data_train, batch_size=50, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape = len(X_train_sc[0])).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/300, train_loss = 1.649744\n",
      "epoch : 1/300, val_loss = 1.873881\n",
      "epoch : 2/300, train_loss = 1.269048\n",
      "epoch : 2/300, val_loss = 1.302953\n",
      "epoch : 3/300, train_loss = 1.116839\n",
      "epoch : 3/300, val_loss = 1.110397\n",
      "epoch : 4/300, train_loss = 1.018360\n",
      "epoch : 4/300, val_loss = 1.012367\n",
      "epoch : 5/300, train_loss = 0.947758\n",
      "epoch : 5/300, val_loss = 0.953015\n",
      "epoch : 6/300, train_loss = 0.895796\n",
      "epoch : 6/300, val_loss = 0.918318\n",
      "epoch : 7/300, train_loss = 0.855828\n",
      "epoch : 7/300, val_loss = 0.917071\n",
      "epoch : 8/300, train_loss = 0.825374\n",
      "epoch : 8/300, val_loss = 0.915940\n",
      "epoch : 9/300, train_loss = 0.801172\n",
      "epoch : 9/300, val_loss = 0.864377\n",
      "epoch : 10/300, train_loss = 0.773434\n",
      "epoch : 10/300, val_loss = 0.870696\n",
      "epoch : 11/300, train_loss = 0.743416\n",
      "epoch : 11/300, val_loss = 0.828268\n",
      "epoch : 12/300, train_loss = 0.713683\n",
      "epoch : 12/300, val_loss = 0.807799\n",
      "epoch : 13/300, train_loss = 0.687650\n",
      "epoch : 13/300, val_loss = 0.778897\n",
      "epoch : 14/300, train_loss = 0.664801\n",
      "epoch : 14/300, val_loss = 0.772418\n",
      "epoch : 15/300, train_loss = 0.649452\n",
      "epoch : 15/300, val_loss = 0.752299\n",
      "epoch : 16/300, train_loss = 0.630881\n",
      "epoch : 16/300, val_loss = 0.745754\n",
      "epoch : 17/300, train_loss = 0.615986\n",
      "epoch : 17/300, val_loss = 0.750834\n",
      "epoch : 18/300, train_loss = 0.604071\n",
      "epoch : 18/300, val_loss = 0.745745\n",
      "epoch : 19/300, train_loss = 0.590604\n",
      "epoch : 19/300, val_loss = 0.729097\n",
      "epoch : 20/300, train_loss = 0.577981\n",
      "epoch : 20/300, val_loss = 0.720812\n",
      "epoch : 21/300, train_loss = 0.564706\n",
      "epoch : 21/300, val_loss = 0.724301\n",
      "epoch : 22/300, train_loss = 0.551097\n",
      "epoch : 22/300, val_loss = 0.717271\n",
      "epoch : 23/300, train_loss = 0.541148\n",
      "epoch : 23/300, val_loss = 0.705963\n",
      "epoch : 24/300, train_loss = 0.531503\n",
      "epoch : 24/300, val_loss = 0.706249\n",
      "epoch : 25/300, train_loss = 0.522843\n",
      "epoch : 25/300, val_loss = 0.697579\n",
      "epoch : 26/300, train_loss = 0.513297\n",
      "epoch : 26/300, val_loss = 0.704013\n",
      "epoch : 27/300, train_loss = 0.505825\n",
      "epoch : 27/300, val_loss = 0.711211\n",
      "epoch : 28/300, train_loss = 0.503028\n",
      "epoch : 28/300, val_loss = 0.710179\n",
      "epoch : 29/300, train_loss = 0.496716\n",
      "epoch : 29/300, val_loss = 0.698987\n",
      "epoch : 30/300, train_loss = 0.486312\n",
      "epoch : 30/300, val_loss = 0.692518\n",
      "epoch : 31/300, train_loss = 0.481937\n",
      "epoch : 31/300, val_loss = 0.689694\n",
      "epoch : 32/300, train_loss = 0.478791\n",
      "epoch : 32/300, val_loss = 0.688997\n",
      "epoch : 33/300, train_loss = 0.471368\n",
      "epoch : 33/300, val_loss = 0.691191\n",
      "epoch : 34/300, train_loss = 0.467521\n",
      "epoch : 34/300, val_loss = 0.690254\n",
      "epoch : 35/300, train_loss = 0.463879\n",
      "epoch : 35/300, val_loss = 0.754752\n",
      "epoch : 36/300, train_loss = 0.458776\n",
      "epoch : 36/300, val_loss = 0.688539\n",
      "epoch : 37/300, train_loss = 0.454085\n",
      "epoch : 37/300, val_loss = 0.700563\n",
      "epoch : 38/300, train_loss = 0.450165\n",
      "epoch : 38/300, val_loss = 0.688121\n",
      "epoch : 39/300, train_loss = 0.447084\n",
      "epoch : 39/300, val_loss = 0.684301\n",
      "epoch : 40/300, train_loss = 0.443653\n",
      "epoch : 40/300, val_loss = 0.691118\n",
      "epoch : 41/300, train_loss = 0.441291\n",
      "epoch : 41/300, val_loss = 0.689307\n",
      "epoch : 42/300, train_loss = 0.438321\n",
      "epoch : 42/300, val_loss = 0.684009\n",
      "epoch : 43/300, train_loss = 0.436490\n",
      "epoch : 43/300, val_loss = 0.683877\n",
      "epoch : 44/300, train_loss = 0.432859\n",
      "epoch : 44/300, val_loss = 0.691208\n",
      "epoch : 45/300, train_loss = 0.431403\n",
      "epoch : 45/300, val_loss = 0.701574\n",
      "epoch : 46/300, train_loss = 0.428125\n",
      "epoch : 46/300, val_loss = 0.685861\n",
      "epoch : 47/300, train_loss = 0.424043\n",
      "epoch : 47/300, val_loss = 0.687286\n",
      "epoch : 48/300, train_loss = 0.422147\n",
      "epoch : 48/300, val_loss = 0.689754\n",
      "epoch : 49/300, train_loss = 0.419933\n",
      "epoch : 49/300, val_loss = 0.723978\n",
      "epoch : 50/300, train_loss = 0.418481\n",
      "epoch : 50/300, val_loss = 0.701547\n",
      "epoch : 51/300, train_loss = 0.416290\n",
      "epoch : 51/300, val_loss = 0.689827\n",
      "epoch : 52/300, train_loss = 0.415442\n",
      "epoch : 52/300, val_loss = 0.687692\n",
      "epoch : 53/300, train_loss = 0.414373\n",
      "epoch : 53/300, val_loss = 0.699005\n",
      "epoch : 54/300, train_loss = 0.415710\n",
      "epoch : 54/300, val_loss = 0.692750\n",
      "epoch : 55/300, train_loss = 0.415576\n",
      "epoch : 55/300, val_loss = 0.692285\n",
      "epoch : 56/300, train_loss = 0.415875\n",
      "epoch : 56/300, val_loss = 0.700479\n",
      "epoch : 57/300, train_loss = 0.415156\n",
      "epoch : 57/300, val_loss = 0.696772\n",
      "epoch : 58/300, train_loss = 0.411821\n",
      "epoch : 58/300, val_loss = 0.695704\n",
      "epoch : 59/300, train_loss = 0.411805\n",
      "epoch : 59/300, val_loss = 0.694409\n",
      "epoch : 60/300, train_loss = 0.409561\n",
      "epoch : 60/300, val_loss = 0.694313\n",
      "epoch : 61/300, train_loss = 0.404802\n",
      "epoch : 61/300, val_loss = 0.691371\n",
      "epoch : 62/300, train_loss = 0.402612\n",
      "epoch : 62/300, val_loss = 0.687365\n",
      "epoch : 63/300, train_loss = 0.399003\n",
      "epoch : 63/300, val_loss = 0.691690\n",
      "epoch : 64/300, train_loss = 0.396384\n",
      "epoch : 64/300, val_loss = 0.692902\n",
      "epoch : 65/300, train_loss = 0.393200\n",
      "epoch : 65/300, val_loss = 0.685605\n",
      "epoch : 66/300, train_loss = 0.391029\n",
      "epoch : 66/300, val_loss = 0.691515\n",
      "epoch : 67/300, train_loss = 0.390276\n",
      "epoch : 67/300, val_loss = 0.685720\n",
      "epoch : 68/300, train_loss = 0.388546\n",
      "epoch : 68/300, val_loss = 0.688102\n",
      "epoch : 69/300, train_loss = 0.388351\n",
      "epoch : 69/300, val_loss = 0.693370\n",
      "epoch : 70/300, train_loss = 0.389400\n",
      "epoch : 70/300, val_loss = 0.715634\n",
      "epoch : 71/300, train_loss = 0.386694\n",
      "epoch : 71/300, val_loss = 0.686865\n",
      "epoch : 72/300, train_loss = 0.384111\n",
      "epoch : 72/300, val_loss = 0.687910\n",
      "epoch : 73/300, train_loss = 0.381850\n",
      "epoch : 73/300, val_loss = 0.691934\n",
      "epoch : 74/300, train_loss = 0.379351\n",
      "epoch : 74/300, val_loss = 0.689197\n",
      "epoch : 75/300, train_loss = 0.377791\n",
      "epoch : 75/300, val_loss = 0.686990\n",
      "epoch : 76/300, train_loss = 0.377134\n",
      "epoch : 76/300, val_loss = 0.692590\n",
      "epoch : 77/300, train_loss = 0.376371\n",
      "epoch : 77/300, val_loss = 0.693996\n",
      "epoch : 78/300, train_loss = 0.377579\n",
      "epoch : 78/300, val_loss = 0.692411\n",
      "epoch : 79/300, train_loss = 0.378700\n",
      "epoch : 79/300, val_loss = 0.700961\n",
      "epoch : 80/300, train_loss = 0.378489\n",
      "epoch : 80/300, val_loss = 0.699993\n",
      "epoch : 81/300, train_loss = 0.378085\n",
      "epoch : 81/300, val_loss = 0.698763\n",
      "epoch : 82/300, train_loss = 0.376635\n",
      "epoch : 82/300, val_loss = 0.696361\n",
      "epoch : 83/300, train_loss = 0.376582\n",
      "epoch : 83/300, val_loss = 0.692977\n",
      "epoch : 84/300, train_loss = 0.376404\n",
      "epoch : 84/300, val_loss = 0.694853\n",
      "epoch : 85/300, train_loss = 0.377083\n",
      "epoch : 85/300, val_loss = 0.700378\n",
      "epoch : 86/300, train_loss = 0.376981\n",
      "epoch : 86/300, val_loss = 0.701402\n",
      "epoch : 87/300, train_loss = 0.375762\n",
      "epoch : 87/300, val_loss = 0.695637\n",
      "epoch : 88/300, train_loss = 0.378303\n",
      "epoch : 88/300, val_loss = 0.700069\n",
      "epoch : 89/300, train_loss = 0.376562\n",
      "epoch : 89/300, val_loss = 0.696513\n",
      "epoch : 90/300, train_loss = 0.375513\n",
      "epoch : 90/300, val_loss = 0.695689\n",
      "epoch : 91/300, train_loss = 0.371988\n",
      "epoch : 91/300, val_loss = 0.694262\n",
      "epoch : 92/300, train_loss = 0.371938\n",
      "epoch : 92/300, val_loss = 0.693117\n",
      "epoch : 93/300, train_loss = 0.368626\n",
      "epoch : 93/300, val_loss = 0.697487\n",
      "epoch : 94/300, train_loss = 0.367982\n",
      "epoch : 94/300, val_loss = 0.695591\n",
      "epoch : 95/300, train_loss = 0.365044\n",
      "epoch : 95/300, val_loss = 0.693871\n",
      "epoch : 96/300, train_loss = 0.364635\n",
      "epoch : 96/300, val_loss = 0.693634\n",
      "epoch : 97/300, train_loss = 0.363037\n",
      "epoch : 97/300, val_loss = 0.694638\n",
      "epoch : 98/300, train_loss = 0.363035\n",
      "epoch : 98/300, val_loss = 0.692626\n",
      "epoch : 99/300, train_loss = 0.361663\n",
      "epoch : 99/300, val_loss = 0.694138\n",
      "epoch : 100/300, train_loss = 0.360658\n",
      "epoch : 100/300, val_loss = 0.697246\n",
      "epoch : 101/300, train_loss = 0.361676\n",
      "epoch : 101/300, val_loss = 0.694743\n",
      "epoch : 102/300, train_loss = 0.361883\n",
      "epoch : 102/300, val_loss = 0.699440\n",
      "epoch : 103/300, train_loss = 0.361542\n",
      "epoch : 103/300, val_loss = 0.697794\n",
      "epoch : 104/300, train_loss = 0.361836\n",
      "epoch : 104/300, val_loss = 0.696467\n",
      "epoch : 105/300, train_loss = 0.361215\n",
      "epoch : 105/300, val_loss = 0.703052\n",
      "epoch : 106/300, train_loss = 0.361377\n",
      "epoch : 106/300, val_loss = 0.708076\n",
      "epoch : 107/300, train_loss = 0.358997\n",
      "epoch : 107/300, val_loss = 0.696652\n",
      "epoch : 108/300, train_loss = 0.358866\n",
      "epoch : 108/300, val_loss = 0.694298\n",
      "epoch : 109/300, train_loss = 0.357773\n",
      "epoch : 109/300, val_loss = 0.700077\n",
      "epoch : 110/300, train_loss = 0.357060\n",
      "epoch : 110/300, val_loss = 0.700309\n",
      "epoch : 111/300, train_loss = 0.356611\n",
      "epoch : 111/300, val_loss = 0.696913\n",
      "epoch : 112/300, train_loss = 0.357065\n",
      "epoch : 112/300, val_loss = 0.698478\n",
      "epoch : 113/300, train_loss = 0.356338\n",
      "epoch : 113/300, val_loss = 0.700279\n",
      "epoch : 114/300, train_loss = 0.355863\n",
      "epoch : 114/300, val_loss = 0.722686\n",
      "epoch : 115/300, train_loss = 0.355604\n",
      "epoch : 115/300, val_loss = 0.699090\n",
      "epoch : 116/300, train_loss = 0.355747\n",
      "epoch : 116/300, val_loss = 0.696898\n",
      "epoch : 117/300, train_loss = 0.354054\n",
      "epoch : 117/300, val_loss = 0.702058\n",
      "epoch : 118/300, train_loss = 0.353856\n",
      "epoch : 118/300, val_loss = 0.702393\n",
      "epoch : 119/300, train_loss = 0.353822\n",
      "epoch : 119/300, val_loss = 0.697772\n",
      "epoch : 120/300, train_loss = 0.353227\n",
      "epoch : 120/300, val_loss = 0.700195\n",
      "epoch : 121/300, train_loss = 0.353609\n",
      "epoch : 121/300, val_loss = 0.701524\n",
      "epoch : 122/300, train_loss = 0.353225\n",
      "epoch : 122/300, val_loss = 0.702372\n",
      "epoch : 123/300, train_loss = 0.352750\n",
      "epoch : 123/300, val_loss = 0.703661\n",
      "epoch : 124/300, train_loss = 0.352878\n",
      "epoch : 124/300, val_loss = 0.700054\n",
      "epoch : 125/300, train_loss = 0.352720\n",
      "epoch : 125/300, val_loss = 0.701238\n",
      "epoch : 126/300, train_loss = 0.354147\n",
      "epoch : 126/300, val_loss = 0.709430\n",
      "epoch : 127/300, train_loss = 0.354671\n",
      "epoch : 127/300, val_loss = 0.704031\n",
      "epoch : 128/300, train_loss = 0.353746\n",
      "epoch : 128/300, val_loss = 0.703936\n",
      "epoch : 129/300, train_loss = 0.353003\n",
      "epoch : 129/300, val_loss = 0.728876\n",
      "epoch : 130/300, train_loss = 0.350777\n",
      "epoch : 130/300, val_loss = 0.708780\n",
      "epoch : 131/300, train_loss = 0.350663\n",
      "epoch : 131/300, val_loss = 0.700552\n",
      "epoch : 132/300, train_loss = 0.348471\n",
      "epoch : 132/300, val_loss = 0.696161\n",
      "epoch : 133/300, train_loss = 0.347214\n",
      "epoch : 133/300, val_loss = 0.701822\n",
      "epoch : 134/300, train_loss = 0.346529\n",
      "epoch : 134/300, val_loss = 0.702898\n",
      "epoch : 135/300, train_loss = 0.346450\n",
      "epoch : 135/300, val_loss = 0.701740\n",
      "epoch : 136/300, train_loss = 0.346762\n",
      "epoch : 136/300, val_loss = 0.700498\n",
      "epoch : 137/300, train_loss = 0.346521\n",
      "epoch : 137/300, val_loss = 0.706421\n",
      "epoch : 138/300, train_loss = 0.347871\n",
      "epoch : 138/300, val_loss = 0.720390\n",
      "epoch : 139/300, train_loss = 0.347730\n",
      "epoch : 139/300, val_loss = 0.705234\n",
      "epoch : 140/300, train_loss = 0.348872\n",
      "epoch : 140/300, val_loss = 0.704564\n",
      "epoch : 141/300, train_loss = 0.348850\n",
      "epoch : 141/300, val_loss = 0.705342\n",
      "epoch : 142/300, train_loss = 0.348523\n",
      "epoch : 142/300, val_loss = 0.705267\n",
      "epoch : 143/300, train_loss = 0.348003\n",
      "epoch : 143/300, val_loss = 0.703610\n",
      "epoch : 144/300, train_loss = 0.347667\n",
      "epoch : 144/300, val_loss = 0.703830\n",
      "epoch : 145/300, train_loss = 0.345431\n",
      "epoch : 145/300, val_loss = 0.704395\n",
      "epoch : 146/300, train_loss = 0.345618\n",
      "epoch : 146/300, val_loss = 0.701979\n",
      "epoch : 147/300, train_loss = 0.345035\n",
      "epoch : 147/300, val_loss = 0.704339\n",
      "epoch : 148/300, train_loss = 0.344124\n",
      "epoch : 148/300, val_loss = 0.703718\n",
      "epoch : 149/300, train_loss = 0.343342\n",
      "epoch : 149/300, val_loss = 0.702659\n",
      "epoch : 150/300, train_loss = 0.342894\n",
      "epoch : 150/300, val_loss = 0.707085\n",
      "epoch : 151/300, train_loss = 0.343399\n",
      "epoch : 151/300, val_loss = 0.701173\n",
      "epoch : 152/300, train_loss = 0.343883\n",
      "epoch : 152/300, val_loss = 0.701882\n",
      "epoch : 153/300, train_loss = 0.343305\n",
      "epoch : 153/300, val_loss = 0.707892\n",
      "epoch : 154/300, train_loss = 0.343231\n",
      "epoch : 154/300, val_loss = 0.703476\n",
      "epoch : 155/300, train_loss = 0.342436\n",
      "epoch : 155/300, val_loss = 0.701015\n",
      "epoch : 156/300, train_loss = 0.342487\n",
      "epoch : 156/300, val_loss = 0.708491\n",
      "epoch : 157/300, train_loss = 0.343503\n",
      "epoch : 157/300, val_loss = 0.706391\n",
      "epoch : 158/300, train_loss = 0.342267\n",
      "epoch : 158/300, val_loss = 0.703197\n",
      "epoch : 159/300, train_loss = 0.341866\n",
      "epoch : 159/300, val_loss = 0.704005\n",
      "epoch : 160/300, train_loss = 0.342271\n",
      "epoch : 160/300, val_loss = 0.706046\n",
      "epoch : 161/300, train_loss = 0.342591\n",
      "epoch : 161/300, val_loss = 0.705047\n",
      "epoch : 162/300, train_loss = 0.341494\n",
      "epoch : 162/300, val_loss = 0.705887\n",
      "epoch : 163/300, train_loss = 0.341122\n",
      "epoch : 163/300, val_loss = 0.702798\n",
      "epoch : 164/300, train_loss = 0.341583\n",
      "epoch : 164/300, val_loss = 0.703422\n",
      "epoch : 165/300, train_loss = 0.341137\n",
      "epoch : 165/300, val_loss = 0.709776\n",
      "epoch : 166/300, train_loss = 0.341436\n",
      "epoch : 166/300, val_loss = 0.705970\n",
      "epoch : 167/300, train_loss = 0.341141\n",
      "epoch : 167/300, val_loss = 0.701494\n",
      "epoch : 168/300, train_loss = 0.340864\n",
      "epoch : 168/300, val_loss = 0.707204\n",
      "epoch : 169/300, train_loss = 0.341374\n",
      "epoch : 169/300, val_loss = 0.718940\n",
      "epoch : 170/300, train_loss = 0.341680\n",
      "epoch : 170/300, val_loss = 0.704055\n",
      "epoch : 171/300, train_loss = 0.343159\n",
      "epoch : 171/300, val_loss = 0.712481\n",
      "epoch : 172/300, train_loss = 0.342229\n",
      "epoch : 172/300, val_loss = 0.713772\n",
      "epoch : 173/300, train_loss = 0.342744\n",
      "epoch : 173/300, val_loss = 0.703154\n",
      "epoch : 174/300, train_loss = 0.342985\n",
      "epoch : 174/300, val_loss = 0.705987\n",
      "epoch : 175/300, train_loss = 0.342757\n",
      "epoch : 175/300, val_loss = 0.709817\n",
      "epoch : 176/300, train_loss = 0.342254\n",
      "epoch : 176/300, val_loss = 0.706657\n",
      "epoch : 177/300, train_loss = 0.341308\n",
      "epoch : 177/300, val_loss = 0.703591\n",
      "epoch : 178/300, train_loss = 0.340373\n",
      "epoch : 178/300, val_loss = 0.705014\n",
      "epoch : 179/300, train_loss = 0.339820\n",
      "epoch : 179/300, val_loss = 0.707754\n",
      "epoch : 180/300, train_loss = 0.339214\n",
      "epoch : 180/300, val_loss = 0.705556\n",
      "epoch : 181/300, train_loss = 0.338048\n",
      "epoch : 181/300, val_loss = 0.704596\n",
      "epoch : 182/300, train_loss = 0.339038\n",
      "epoch : 182/300, val_loss = 0.699554\n",
      "epoch : 183/300, train_loss = 0.337273\n",
      "epoch : 183/300, val_loss = 0.707915\n",
      "epoch : 184/300, train_loss = 0.337028\n",
      "epoch : 184/300, val_loss = 0.707235\n",
      "epoch : 185/300, train_loss = 0.338247\n",
      "epoch : 185/300, val_loss = 0.703996\n",
      "epoch : 186/300, train_loss = 0.338282\n",
      "epoch : 186/300, val_loss = 0.703365\n",
      "epoch : 187/300, train_loss = 0.338979\n",
      "epoch : 187/300, val_loss = 0.706079\n",
      "epoch : 188/300, train_loss = 0.337223\n",
      "epoch : 188/300, val_loss = 0.706813\n",
      "epoch : 189/300, train_loss = 0.337373\n",
      "epoch : 189/300, val_loss = 0.704402\n",
      "epoch : 190/300, train_loss = 0.336392\n",
      "epoch : 190/300, val_loss = 0.702735\n",
      "epoch : 191/300, train_loss = 0.335597\n",
      "epoch : 191/300, val_loss = 0.704838\n",
      "epoch : 192/300, train_loss = 0.335048\n",
      "epoch : 192/300, val_loss = 0.705451\n",
      "epoch : 193/300, train_loss = 0.334653\n",
      "epoch : 193/300, val_loss = 0.740486\n",
      "epoch : 194/300, train_loss = 0.336714\n",
      "epoch : 194/300, val_loss = 0.702129\n",
      "epoch : 195/300, train_loss = 0.339553\n",
      "epoch : 195/300, val_loss = 0.711711\n",
      "epoch : 196/300, train_loss = 0.339903\n",
      "epoch : 196/300, val_loss = 0.707530\n",
      "epoch : 197/300, train_loss = 0.338181\n",
      "epoch : 197/300, val_loss = 0.707131\n",
      "epoch : 198/300, train_loss = 0.337219\n",
      "epoch : 198/300, val_loss = 0.704834\n",
      "epoch : 199/300, train_loss = 0.336013\n",
      "epoch : 199/300, val_loss = 0.703981\n",
      "epoch : 200/300, train_loss = 0.334948\n",
      "epoch : 200/300, val_loss = 0.704688\n",
      "epoch : 201/300, train_loss = 0.334844\n",
      "epoch : 201/300, val_loss = 0.705057\n",
      "epoch : 202/300, train_loss = 0.335173\n",
      "epoch : 202/300, val_loss = 0.705975\n",
      "epoch : 203/300, train_loss = 0.333696\n",
      "epoch : 203/300, val_loss = 0.707034\n",
      "epoch : 204/300, train_loss = 0.336591\n",
      "epoch : 204/300, val_loss = 0.709614\n",
      "epoch : 205/300, train_loss = 0.338122\n",
      "epoch : 205/300, val_loss = 0.709231\n",
      "epoch : 206/300, train_loss = 0.335853\n",
      "epoch : 206/300, val_loss = 0.708249\n",
      "epoch : 207/300, train_loss = 0.334900\n",
      "epoch : 207/300, val_loss = 0.703335\n",
      "epoch : 208/300, train_loss = 0.334884\n",
      "epoch : 208/300, val_loss = 0.708747\n",
      "epoch : 209/300, train_loss = 0.334535\n",
      "epoch : 209/300, val_loss = 0.709309\n",
      "epoch : 210/300, train_loss = 0.333810\n",
      "epoch : 210/300, val_loss = 0.704014\n",
      "epoch : 211/300, train_loss = 0.332569\n",
      "epoch : 211/300, val_loss = 0.706196\n",
      "epoch : 212/300, train_loss = 0.333428\n",
      "epoch : 212/300, val_loss = 0.708496\n",
      "epoch : 213/300, train_loss = 0.332764\n",
      "epoch : 213/300, val_loss = 0.710609\n",
      "epoch : 214/300, train_loss = 0.333194\n",
      "epoch : 214/300, val_loss = 0.703751\n",
      "epoch : 215/300, train_loss = 0.332295\n",
      "epoch : 215/300, val_loss = 0.706462\n",
      "epoch : 216/300, train_loss = 0.331253\n",
      "epoch : 216/300, val_loss = 0.709165\n",
      "epoch : 217/300, train_loss = 0.331527\n",
      "epoch : 217/300, val_loss = 0.704356\n",
      "epoch : 218/300, train_loss = 0.330882\n",
      "epoch : 218/300, val_loss = 0.707825\n",
      "epoch : 219/300, train_loss = 0.331536\n",
      "epoch : 219/300, val_loss = 0.709328\n",
      "epoch : 220/300, train_loss = 0.331264\n",
      "epoch : 220/300, val_loss = 0.708617\n",
      "epoch : 221/300, train_loss = 0.334097\n",
      "epoch : 221/300, val_loss = 0.708601\n",
      "epoch : 222/300, train_loss = 0.334269\n",
      "epoch : 222/300, val_loss = 0.712233\n",
      "epoch : 223/300, train_loss = 0.335024\n",
      "epoch : 223/300, val_loss = 0.712075\n",
      "epoch : 224/300, train_loss = 0.336740\n",
      "epoch : 224/300, val_loss = 0.707137\n",
      "epoch : 225/300, train_loss = 0.334097\n",
      "epoch : 225/300, val_loss = 0.714547\n",
      "epoch : 226/300, train_loss = 0.334976\n",
      "epoch : 226/300, val_loss = 0.709939\n",
      "epoch : 227/300, train_loss = 0.334841\n",
      "epoch : 227/300, val_loss = 0.712464\n",
      "epoch : 228/300, train_loss = 0.335664\n",
      "epoch : 228/300, val_loss = 0.709108\n",
      "epoch : 229/300, train_loss = 0.333446\n",
      "epoch : 229/300, val_loss = 0.709449\n",
      "epoch : 230/300, train_loss = 0.331147\n",
      "epoch : 230/300, val_loss = 0.710584\n",
      "epoch : 231/300, train_loss = 0.330810\n",
      "epoch : 231/300, val_loss = 0.708707\n",
      "epoch : 232/300, train_loss = 0.329809\n",
      "epoch : 232/300, val_loss = 0.710124\n",
      "epoch : 233/300, train_loss = 0.330402\n",
      "epoch : 233/300, val_loss = 0.710692\n",
      "epoch : 234/300, train_loss = 0.330994\n",
      "epoch : 234/300, val_loss = 0.713030\n",
      "epoch : 235/300, train_loss = 0.331677\n",
      "epoch : 235/300, val_loss = 0.705545\n",
      "epoch : 236/300, train_loss = 0.329955\n",
      "epoch : 236/300, val_loss = 0.714199\n",
      "epoch : 237/300, train_loss = 0.329762\n",
      "epoch : 237/300, val_loss = 0.708747\n",
      "epoch : 238/300, train_loss = 0.328720\n",
      "epoch : 238/300, val_loss = 0.709783\n",
      "epoch : 239/300, train_loss = 0.329180\n",
      "epoch : 239/300, val_loss = 0.706287\n",
      "epoch : 240/300, train_loss = 0.328725\n",
      "epoch : 240/300, val_loss = 0.711436\n",
      "epoch : 241/300, train_loss = 0.328930\n",
      "epoch : 241/300, val_loss = 0.709457\n",
      "epoch : 242/300, train_loss = 0.328456\n",
      "epoch : 242/300, val_loss = 0.709310\n",
      "epoch : 243/300, train_loss = 0.328738\n",
      "epoch : 243/300, val_loss = 0.708628\n",
      "epoch : 244/300, train_loss = 0.328629\n",
      "epoch : 244/300, val_loss = 0.711505\n",
      "epoch : 245/300, train_loss = 0.328309\n",
      "epoch : 245/300, val_loss = 0.708650\n",
      "epoch : 246/300, train_loss = 0.327788\n",
      "epoch : 246/300, val_loss = 0.709371\n",
      "epoch : 247/300, train_loss = 0.327762\n",
      "epoch : 247/300, val_loss = 0.709127\n",
      "epoch : 248/300, train_loss = 0.326960\n",
      "epoch : 248/300, val_loss = 0.710344\n",
      "epoch : 249/300, train_loss = 0.328063\n",
      "epoch : 249/300, val_loss = 0.712499\n",
      "epoch : 250/300, train_loss = 0.328510\n",
      "epoch : 250/300, val_loss = 0.708215\n",
      "epoch : 251/300, train_loss = 0.327511\n",
      "epoch : 251/300, val_loss = 0.709194\n",
      "epoch : 252/300, train_loss = 0.327267\n",
      "epoch : 252/300, val_loss = 0.711829\n",
      "epoch : 253/300, train_loss = 0.326774\n",
      "epoch : 253/300, val_loss = 0.711203\n",
      "epoch : 254/300, train_loss = 0.327233\n",
      "epoch : 254/300, val_loss = 0.706481\n",
      "epoch : 255/300, train_loss = 0.326520\n",
      "epoch : 255/300, val_loss = 0.713325\n",
      "epoch : 256/300, train_loss = 0.326432\n",
      "epoch : 256/300, val_loss = 0.713578\n",
      "epoch : 257/300, train_loss = 0.326410\n",
      "epoch : 257/300, val_loss = 0.707200\n",
      "epoch : 258/300, train_loss = 0.326039\n",
      "epoch : 258/300, val_loss = 0.713243\n",
      "epoch : 259/300, train_loss = 0.326951\n",
      "epoch : 259/300, val_loss = 0.723868\n",
      "epoch : 260/300, train_loss = 0.328106\n",
      "epoch : 260/300, val_loss = 0.709872\n",
      "epoch : 261/300, train_loss = 0.328338\n",
      "epoch : 261/300, val_loss = 0.710133\n",
      "epoch : 262/300, train_loss = 0.327053\n",
      "epoch : 262/300, val_loss = 0.714704\n",
      "epoch : 263/300, train_loss = 0.327606\n",
      "epoch : 263/300, val_loss = 0.712614\n",
      "epoch : 264/300, train_loss = 0.326899\n",
      "epoch : 264/300, val_loss = 0.713061\n",
      "epoch : 265/300, train_loss = 0.327670\n",
      "epoch : 265/300, val_loss = 0.709267\n",
      "epoch : 266/300, train_loss = 0.327559\n",
      "epoch : 266/300, val_loss = 0.711030\n",
      "epoch : 267/300, train_loss = 0.327488\n",
      "epoch : 267/300, val_loss = 0.714820\n",
      "epoch : 268/300, train_loss = 0.327450\n",
      "epoch : 268/300, val_loss = 0.711840\n",
      "epoch : 269/300, train_loss = 0.328166\n",
      "epoch : 269/300, val_loss = 0.709339\n",
      "epoch : 270/300, train_loss = 0.327793\n",
      "epoch : 270/300, val_loss = 0.711443\n",
      "epoch : 271/300, train_loss = 0.327180\n",
      "epoch : 271/300, val_loss = 0.714051\n",
      "epoch : 272/300, train_loss = 0.327847\n",
      "epoch : 272/300, val_loss = 0.710266\n",
      "epoch : 273/300, train_loss = 0.326642\n",
      "epoch : 273/300, val_loss = 0.710220\n",
      "epoch : 274/300, train_loss = 0.326565\n",
      "epoch : 274/300, val_loss = 0.709821\n",
      "epoch : 275/300, train_loss = 0.325223\n",
      "epoch : 275/300, val_loss = 0.713934\n",
      "epoch : 276/300, train_loss = 0.325730\n",
      "epoch : 276/300, val_loss = 0.713670\n",
      "epoch : 277/300, train_loss = 0.325550\n",
      "epoch : 277/300, val_loss = 0.709770\n",
      "epoch : 278/300, train_loss = 0.325519\n",
      "epoch : 278/300, val_loss = 0.714054\n",
      "epoch : 279/300, train_loss = 0.325584\n",
      "epoch : 279/300, val_loss = 0.716676\n",
      "epoch : 280/300, train_loss = 0.325841\n",
      "epoch : 280/300, val_loss = 0.709976\n",
      "epoch : 281/300, train_loss = 0.325403\n",
      "epoch : 281/300, val_loss = 0.712579\n",
      "epoch : 282/300, train_loss = 0.325688\n",
      "epoch : 282/300, val_loss = 0.715104\n",
      "epoch : 283/300, train_loss = 0.325921\n",
      "epoch : 283/300, val_loss = 0.712711\n",
      "epoch : 284/300, train_loss = 0.326188\n",
      "epoch : 284/300, val_loss = 0.710981\n",
      "epoch : 285/300, train_loss = 0.326367\n",
      "epoch : 285/300, val_loss = 0.714415\n",
      "epoch : 286/300, train_loss = 0.326966\n",
      "epoch : 286/300, val_loss = 0.716249\n",
      "epoch : 287/300, train_loss = 0.327541\n",
      "epoch : 287/300, val_loss = 0.709859\n",
      "epoch : 288/300, train_loss = 0.327383\n",
      "epoch : 288/300, val_loss = 0.714781\n",
      "epoch : 289/300, train_loss = 0.329499\n",
      "epoch : 289/300, val_loss = 0.714992\n",
      "epoch : 290/300, train_loss = 0.330118\n",
      "epoch : 290/300, val_loss = 0.713655\n",
      "epoch : 291/300, train_loss = 0.330504\n",
      "epoch : 291/300, val_loss = 0.708669\n",
      "epoch : 292/300, train_loss = 0.328634\n",
      "epoch : 292/300, val_loss = 0.715535\n",
      "epoch : 293/300, train_loss = 0.327804\n",
      "epoch : 293/300, val_loss = 0.709769\n",
      "epoch : 294/300, train_loss = 0.326304\n",
      "epoch : 294/300, val_loss = 0.711506\n",
      "epoch : 295/300, train_loss = 0.325718\n",
      "epoch : 295/300, val_loss = 0.713464\n",
      "epoch : 296/300, train_loss = 0.325618\n",
      "epoch : 296/300, val_loss = 0.709927\n",
      "epoch : 297/300, train_loss = 0.324789\n",
      "epoch : 297/300, val_loss = 0.713280\n",
      "epoch : 298/300, train_loss = 0.324700\n",
      "epoch : 298/300, val_loss = 0.712349\n",
      "epoch : 299/300, train_loss = 0.323947\n",
      "epoch : 299/300, val_loss = 0.712002\n",
      "epoch : 300/300, train_loss = 0.324356\n",
      "epoch : 300/300, val_loss = 0.714345\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "epochs=300\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        lol, outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train.append(loss)\n",
    "\n",
    "\n",
    "    #For Valid Loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            code, outputs = model(batch)\n",
    "            loss_val =criterion(outputs, batch)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    losses_val.append(val_loss)\n",
    "\n",
    "\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    print(\"epoch : {}/{}, val_loss = {:.6f}\".format(epoch + 1, epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train, label = 'train_loss')\n",
    "plt.plot(losses_val, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"../../Figures/AE_GENE_300.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Gene Autoencoder Taining\n",
    "\n",
    "data_full = MyDataset(X_full_sc)\n",
    "full_loader = DataLoader(data_full, batch_size=50)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE(input_shape = len(X_full_sc[0])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/90, train_loss = 2.046481\n",
      "epoch : 2/90, train_loss = 1.586013\n",
      "epoch : 3/90, train_loss = 1.415245\n",
      "epoch : 4/90, train_loss = 1.300330\n",
      "epoch : 5/90, train_loss = 1.217544\n",
      "epoch : 6/90, train_loss = 1.153237\n",
      "epoch : 7/90, train_loss = 1.103091\n",
      "epoch : 8/90, train_loss = 1.062989\n",
      "epoch : 9/90, train_loss = 1.029086\n",
      "epoch : 10/90, train_loss = 0.998188\n",
      "epoch : 11/90, train_loss = 0.969073\n",
      "epoch : 12/90, train_loss = 0.943585\n",
      "epoch : 13/90, train_loss = 0.912189\n",
      "epoch : 14/90, train_loss = 0.891617\n",
      "epoch : 15/90, train_loss = 0.870798\n",
      "epoch : 16/90, train_loss = 0.852509\n",
      "epoch : 17/90, train_loss = 0.837566\n",
      "epoch : 18/90, train_loss = 0.822329\n",
      "epoch : 19/90, train_loss = 0.807550\n",
      "epoch : 20/90, train_loss = 0.785314\n",
      "epoch : 21/90, train_loss = 0.770507\n",
      "epoch : 22/90, train_loss = 0.760360\n",
      "epoch : 23/90, train_loss = 0.752065\n",
      "epoch : 24/90, train_loss = 0.739644\n",
      "epoch : 25/90, train_loss = 0.731057\n",
      "epoch : 26/90, train_loss = 0.720784\n",
      "epoch : 27/90, train_loss = 0.712438\n",
      "epoch : 28/90, train_loss = 0.704037\n",
      "epoch : 29/90, train_loss = 0.696496\n",
      "epoch : 30/90, train_loss = 0.688784\n",
      "epoch : 31/90, train_loss = 0.685856\n",
      "epoch : 32/90, train_loss = 0.680832\n",
      "epoch : 33/90, train_loss = 0.678235\n",
      "epoch : 34/90, train_loss = 0.672094\n",
      "epoch : 35/90, train_loss = 0.666950\n",
      "epoch : 36/90, train_loss = 0.662721\n",
      "epoch : 37/90, train_loss = 0.656870\n",
      "epoch : 38/90, train_loss = 0.653817\n",
      "epoch : 39/90, train_loss = 0.649950\n",
      "epoch : 40/90, train_loss = 0.648663\n",
      "epoch : 41/90, train_loss = 0.644684\n",
      "epoch : 42/90, train_loss = 0.641240\n",
      "epoch : 43/90, train_loss = 0.639912\n",
      "epoch : 44/90, train_loss = 0.636294\n",
      "epoch : 45/90, train_loss = 0.633994\n",
      "epoch : 46/90, train_loss = 0.629914\n",
      "epoch : 47/90, train_loss = 0.627964\n",
      "epoch : 48/90, train_loss = 0.628537\n",
      "epoch : 49/90, train_loss = 0.624541\n",
      "epoch : 50/90, train_loss = 0.622828\n",
      "epoch : 51/90, train_loss = 0.619926\n",
      "epoch : 52/90, train_loss = 0.617688\n",
      "epoch : 53/90, train_loss = 0.613324\n",
      "epoch : 54/90, train_loss = 0.609750\n",
      "epoch : 55/90, train_loss = 0.607242\n",
      "epoch : 56/90, train_loss = 0.606325\n",
      "epoch : 57/90, train_loss = 0.605784\n",
      "epoch : 58/90, train_loss = 0.603135\n",
      "epoch : 59/90, train_loss = 0.600021\n",
      "epoch : 60/90, train_loss = 0.597612\n",
      "epoch : 61/90, train_loss = 0.596376\n",
      "epoch : 62/90, train_loss = 0.593950\n",
      "epoch : 63/90, train_loss = 0.592016\n",
      "epoch : 64/90, train_loss = 0.591973\n",
      "epoch : 65/90, train_loss = 0.591200\n",
      "epoch : 66/90, train_loss = 0.588172\n",
      "epoch : 67/90, train_loss = 0.586982\n",
      "epoch : 68/90, train_loss = 0.585540\n",
      "epoch : 69/90, train_loss = 0.583979\n",
      "epoch : 70/90, train_loss = 0.581274\n",
      "epoch : 71/90, train_loss = 0.580474\n",
      "epoch : 72/90, train_loss = 0.582404\n",
      "epoch : 73/90, train_loss = 0.578576\n",
      "epoch : 74/90, train_loss = 0.576644\n",
      "epoch : 75/90, train_loss = 0.573641\n",
      "epoch : 76/90, train_loss = 0.572997\n",
      "epoch : 77/90, train_loss = 0.576234\n",
      "epoch : 78/90, train_loss = 0.575984\n",
      "epoch : 79/90, train_loss = 0.571149\n",
      "epoch : 80/90, train_loss = 0.575188\n",
      "epoch : 81/90, train_loss = 0.575638\n",
      "epoch : 82/90, train_loss = 0.574672\n",
      "epoch : 83/90, train_loss = 0.572438\n",
      "epoch : 84/90, train_loss = 0.569286\n",
      "epoch : 85/90, train_loss = 0.568231\n",
      "epoch : 86/90, train_loss = 0.564418\n",
      "epoch : 87/90, train_loss = 0.562456\n",
      "epoch : 88/90, train_loss = 0.559504\n",
      "epoch : 89/90, train_loss = 0.559952\n",
      "epoch : 90/90, train_loss = 0.559200\n"
     ]
    }
   ],
   "source": [
    "losses_train_final = []\n",
    "\n",
    "epochs=90\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in full_loader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        code, outputs = model(batch_features)\n",
    "        \n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train_final.append(loss)\n",
    "\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(X_full_sc,dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out,out2 = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5faf8a12-a2aa-44f1-b099-02106766ec94</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.640041</td>\n",
       "      <td>-0.651243</td>\n",
       "      <td>-0.515404</td>\n",
       "      <td>-0.495131</td>\n",
       "      <td>-0.517379</td>\n",
       "      <td>0.588646</td>\n",
       "      <td>1.863894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.363178</td>\n",
       "      <td>-0.542616</td>\n",
       "      <td>-0.228780</td>\n",
       "      <td>-0.752959</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.072883</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>1.412789</td>\n",
       "      <td>0.309494</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6855a406-c085-45c7-b789-981786f0c775</td>\n",
       "      <td>0.524985</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.570597</td>\n",
       "      <td>-0.651243</td>\n",
       "      <td>-0.502745</td>\n",
       "      <td>-0.567748</td>\n",
       "      <td>-0.517379</td>\n",
       "      <td>0.496048</td>\n",
       "      <td>0.408932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098739</td>\n",
       "      <td>-0.278106</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>-0.752959</td>\n",
       "      <td>-0.714272</td>\n",
       "      <td>-0.146005</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>-0.440188</td>\n",
       "      <td>-0.172580</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2445ad6-2a6a-4ec3-84d8-93cc3c180a58</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.708642</td>\n",
       "      <td>1.080668</td>\n",
       "      <td>-0.108780</td>\n",
       "      <td>-0.567748</td>\n",
       "      <td>-0.517379</td>\n",
       "      <td>0.522610</td>\n",
       "      <td>1.562211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.774314</td>\n",
       "      <td>-0.542616</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>0.659960</td>\n",
       "      <td>-0.714272</td>\n",
       "      <td>-0.541447</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>-0.440188</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cded92df-9367-4ee5-930c-6c87bf2c8eb0</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.708642</td>\n",
       "      <td>-0.651243</td>\n",
       "      <td>0.383334</td>\n",
       "      <td>-0.567748</td>\n",
       "      <td>-0.517379</td>\n",
       "      <td>0.747962</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653396</td>\n",
       "      <td>-0.542616</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>-0.439350</td>\n",
       "      <td>-0.714272</td>\n",
       "      <td>-0.541447</td>\n",
       "      <td>-0.368792</td>\n",
       "      <td>-0.440188</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b167e70-e4e6-47f7-9fe9-11cf20f0d442</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.575235</td>\n",
       "      <td>-0.361519</td>\n",
       "      <td>-0.559405</td>\n",
       "      <td>-0.049738</td>\n",
       "      <td>-0.517379</td>\n",
       "      <td>-0.508362</td>\n",
       "      <td>-0.064147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>-0.542616</td>\n",
       "      <td>-0.577881</td>\n",
       "      <td>0.292234</td>\n",
       "      <td>0.232081</td>\n",
       "      <td>-0.541447</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>0.329495</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>1120d776-47df-4f72-862e-427d3c44dd43</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>3.368099</td>\n",
       "      <td>-0.708642</td>\n",
       "      <td>-0.651243</td>\n",
       "      <td>2.451207</td>\n",
       "      <td>-0.567748</td>\n",
       "      <td>2.684066</td>\n",
       "      <td>-0.508362</td>\n",
       "      <td>1.334759</td>\n",
       "      <td>...</td>\n",
       "      <td>2.597004</td>\n",
       "      <td>2.762867</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>-0.741728</td>\n",
       "      <td>-0.714272</td>\n",
       "      <td>-0.541447</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>1.133704</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>3.877753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>f9cd1c16-3be3-415c-8d7b-9249c3b1c7fa</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>1.134392</td>\n",
       "      <td>-0.708642</td>\n",
       "      <td>-0.651243</td>\n",
       "      <td>-0.559405</td>\n",
       "      <td>-0.567748</td>\n",
       "      <td>-0.058601</td>\n",
       "      <td>1.706750</td>\n",
       "      <td>1.294286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.774314</td>\n",
       "      <td>-0.542616</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>-0.752959</td>\n",
       "      <td>-0.714272</td>\n",
       "      <td>-0.541447</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>-0.440188</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>218eb2f2-24b1-4e41-9071-26ed97a2edc1</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.385373</td>\n",
       "      <td>-0.651243</td>\n",
       "      <td>-0.559405</td>\n",
       "      <td>0.202025</td>\n",
       "      <td>-0.517379</td>\n",
       "      <td>-0.508362</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.774314</td>\n",
       "      <td>-0.465337</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>-0.752959</td>\n",
       "      <td>1.403707</td>\n",
       "      <td>0.613643</td>\n",
       "      <td>-0.592596</td>\n",
       "      <td>-0.440188</td>\n",
       "      <td>2.280259</td>\n",
       "      <td>0.142929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>978939fa-a27f-46ff-9120-7a43e3588b28</td>\n",
       "      <td>1.776533</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>3.526804</td>\n",
       "      <td>3.228420</td>\n",
       "      <td>2.415606</td>\n",
       "      <td>2.524337</td>\n",
       "      <td>3.241502</td>\n",
       "      <td>-0.508362</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809077</td>\n",
       "      <td>2.943850</td>\n",
       "      <td>3.043370</td>\n",
       "      <td>-0.244963</td>\n",
       "      <td>-0.714272</td>\n",
       "      <td>2.856963</td>\n",
       "      <td>1.960175</td>\n",
       "      <td>1.890877</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>2.132638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>a1b28726-ff2b-4a60-a48a-6e4ce62fa853</td>\n",
       "      <td>-0.582455</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>-0.014624</td>\n",
       "      <td>-0.104007</td>\n",
       "      <td>-0.559405</td>\n",
       "      <td>-0.567748</td>\n",
       "      <td>-0.451548</td>\n",
       "      <td>-0.508362</td>\n",
       "      <td>0.980024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608857</td>\n",
       "      <td>-0.542616</td>\n",
       "      <td>-0.617647</td>\n",
       "      <td>-0.752959</td>\n",
       "      <td>0.198908</td>\n",
       "      <td>-0.541447</td>\n",
       "      <td>-0.577040</td>\n",
       "      <td>-0.440188</td>\n",
       "      <td>-0.563508</td>\n",
       "      <td>-0.453034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file_name         0         1         2  \\\n",
       "0    5faf8a12-a2aa-44f1-b099-02106766ec94 -0.582455 -0.701758 -0.640041   \n",
       "1    6855a406-c085-45c7-b789-981786f0c775  0.524985 -0.701758 -0.570597   \n",
       "2    e2445ad6-2a6a-4ec3-84d8-93cc3c180a58 -0.582455 -0.701758 -0.708642   \n",
       "3    cded92df-9367-4ee5-930c-6c87bf2c8eb0 -0.582455 -0.701758 -0.708642   \n",
       "4    4b167e70-e4e6-47f7-9fe9-11cf20f0d442 -0.582455 -0.701758 -0.575235   \n",
       "..                                    ...       ...       ...       ...   \n",
       "949  1120d776-47df-4f72-862e-427d3c44dd43 -0.582455  3.368099 -0.708642   \n",
       "950  f9cd1c16-3be3-415c-8d7b-9249c3b1c7fa -0.582455  1.134392 -0.708642   \n",
       "951  218eb2f2-24b1-4e41-9071-26ed97a2edc1 -0.582455 -0.701758 -0.385373   \n",
       "952  978939fa-a27f-46ff-9120-7a43e3588b28  1.776533 -0.701758  3.526804   \n",
       "953  a1b28726-ff2b-4a60-a48a-6e4ce62fa853 -0.582455 -0.701758 -0.014624   \n",
       "\n",
       "            3         4         5         6         7         8  ...  \\\n",
       "0   -0.651243 -0.515404 -0.495131 -0.517379  0.588646  1.863894  ...   \n",
       "1   -0.651243 -0.502745 -0.567748 -0.517379  0.496048  0.408932  ...   \n",
       "2    1.080668 -0.108780 -0.567748 -0.517379  0.522610  1.562211  ...   \n",
       "3   -0.651243  0.383334 -0.567748 -0.517379  0.747962 -0.877069  ...   \n",
       "4   -0.361519 -0.559405 -0.049738 -0.517379 -0.508362 -0.064147  ...   \n",
       "..        ...       ...       ...       ...       ...       ...  ...   \n",
       "949 -0.651243  2.451207 -0.567748  2.684066 -0.508362  1.334759  ...   \n",
       "950 -0.651243 -0.559405 -0.567748 -0.058601  1.706750  1.294286  ...   \n",
       "951 -0.651243 -0.559405  0.202025 -0.517379 -0.508362 -0.877069  ...   \n",
       "952  3.228420  2.415606  2.524337  3.241502 -0.508362 -0.877069  ...   \n",
       "953 -0.104007 -0.559405 -0.567748 -0.451548 -0.508362  0.980024  ...   \n",
       "\n",
       "          118       119       120       121       122       123       124  \\\n",
       "0   -0.363178 -0.542616 -0.228780 -0.752959  0.219008  0.072883 -0.592596   \n",
       "1    0.098739 -0.278106 -0.617647 -0.752959 -0.714272 -0.146005 -0.592596   \n",
       "2   -0.774314 -0.542616 -0.617647  0.659960 -0.714272 -0.541447 -0.592596   \n",
       "3   -0.653396 -0.542616 -0.617647 -0.439350 -0.714272 -0.541447 -0.368792   \n",
       "4    0.419900 -0.542616 -0.577881  0.292234  0.232081 -0.541447 -0.592596   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "949  2.597004  2.762867 -0.617647 -0.741728 -0.714272 -0.541447 -0.592596   \n",
       "950 -0.774314 -0.542616 -0.617647 -0.752959 -0.714272 -0.541447 -0.592596   \n",
       "951 -0.774314 -0.465337 -0.617647 -0.752959  1.403707  0.613643 -0.592596   \n",
       "952  0.809077  2.943850  3.043370 -0.244963 -0.714272  2.856963  1.960175   \n",
       "953  0.608857 -0.542616 -0.617647 -0.752959  0.198908 -0.541447 -0.577040   \n",
       "\n",
       "          125       126       127  \n",
       "0    1.412789  0.309494 -0.453034  \n",
       "1   -0.440188 -0.172580 -0.453034  \n",
       "2   -0.440188 -0.563508 -0.453034  \n",
       "3   -0.440188 -0.563508 -0.453034  \n",
       "4    0.329495 -0.563508 -0.453034  \n",
       "..        ...       ...       ...  \n",
       "949  1.133704 -0.563508  3.877753  \n",
       "950 -0.440188 -0.563508 -0.453034  \n",
       "951 -0.440188  2.280259  0.142929  \n",
       "952  1.890877 -0.563508  2.132638  \n",
       "953 -0.440188 -0.563508 -0.453034  \n",
       "\n",
       "[954 rows x 129 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df = pd.DataFrame(out)\n",
    "latent_df.insert(0,'file_name',gene_df['file_name'])\n",
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"../../data/gene_df_128.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ef9037651b8fb300183737a1adf54e758a8413bef4becc8f06877b013d9a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
