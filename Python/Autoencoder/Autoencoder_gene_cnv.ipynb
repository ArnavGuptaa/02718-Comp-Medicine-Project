{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type = Ignore\n",
    "#Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from AEModel import AE, MyDataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "gene_df= pd.read_csv(\"../../data/master_gene_df.csv\",encoding = \"UTF-8\")\n",
    "cnv_df = pd.read_csv(\"../../data/master_cnv_df.csv\",encoding = \"UTF-8\")\n",
    "label_df = pd.read_csv(\"../../data/final_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv_df = cnv_df.merge(label_df[['Case_ID_Final','File ID']], left_on='file_name',right_on = 'File ID', how = 'left')\n",
    "cnv_df.drop(columns=['File ID','file_name'],inplace = True)\n",
    "\n",
    "gene_df = gene_df.merge(label_df[['Case_ID_Final','File ID']], left_on='file_name',right_on = 'File ID', how = 'left')\n",
    "gene_df.drop(columns=['File ID','file_name'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>OR4F5_x</th>\n",
       "      <th>OR4F29_x</th>\n",
       "      <th>OR4F16_x</th>\n",
       "      <th>SAMD11_x</th>\n",
       "      <th>NOC2L_x</th>\n",
       "      <th>KLHL17_x</th>\n",
       "      <th>PLEKHN1_x</th>\n",
       "      <th>PERM1_x</th>\n",
       "      <th>HES4_x</th>\n",
       "      <th>...</th>\n",
       "      <th>DHX34_y</th>\n",
       "      <th>XAGE1B_y</th>\n",
       "      <th>ARL8A_y</th>\n",
       "      <th>KCTD8_y</th>\n",
       "      <th>SLX1B_y</th>\n",
       "      <th>CPNE6_y</th>\n",
       "      <th>XYLB_y</th>\n",
       "      <th>PKN3_y</th>\n",
       "      <th>RGS1_y</th>\n",
       "      <th>PGK1_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-33-4582</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0345</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>69.3370</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>12.0017</td>\n",
       "      <td>3.3554</td>\n",
       "      <td>20.8139</td>\n",
       "      <td>438.9808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-LA-A7SW</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.3901</td>\n",
       "      <td>0.2246</td>\n",
       "      <td>102.4349</td>\n",
       "      <td>1.7566</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>5.4715</td>\n",
       "      <td>36.4585</td>\n",
       "      <td>10.6996</td>\n",
       "      <td>276.3415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-43-5670</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.2041</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>72.1953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>1.9572</td>\n",
       "      <td>9.5935</td>\n",
       "      <td>7.7148</td>\n",
       "      <td>342.4987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-21-5782</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.6639</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>72.4760</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>4.6417</td>\n",
       "      <td>15.3004</td>\n",
       "      <td>13.2821</td>\n",
       "      <td>718.7201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-22-4601</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.5580</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>111.5538</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.7219</td>\n",
       "      <td>14.1261</td>\n",
       "      <td>17.6472</td>\n",
       "      <td>416.1310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        case_id  OR4F5_x  OR4F29_x  OR4F16_x  SAMD11_x  NOC2L_x  KLHL17_x  \\\n",
       "0  TCGA-33-4582      3.0       3.0       3.0       3.0      3.0       3.0   \n",
       "1  TCGA-LA-A7SW      2.0       2.0       2.0       2.0      2.0       2.0   \n",
       "2  TCGA-43-5670      3.0       3.0       3.0       3.0      3.0       3.0   \n",
       "3  TCGA-21-5782      2.0       2.0       2.0       2.0      2.0       2.0   \n",
       "4  TCGA-22-4601      2.0       2.0       2.0       2.0      2.0       2.0   \n",
       "\n",
       "   PLEKHN1_x  PERM1_x  HES4_x  ...  DHX34_y  XAGE1B_y   ARL8A_y  KCTD8_y  \\\n",
       "0        3.0      3.0     3.0  ...  17.0345    0.0000   69.3370   0.0000   \n",
       "1        2.0      2.0     2.0  ...  36.3901    0.2246  102.4349   1.7566   \n",
       "2        3.0      3.0     3.0  ...  23.2041    0.0000   72.1953   0.0000   \n",
       "3        2.0      2.0     2.0  ...  17.6639    0.1635   72.4760   0.8953   \n",
       "4        2.0      2.0     2.0  ...  29.5580    0.0000  111.5538   0.0000   \n",
       "\n",
       "   SLX1B_y  CPNE6_y   XYLB_y   PKN3_y   RGS1_y    PGK1_y  \n",
       "0   0.0000   0.0090  12.0017   3.3554  20.8139  438.9808  \n",
       "1   0.0645   0.0265   5.4715  36.4585  10.6996  276.3415  \n",
       "2   0.0479   0.0197   1.9572   9.5935   7.7148  342.4987  \n",
       "3   0.0000   0.8870   4.6417  15.3004  13.2821  718.7201  \n",
       "4   0.0538   0.0000   7.7219  14.1261  17.6472  416.1310  \n",
       "\n",
       "[5 rows x 38337 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.merge(left=cnv_df, right=gene_df, on='Case_ID_Final')\n",
    "cases = features_df.pop('Case_ID_Final')\n",
    "features_df.insert(0,'case_id',cases)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is used to load the dataset for training\n",
    "pd_train_dataset, pd_test_dataset = train_test_split(features_df.iloc[:,1:], test_size=0.2)\n",
    "\n",
    "X_train_sc = StandardScaler().fit_transform(pd_train_dataset)\n",
    "\n",
    "X_test_sc = StandardScaler().fit_transform(pd_test_dataset)\n",
    "\n",
    "X_full_sc = StandardScaler().fit_transform(features_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MyDataset(X_train_sc)\n",
    "data_test = MyDataset(X_test_sc)\n",
    "data_full = MyDataset(X_full_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(data_train, batch_size=50, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape = len(X_train_sc[0])).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/400, train_loss = 1.488519\n",
      "epoch : 1/400, val_loss = 1.268210\n",
      "epoch : 2/400, train_loss = 1.089929\n",
      "epoch : 2/400, val_loss = 1.203298\n",
      "epoch : 3/400, train_loss = 0.981583\n",
      "epoch : 3/400, val_loss = 0.975801\n",
      "epoch : 4/400, train_loss = 0.916656\n",
      "epoch : 4/400, val_loss = 0.866326\n",
      "epoch : 5/400, train_loss = 0.861270\n",
      "epoch : 5/400, val_loss = 0.829668\n",
      "epoch : 6/400, train_loss = 0.813959\n",
      "epoch : 6/400, val_loss = 0.797318\n",
      "epoch : 7/400, train_loss = 0.773432\n",
      "epoch : 7/400, val_loss = 0.776089\n",
      "epoch : 8/400, train_loss = 0.737133\n",
      "epoch : 8/400, val_loss = 0.761105\n",
      "epoch : 9/400, train_loss = 0.706941\n",
      "epoch : 9/400, val_loss = 0.732441\n",
      "epoch : 10/400, train_loss = 0.678627\n",
      "epoch : 10/400, val_loss = 0.713061\n",
      "epoch : 11/400, train_loss = 0.653937\n",
      "epoch : 11/400, val_loss = 0.690845\n",
      "epoch : 12/400, train_loss = 0.630178\n",
      "epoch : 12/400, val_loss = 0.678003\n",
      "epoch : 13/400, train_loss = 0.610052\n",
      "epoch : 13/400, val_loss = 0.672499\n",
      "epoch : 14/400, train_loss = 0.593126\n",
      "epoch : 14/400, val_loss = 0.666528\n",
      "epoch : 15/400, train_loss = 0.577996\n",
      "epoch : 15/400, val_loss = 0.659688\n",
      "epoch : 16/400, train_loss = 0.564689\n",
      "epoch : 16/400, val_loss = 0.651988\n",
      "epoch : 17/400, train_loss = 0.549661\n",
      "epoch : 17/400, val_loss = 0.645001\n",
      "epoch : 18/400, train_loss = 0.537477\n",
      "epoch : 18/400, val_loss = 0.636919\n",
      "epoch : 19/400, train_loss = 0.524877\n",
      "epoch : 19/400, val_loss = 0.625165\n",
      "epoch : 20/400, train_loss = 0.513663\n",
      "epoch : 20/400, val_loss = 0.628675\n",
      "epoch : 21/400, train_loss = 0.503423\n",
      "epoch : 21/400, val_loss = 0.644911\n",
      "epoch : 22/400, train_loss = 0.495236\n",
      "epoch : 22/400, val_loss = 0.635165\n",
      "epoch : 23/400, train_loss = 0.488393\n",
      "epoch : 23/400, val_loss = 0.631242\n",
      "epoch : 24/400, train_loss = 0.483723\n",
      "epoch : 24/400, val_loss = 0.642975\n",
      "epoch : 25/400, train_loss = 0.480683\n",
      "epoch : 25/400, val_loss = 0.615911\n",
      "epoch : 26/400, train_loss = 0.470224\n",
      "epoch : 26/400, val_loss = 0.622031\n",
      "epoch : 27/400, train_loss = 0.461468\n",
      "epoch : 27/400, val_loss = 0.604699\n",
      "epoch : 28/400, train_loss = 0.449884\n",
      "epoch : 28/400, val_loss = 0.590471\n",
      "epoch : 29/400, train_loss = 0.440830\n",
      "epoch : 29/400, val_loss = 0.590145\n",
      "epoch : 30/400, train_loss = 0.433533\n",
      "epoch : 30/400, val_loss = 0.592096\n",
      "epoch : 31/400, train_loss = 0.426954\n",
      "epoch : 31/400, val_loss = 0.589103\n",
      "epoch : 32/400, train_loss = 0.420103\n",
      "epoch : 32/400, val_loss = 0.583597\n",
      "epoch : 33/400, train_loss = 0.414447\n",
      "epoch : 33/400, val_loss = 0.575965\n",
      "epoch : 34/400, train_loss = 0.409427\n",
      "epoch : 34/400, val_loss = 0.582861\n",
      "epoch : 35/400, train_loss = 0.405946\n",
      "epoch : 35/400, val_loss = 0.584817\n",
      "epoch : 36/400, train_loss = 0.400927\n",
      "epoch : 36/400, val_loss = 0.581734\n",
      "epoch : 37/400, train_loss = 0.396846\n",
      "epoch : 37/400, val_loss = 0.578264\n",
      "epoch : 38/400, train_loss = 0.392562\n",
      "epoch : 38/400, val_loss = 0.573426\n",
      "epoch : 39/400, train_loss = 0.388595\n",
      "epoch : 39/400, val_loss = 0.581961\n",
      "epoch : 40/400, train_loss = 0.385594\n",
      "epoch : 40/400, val_loss = 0.584005\n",
      "epoch : 41/400, train_loss = 0.383000\n",
      "epoch : 41/400, val_loss = 0.580002\n",
      "epoch : 42/400, train_loss = 0.379327\n",
      "epoch : 42/400, val_loss = 0.578360\n",
      "epoch : 43/400, train_loss = 0.375351\n",
      "epoch : 43/400, val_loss = 0.576471\n",
      "epoch : 44/400, train_loss = 0.372508\n",
      "epoch : 44/400, val_loss = 0.576970\n",
      "epoch : 45/400, train_loss = 0.369474\n",
      "epoch : 45/400, val_loss = 0.572462\n",
      "epoch : 46/400, train_loss = 0.367067\n",
      "epoch : 46/400, val_loss = 0.574483\n",
      "epoch : 47/400, train_loss = 0.365407\n",
      "epoch : 47/400, val_loss = 0.577781\n",
      "epoch : 48/400, train_loss = 0.361899\n",
      "epoch : 48/400, val_loss = 0.597005\n",
      "epoch : 49/400, train_loss = 0.362254\n",
      "epoch : 49/400, val_loss = 0.584917\n",
      "epoch : 50/400, train_loss = 0.360635\n",
      "epoch : 50/400, val_loss = 0.573797\n",
      "epoch : 51/400, train_loss = 0.357306\n",
      "epoch : 51/400, val_loss = 0.582394\n",
      "epoch : 52/400, train_loss = 0.356160\n",
      "epoch : 52/400, val_loss = 0.576171\n",
      "epoch : 53/400, train_loss = 0.353151\n",
      "epoch : 53/400, val_loss = 0.582494\n",
      "epoch : 54/400, train_loss = 0.351280\n",
      "epoch : 54/400, val_loss = 0.573930\n",
      "epoch : 55/400, train_loss = 0.347711\n",
      "epoch : 55/400, val_loss = 0.580969\n",
      "epoch : 56/400, train_loss = 0.345126\n",
      "epoch : 56/400, val_loss = 0.583077\n",
      "epoch : 57/400, train_loss = 0.342833\n",
      "epoch : 57/400, val_loss = 0.574055\n",
      "epoch : 58/400, train_loss = 0.339262\n",
      "epoch : 58/400, val_loss = 0.572416\n",
      "epoch : 59/400, train_loss = 0.337853\n",
      "epoch : 59/400, val_loss = 0.570889\n",
      "epoch : 60/400, train_loss = 0.336280\n",
      "epoch : 60/400, val_loss = 0.569794\n",
      "epoch : 61/400, train_loss = 0.334019\n",
      "epoch : 61/400, val_loss = 0.578136\n",
      "epoch : 62/400, train_loss = 0.332427\n",
      "epoch : 62/400, val_loss = 0.576662\n",
      "epoch : 63/400, train_loss = 0.331001\n",
      "epoch : 63/400, val_loss = 0.580608\n",
      "epoch : 64/400, train_loss = 0.329185\n",
      "epoch : 64/400, val_loss = 0.573648\n",
      "epoch : 65/400, train_loss = 0.326912\n",
      "epoch : 65/400, val_loss = 0.572609\n",
      "epoch : 66/400, train_loss = 0.324556\n",
      "epoch : 66/400, val_loss = 0.576199\n",
      "epoch : 67/400, train_loss = 0.324084\n",
      "epoch : 67/400, val_loss = 0.576910\n",
      "epoch : 68/400, train_loss = 0.321326\n",
      "epoch : 68/400, val_loss = 0.573068\n",
      "epoch : 69/400, train_loss = 0.319812\n",
      "epoch : 69/400, val_loss = 0.573671\n",
      "epoch : 70/400, train_loss = 0.319322\n",
      "epoch : 70/400, val_loss = 0.579660\n",
      "epoch : 71/400, train_loss = 0.318726\n",
      "epoch : 71/400, val_loss = 0.579189\n",
      "epoch : 72/400, train_loss = 0.318286\n",
      "epoch : 72/400, val_loss = 0.577462\n",
      "epoch : 73/400, train_loss = 0.317521\n",
      "epoch : 73/400, val_loss = 0.578887\n",
      "epoch : 74/400, train_loss = 0.318245\n",
      "epoch : 74/400, val_loss = 0.578139\n",
      "epoch : 75/400, train_loss = 0.316461\n",
      "epoch : 75/400, val_loss = 0.583370\n",
      "epoch : 76/400, train_loss = 0.314916\n",
      "epoch : 76/400, val_loss = 0.584339\n",
      "epoch : 77/400, train_loss = 0.313231\n",
      "epoch : 77/400, val_loss = 0.570261\n",
      "epoch : 78/400, train_loss = 0.310192\n",
      "epoch : 78/400, val_loss = 0.575598\n",
      "epoch : 79/400, train_loss = 0.308914\n",
      "epoch : 79/400, val_loss = 0.577764\n",
      "epoch : 80/400, train_loss = 0.306914\n",
      "epoch : 80/400, val_loss = 0.574393\n",
      "epoch : 81/400, train_loss = 0.305491\n",
      "epoch : 81/400, val_loss = 0.571886\n",
      "epoch : 82/400, train_loss = 0.303457\n",
      "epoch : 82/400, val_loss = 0.582118\n",
      "epoch : 83/400, train_loss = 0.304113\n",
      "epoch : 83/400, val_loss = 0.583770\n",
      "epoch : 84/400, train_loss = 0.303592\n",
      "epoch : 84/400, val_loss = 0.577335\n",
      "epoch : 85/400, train_loss = 0.302695\n",
      "epoch : 85/400, val_loss = 0.575242\n",
      "epoch : 86/400, train_loss = 0.301950\n",
      "epoch : 86/400, val_loss = 0.579512\n",
      "epoch : 87/400, train_loss = 0.301477\n",
      "epoch : 87/400, val_loss = 0.568842\n",
      "epoch : 88/400, train_loss = 0.297992\n",
      "epoch : 88/400, val_loss = 0.580769\n",
      "epoch : 89/400, train_loss = 0.296686\n",
      "epoch : 89/400, val_loss = 0.575214\n",
      "epoch : 90/400, train_loss = 0.295038\n",
      "epoch : 90/400, val_loss = 0.572788\n",
      "epoch : 91/400, train_loss = 0.293435\n",
      "epoch : 91/400, val_loss = 0.581183\n",
      "epoch : 92/400, train_loss = 0.293084\n",
      "epoch : 92/400, val_loss = 0.576722\n",
      "epoch : 93/400, train_loss = 0.292917\n",
      "epoch : 93/400, val_loss = 0.571200\n",
      "epoch : 94/400, train_loss = 0.291275\n",
      "epoch : 94/400, val_loss = 0.580749\n",
      "epoch : 95/400, train_loss = 0.292091\n",
      "epoch : 95/400, val_loss = 0.580519\n",
      "epoch : 96/400, train_loss = 0.291192\n",
      "epoch : 96/400, val_loss = 0.579634\n",
      "epoch : 97/400, train_loss = 0.290917\n",
      "epoch : 97/400, val_loss = 0.577544\n",
      "epoch : 98/400, train_loss = 0.289611\n",
      "epoch : 98/400, val_loss = 0.574733\n",
      "epoch : 99/400, train_loss = 0.287708\n",
      "epoch : 99/400, val_loss = 0.573629\n",
      "epoch : 100/400, train_loss = 0.285902\n",
      "epoch : 100/400, val_loss = 0.575934\n",
      "epoch : 101/400, train_loss = 0.284581\n",
      "epoch : 101/400, val_loss = 0.574916\n",
      "epoch : 102/400, train_loss = 0.282988\n",
      "epoch : 102/400, val_loss = 0.579042\n",
      "epoch : 103/400, train_loss = 0.282215\n",
      "epoch : 103/400, val_loss = 0.578456\n",
      "epoch : 104/400, train_loss = 0.281459\n",
      "epoch : 104/400, val_loss = 0.574449\n",
      "epoch : 105/400, train_loss = 0.280977\n",
      "epoch : 105/400, val_loss = 0.579045\n",
      "epoch : 106/400, train_loss = 0.281185\n",
      "epoch : 106/400, val_loss = 0.580929\n",
      "epoch : 107/400, train_loss = 0.281812\n",
      "epoch : 107/400, val_loss = 0.582231\n",
      "epoch : 108/400, train_loss = 0.282437\n",
      "epoch : 108/400, val_loss = 0.583344\n",
      "epoch : 109/400, train_loss = 0.282102\n",
      "epoch : 109/400, val_loss = 0.586502\n",
      "epoch : 110/400, train_loss = 0.282456\n",
      "epoch : 110/400, val_loss = 0.583814\n",
      "epoch : 111/400, train_loss = 0.282832\n",
      "epoch : 111/400, val_loss = 0.582036\n",
      "epoch : 112/400, train_loss = 0.281682\n",
      "epoch : 112/400, val_loss = 0.579610\n",
      "epoch : 113/400, train_loss = 0.280358\n",
      "epoch : 113/400, val_loss = 0.585589\n",
      "epoch : 114/400, train_loss = 0.279600\n",
      "epoch : 114/400, val_loss = 0.584820\n",
      "epoch : 115/400, train_loss = 0.277889\n",
      "epoch : 115/400, val_loss = 0.583655\n",
      "epoch : 116/400, train_loss = 0.277552\n",
      "epoch : 116/400, val_loss = 0.575840\n",
      "epoch : 117/400, train_loss = 0.276903\n",
      "epoch : 117/400, val_loss = 0.578614\n",
      "epoch : 118/400, train_loss = 0.277454\n",
      "epoch : 118/400, val_loss = 0.579778\n",
      "epoch : 119/400, train_loss = 0.277494\n",
      "epoch : 119/400, val_loss = 0.592027\n",
      "epoch : 120/400, train_loss = 0.278069\n",
      "epoch : 120/400, val_loss = 0.593850\n",
      "epoch : 121/400, train_loss = 0.278720\n",
      "epoch : 121/400, val_loss = 0.593751\n",
      "epoch : 122/400, train_loss = 0.278699\n",
      "epoch : 122/400, val_loss = 0.582238\n",
      "epoch : 123/400, train_loss = 0.278088\n",
      "epoch : 123/400, val_loss = 0.582604\n",
      "epoch : 124/400, train_loss = 0.278174\n",
      "epoch : 124/400, val_loss = 0.580529\n",
      "epoch : 125/400, train_loss = 0.277250\n",
      "epoch : 125/400, val_loss = 0.581423\n",
      "epoch : 126/400, train_loss = 0.276138\n",
      "epoch : 126/400, val_loss = 0.592963\n",
      "epoch : 127/400, train_loss = 0.275025\n",
      "epoch : 127/400, val_loss = 0.593561\n",
      "epoch : 128/400, train_loss = 0.274792\n",
      "epoch : 128/400, val_loss = 0.589291\n",
      "epoch : 129/400, train_loss = 0.274400\n",
      "epoch : 129/400, val_loss = 0.581733\n",
      "epoch : 130/400, train_loss = 0.274058\n",
      "epoch : 130/400, val_loss = 0.585653\n",
      "epoch : 131/400, train_loss = 0.274734\n",
      "epoch : 131/400, val_loss = 0.593982\n",
      "epoch : 132/400, train_loss = 0.277959\n",
      "epoch : 132/400, val_loss = 0.586507\n",
      "epoch : 133/400, train_loss = 0.277530\n",
      "epoch : 133/400, val_loss = 0.590053\n",
      "epoch : 134/400, train_loss = 0.277511\n",
      "epoch : 134/400, val_loss = 0.612542\n",
      "epoch : 135/400, train_loss = 0.282278\n",
      "epoch : 135/400, val_loss = 0.588537\n",
      "epoch : 136/400, train_loss = 0.279024\n",
      "epoch : 136/400, val_loss = 0.593772\n",
      "epoch : 137/400, train_loss = 0.279505\n",
      "epoch : 137/400, val_loss = 0.585813\n",
      "epoch : 138/400, train_loss = 0.277417\n",
      "epoch : 138/400, val_loss = 0.576102\n",
      "epoch : 139/400, train_loss = 0.273816\n",
      "epoch : 139/400, val_loss = 0.583379\n",
      "epoch : 140/400, train_loss = 0.270821\n",
      "epoch : 140/400, val_loss = 0.593592\n",
      "epoch : 141/400, train_loss = 0.268346\n",
      "epoch : 141/400, val_loss = 0.584129\n",
      "epoch : 142/400, train_loss = 0.264997\n",
      "epoch : 142/400, val_loss = 0.577353\n",
      "epoch : 143/400, train_loss = 0.264250\n",
      "epoch : 143/400, val_loss = 0.572406\n",
      "epoch : 144/400, train_loss = 0.262627\n",
      "epoch : 144/400, val_loss = 0.578977\n",
      "epoch : 145/400, train_loss = 0.261883\n",
      "epoch : 145/400, val_loss = 0.583496\n",
      "epoch : 146/400, train_loss = 0.261510\n",
      "epoch : 146/400, val_loss = 0.584162\n",
      "epoch : 147/400, train_loss = 0.261652\n",
      "epoch : 147/400, val_loss = 0.583633\n",
      "epoch : 148/400, train_loss = 0.262234\n",
      "epoch : 148/400, val_loss = 0.579293\n",
      "epoch : 149/400, train_loss = 0.262892\n",
      "epoch : 149/400, val_loss = 0.583258\n",
      "epoch : 150/400, train_loss = 0.263937\n",
      "epoch : 150/400, val_loss = 0.600536\n",
      "epoch : 151/400, train_loss = 0.266116\n",
      "epoch : 151/400, val_loss = 0.597911\n",
      "epoch : 152/400, train_loss = 0.266460\n",
      "epoch : 152/400, val_loss = 0.584428\n",
      "epoch : 153/400, train_loss = 0.267188\n",
      "epoch : 153/400, val_loss = 0.586446\n",
      "epoch : 154/400, train_loss = 0.268562\n",
      "epoch : 154/400, val_loss = 0.597554\n",
      "epoch : 155/400, train_loss = 0.270667\n",
      "epoch : 155/400, val_loss = 0.585199\n",
      "epoch : 156/400, train_loss = 0.268446\n",
      "epoch : 156/400, val_loss = 0.595286\n",
      "epoch : 157/400, train_loss = 0.267773\n",
      "epoch : 157/400, val_loss = 0.599198\n",
      "epoch : 158/400, train_loss = 0.268294\n",
      "epoch : 158/400, val_loss = 0.589894\n",
      "epoch : 159/400, train_loss = 0.266382\n",
      "epoch : 159/400, val_loss = 0.589282\n",
      "epoch : 160/400, train_loss = 0.266215\n",
      "epoch : 160/400, val_loss = 0.583532\n",
      "epoch : 161/400, train_loss = 0.264508\n",
      "epoch : 161/400, val_loss = 0.586107\n",
      "epoch : 162/400, train_loss = 0.262386\n",
      "epoch : 162/400, val_loss = 0.588061\n",
      "epoch : 163/400, train_loss = 0.260144\n",
      "epoch : 163/400, val_loss = 0.585619\n",
      "epoch : 164/400, train_loss = 0.258005\n",
      "epoch : 164/400, val_loss = 0.583063\n",
      "epoch : 165/400, train_loss = 0.256586\n",
      "epoch : 165/400, val_loss = 0.579830\n",
      "epoch : 166/400, train_loss = 0.256011\n",
      "epoch : 166/400, val_loss = 0.579623\n",
      "epoch : 167/400, train_loss = 0.255279\n",
      "epoch : 167/400, val_loss = 0.582337\n",
      "epoch : 168/400, train_loss = 0.255272\n",
      "epoch : 168/400, val_loss = 0.585296\n",
      "epoch : 169/400, train_loss = 0.254842\n",
      "epoch : 169/400, val_loss = 0.589560\n",
      "epoch : 170/400, train_loss = 0.255036\n",
      "epoch : 170/400, val_loss = 0.587748\n",
      "epoch : 171/400, train_loss = 0.255058\n",
      "epoch : 171/400, val_loss = 0.583195\n",
      "epoch : 172/400, train_loss = 0.255296\n",
      "epoch : 172/400, val_loss = 0.581935\n",
      "epoch : 173/400, train_loss = 0.256299\n",
      "epoch : 173/400, val_loss = 0.582483\n",
      "epoch : 174/400, train_loss = 0.256140\n",
      "epoch : 174/400, val_loss = 0.591871\n",
      "epoch : 175/400, train_loss = 0.256826\n",
      "epoch : 175/400, val_loss = 0.593687\n",
      "epoch : 176/400, train_loss = 0.256858\n",
      "epoch : 176/400, val_loss = 0.594476\n",
      "epoch : 177/400, train_loss = 0.257617\n",
      "epoch : 177/400, val_loss = 0.590710\n",
      "epoch : 178/400, train_loss = 0.257858\n",
      "epoch : 178/400, val_loss = 0.587611\n",
      "epoch : 179/400, train_loss = 0.257176\n",
      "epoch : 179/400, val_loss = 0.584075\n",
      "epoch : 180/400, train_loss = 0.257717\n",
      "epoch : 180/400, val_loss = 0.583753\n",
      "epoch : 181/400, train_loss = 0.256306\n",
      "epoch : 181/400, val_loss = 0.592240\n",
      "epoch : 182/400, train_loss = 0.256476\n",
      "epoch : 182/400, val_loss = 0.594136\n",
      "epoch : 183/400, train_loss = 0.256079\n",
      "epoch : 183/400, val_loss = 0.586691\n",
      "epoch : 184/400, train_loss = 0.256149\n",
      "epoch : 184/400, val_loss = 0.586111\n",
      "epoch : 185/400, train_loss = 0.256846\n",
      "epoch : 185/400, val_loss = 0.586868\n",
      "epoch : 186/400, train_loss = 0.256596\n",
      "epoch : 186/400, val_loss = 0.588656\n",
      "epoch : 187/400, train_loss = 0.258056\n",
      "epoch : 187/400, val_loss = 0.589627\n",
      "epoch : 188/400, train_loss = 0.257845\n",
      "epoch : 188/400, val_loss = 0.594102\n",
      "epoch : 189/400, train_loss = 0.258373\n",
      "epoch : 189/400, val_loss = 0.597123\n",
      "epoch : 190/400, train_loss = 0.258743\n",
      "epoch : 190/400, val_loss = 0.592958\n",
      "epoch : 191/400, train_loss = 0.258618\n",
      "epoch : 191/400, val_loss = 0.591578\n",
      "epoch : 192/400, train_loss = 0.257774\n",
      "epoch : 192/400, val_loss = 0.590735\n",
      "epoch : 193/400, train_loss = 0.257094\n",
      "epoch : 193/400, val_loss = 0.587629\n",
      "epoch : 194/400, train_loss = 0.256050\n",
      "epoch : 194/400, val_loss = 0.585863\n",
      "epoch : 195/400, train_loss = 0.255218\n",
      "epoch : 195/400, val_loss = 0.596854\n",
      "epoch : 196/400, train_loss = 0.255996\n",
      "epoch : 196/400, val_loss = 0.594211\n",
      "epoch : 197/400, train_loss = 0.256012\n",
      "epoch : 197/400, val_loss = 0.594240\n",
      "epoch : 198/400, train_loss = 0.255727\n",
      "epoch : 198/400, val_loss = 0.587771\n",
      "epoch : 199/400, train_loss = 0.255516\n",
      "epoch : 199/400, val_loss = 0.583365\n",
      "epoch : 200/400, train_loss = 0.254391\n",
      "epoch : 200/400, val_loss = 0.589694\n",
      "epoch : 201/400, train_loss = 0.254389\n",
      "epoch : 201/400, val_loss = 0.595122\n",
      "epoch : 202/400, train_loss = 0.253568\n",
      "epoch : 202/400, val_loss = 0.595775\n",
      "epoch : 203/400, train_loss = 0.253231\n",
      "epoch : 203/400, val_loss = 0.592765\n",
      "epoch : 204/400, train_loss = 0.253444\n",
      "epoch : 204/400, val_loss = 0.587167\n",
      "epoch : 205/400, train_loss = 0.253066\n",
      "epoch : 205/400, val_loss = 0.588722\n",
      "epoch : 206/400, train_loss = 0.253818\n",
      "epoch : 206/400, val_loss = 0.583198\n",
      "epoch : 207/400, train_loss = 0.253013\n",
      "epoch : 207/400, val_loss = 0.598319\n",
      "epoch : 208/400, train_loss = 0.252852\n",
      "epoch : 208/400, val_loss = 0.597543\n",
      "epoch : 209/400, train_loss = 0.252560\n",
      "epoch : 209/400, val_loss = 0.590918\n",
      "epoch : 210/400, train_loss = 0.251828\n",
      "epoch : 210/400, val_loss = 0.584408\n",
      "epoch : 211/400, train_loss = 0.251585\n",
      "epoch : 211/400, val_loss = 0.583670\n",
      "epoch : 212/400, train_loss = 0.251873\n",
      "epoch : 212/400, val_loss = 0.604392\n",
      "epoch : 213/400, train_loss = 0.251329\n",
      "epoch : 213/400, val_loss = 0.596701\n",
      "epoch : 214/400, train_loss = 0.251570\n",
      "epoch : 214/400, val_loss = 0.597859\n",
      "epoch : 215/400, train_loss = 0.250906\n",
      "epoch : 215/400, val_loss = 0.591892\n",
      "epoch : 216/400, train_loss = 0.250422\n",
      "epoch : 216/400, val_loss = 0.587627\n",
      "epoch : 217/400, train_loss = 0.250384\n",
      "epoch : 217/400, val_loss = 0.582885\n",
      "epoch : 218/400, train_loss = 0.249858\n",
      "epoch : 218/400, val_loss = 0.593279\n",
      "epoch : 219/400, train_loss = 0.249498\n",
      "epoch : 219/400, val_loss = 0.601767\n",
      "epoch : 220/400, train_loss = 0.250540\n",
      "epoch : 220/400, val_loss = 0.597555\n",
      "epoch : 221/400, train_loss = 0.250951\n",
      "epoch : 221/400, val_loss = 0.587237\n",
      "epoch : 222/400, train_loss = 0.252751\n",
      "epoch : 222/400, val_loss = 0.585918\n",
      "epoch : 223/400, train_loss = 0.252560\n",
      "epoch : 223/400, val_loss = 0.593653\n",
      "epoch : 224/400, train_loss = 0.252305\n",
      "epoch : 224/400, val_loss = 0.601826\n",
      "epoch : 225/400, train_loss = 0.252015\n",
      "epoch : 225/400, val_loss = 0.597873\n",
      "epoch : 226/400, train_loss = 0.251236\n",
      "epoch : 226/400, val_loss = 0.588033\n",
      "epoch : 227/400, train_loss = 0.251387\n",
      "epoch : 227/400, val_loss = 0.588032\n",
      "epoch : 228/400, train_loss = 0.251478\n",
      "epoch : 228/400, val_loss = 0.588175\n",
      "epoch : 229/400, train_loss = 0.250081\n",
      "epoch : 229/400, val_loss = 0.598728\n",
      "epoch : 230/400, train_loss = 0.249673\n",
      "epoch : 230/400, val_loss = 0.598607\n",
      "epoch : 231/400, train_loss = 0.249539\n",
      "epoch : 231/400, val_loss = 0.590478\n",
      "epoch : 232/400, train_loss = 0.249450\n",
      "epoch : 232/400, val_loss = 0.586734\n",
      "epoch : 233/400, train_loss = 0.249247\n",
      "epoch : 233/400, val_loss = 0.593096\n",
      "epoch : 234/400, train_loss = 0.248965\n",
      "epoch : 234/400, val_loss = 0.593719\n",
      "epoch : 235/400, train_loss = 0.248705\n",
      "epoch : 235/400, val_loss = 0.593959\n",
      "epoch : 236/400, train_loss = 0.247961\n",
      "epoch : 236/400, val_loss = 0.592417\n",
      "epoch : 237/400, train_loss = 0.247306\n",
      "epoch : 237/400, val_loss = 0.590522\n",
      "epoch : 238/400, train_loss = 0.246752\n",
      "epoch : 238/400, val_loss = 0.591735\n",
      "epoch : 239/400, train_loss = 0.246459\n",
      "epoch : 239/400, val_loss = 0.591046\n",
      "epoch : 240/400, train_loss = 0.246509\n",
      "epoch : 240/400, val_loss = 0.590577\n",
      "epoch : 241/400, train_loss = 0.246604\n",
      "epoch : 241/400, val_loss = 0.590345\n",
      "epoch : 242/400, train_loss = 0.246617\n",
      "epoch : 242/400, val_loss = 0.595087\n",
      "epoch : 243/400, train_loss = 0.246649\n",
      "epoch : 243/400, val_loss = 0.594148\n",
      "epoch : 244/400, train_loss = 0.246759\n",
      "epoch : 244/400, val_loss = 0.591372\n",
      "epoch : 245/400, train_loss = 0.246344\n",
      "epoch : 245/400, val_loss = 0.592682\n",
      "epoch : 246/400, train_loss = 0.246547\n",
      "epoch : 246/400, val_loss = 0.590248\n",
      "epoch : 247/400, train_loss = 0.246442\n",
      "epoch : 247/400, val_loss = 0.589191\n",
      "epoch : 248/400, train_loss = 0.245496\n",
      "epoch : 248/400, val_loss = 0.592489\n",
      "epoch : 249/400, train_loss = 0.245523\n",
      "epoch : 249/400, val_loss = 0.592690\n",
      "epoch : 250/400, train_loss = 0.244851\n",
      "epoch : 250/400, val_loss = 0.594060\n",
      "epoch : 251/400, train_loss = 0.245484\n",
      "epoch : 251/400, val_loss = 0.591793\n",
      "epoch : 252/400, train_loss = 0.244887\n",
      "epoch : 252/400, val_loss = 0.588831\n",
      "epoch : 253/400, train_loss = 0.244884\n",
      "epoch : 253/400, val_loss = 0.587797\n",
      "epoch : 254/400, train_loss = 0.244634\n",
      "epoch : 254/400, val_loss = 0.590644\n",
      "epoch : 255/400, train_loss = 0.243863\n",
      "epoch : 255/400, val_loss = 0.596765\n",
      "epoch : 256/400, train_loss = 0.244210\n",
      "epoch : 256/400, val_loss = 0.593472\n",
      "epoch : 257/400, train_loss = 0.244255\n",
      "epoch : 257/400, val_loss = 0.591041\n",
      "epoch : 258/400, train_loss = 0.244524\n",
      "epoch : 258/400, val_loss = 0.590084\n",
      "epoch : 259/400, train_loss = 0.245221\n",
      "epoch : 259/400, val_loss = 0.589943\n",
      "epoch : 260/400, train_loss = 0.245422\n",
      "epoch : 260/400, val_loss = 0.597430\n",
      "epoch : 261/400, train_loss = 0.246101\n",
      "epoch : 261/400, val_loss = 0.601448\n",
      "epoch : 262/400, train_loss = 0.245555\n",
      "epoch : 262/400, val_loss = 0.597938\n",
      "epoch : 263/400, train_loss = 0.246725\n",
      "epoch : 263/400, val_loss = 0.589034\n",
      "epoch : 264/400, train_loss = 0.247201\n",
      "epoch : 264/400, val_loss = 0.590098\n",
      "epoch : 265/400, train_loss = 0.247211\n",
      "epoch : 265/400, val_loss = 0.597810\n",
      "epoch : 266/400, train_loss = 0.246918\n",
      "epoch : 266/400, val_loss = 0.605477\n",
      "epoch : 267/400, train_loss = 0.246092\n",
      "epoch : 267/400, val_loss = 0.604393\n",
      "epoch : 268/400, train_loss = 0.246034\n",
      "epoch : 268/400, val_loss = 0.589211\n",
      "epoch : 269/400, train_loss = 0.246201\n",
      "epoch : 269/400, val_loss = 0.588789\n",
      "epoch : 270/400, train_loss = 0.246278\n",
      "epoch : 270/400, val_loss = 0.597171\n",
      "epoch : 271/400, train_loss = 0.246013\n",
      "epoch : 271/400, val_loss = 0.597080\n",
      "epoch : 272/400, train_loss = 0.244953\n",
      "epoch : 272/400, val_loss = 0.596056\n",
      "epoch : 273/400, train_loss = 0.244246\n",
      "epoch : 273/400, val_loss = 0.597579\n",
      "epoch : 274/400, train_loss = 0.244036\n",
      "epoch : 274/400, val_loss = 0.593115\n",
      "epoch : 275/400, train_loss = 0.244528\n",
      "epoch : 275/400, val_loss = 0.590013\n",
      "epoch : 276/400, train_loss = 0.244923\n",
      "epoch : 276/400, val_loss = 0.594130\n",
      "epoch : 277/400, train_loss = 0.245447\n",
      "epoch : 277/400, val_loss = 0.597923\n",
      "epoch : 278/400, train_loss = 0.245790\n",
      "epoch : 278/400, val_loss = 0.600072\n",
      "epoch : 279/400, train_loss = 0.244650\n",
      "epoch : 279/400, val_loss = 0.599904\n",
      "epoch : 280/400, train_loss = 0.244866\n",
      "epoch : 280/400, val_loss = 0.592565\n",
      "epoch : 281/400, train_loss = 0.244157\n",
      "epoch : 281/400, val_loss = 0.590181\n",
      "epoch : 282/400, train_loss = 0.244492\n",
      "epoch : 282/400, val_loss = 0.588213\n",
      "epoch : 283/400, train_loss = 0.244292\n",
      "epoch : 283/400, val_loss = 0.597084\n",
      "epoch : 284/400, train_loss = 0.244319\n",
      "epoch : 284/400, val_loss = 0.600125\n",
      "epoch : 285/400, train_loss = 0.243962\n",
      "epoch : 285/400, val_loss = 0.599224\n",
      "epoch : 286/400, train_loss = 0.243454\n",
      "epoch : 286/400, val_loss = 0.590680\n",
      "epoch : 287/400, train_loss = 0.243852\n",
      "epoch : 287/400, val_loss = 0.589178\n",
      "epoch : 288/400, train_loss = 0.243257\n",
      "epoch : 288/400, val_loss = 0.596306\n",
      "epoch : 289/400, train_loss = 0.244034\n",
      "epoch : 289/400, val_loss = 0.590331\n",
      "epoch : 290/400, train_loss = 0.243020\n",
      "epoch : 290/400, val_loss = 0.601552\n",
      "epoch : 291/400, train_loss = 0.243735\n",
      "epoch : 291/400, val_loss = 0.604359\n",
      "epoch : 292/400, train_loss = 0.244222\n",
      "epoch : 292/400, val_loss = 0.590842\n",
      "epoch : 293/400, train_loss = 0.243455\n",
      "epoch : 293/400, val_loss = 0.591919\n",
      "epoch : 294/400, train_loss = 0.245417\n",
      "epoch : 294/400, val_loss = 0.593671\n",
      "epoch : 295/400, train_loss = 0.246284\n",
      "epoch : 295/400, val_loss = 0.601330\n",
      "epoch : 296/400, train_loss = 0.246337\n",
      "epoch : 296/400, val_loss = 0.604818\n",
      "epoch : 297/400, train_loss = 0.246849\n",
      "epoch : 297/400, val_loss = 0.597031\n",
      "epoch : 298/400, train_loss = 0.246649\n",
      "epoch : 298/400, val_loss = 0.594456\n",
      "epoch : 299/400, train_loss = 0.245488\n",
      "epoch : 299/400, val_loss = 0.592858\n",
      "epoch : 300/400, train_loss = 0.244865\n",
      "epoch : 300/400, val_loss = 0.596246\n",
      "epoch : 301/400, train_loss = 0.243792\n",
      "epoch : 301/400, val_loss = 0.602191\n",
      "epoch : 302/400, train_loss = 0.243136\n",
      "epoch : 302/400, val_loss = 0.595044\n",
      "epoch : 303/400, train_loss = 0.241812\n",
      "epoch : 303/400, val_loss = 0.588037\n",
      "epoch : 304/400, train_loss = 0.241140\n",
      "epoch : 304/400, val_loss = 0.591253\n",
      "epoch : 305/400, train_loss = 0.240737\n",
      "epoch : 305/400, val_loss = 0.597437\n",
      "epoch : 306/400, train_loss = 0.240230\n",
      "epoch : 306/400, val_loss = 0.604894\n",
      "epoch : 307/400, train_loss = 0.240017\n",
      "epoch : 307/400, val_loss = 0.595066\n",
      "epoch : 308/400, train_loss = 0.239692\n",
      "epoch : 308/400, val_loss = 0.587625\n",
      "epoch : 309/400, train_loss = 0.239997\n",
      "epoch : 309/400, val_loss = 0.589556\n",
      "epoch : 310/400, train_loss = 0.240043\n",
      "epoch : 310/400, val_loss = 0.602914\n",
      "epoch : 311/400, train_loss = 0.239899\n",
      "epoch : 311/400, val_loss = 0.603729\n",
      "epoch : 312/400, train_loss = 0.239683\n",
      "epoch : 312/400, val_loss = 0.594075\n",
      "epoch : 313/400, train_loss = 0.239243\n",
      "epoch : 313/400, val_loss = 0.590332\n",
      "epoch : 314/400, train_loss = 0.239370\n",
      "epoch : 314/400, val_loss = 0.592439\n",
      "epoch : 315/400, train_loss = 0.239230\n",
      "epoch : 315/400, val_loss = 0.596459\n",
      "epoch : 316/400, train_loss = 0.238989\n",
      "epoch : 316/400, val_loss = 0.596936\n",
      "epoch : 317/400, train_loss = 0.239346\n",
      "epoch : 317/400, val_loss = 0.597627\n",
      "epoch : 318/400, train_loss = 0.239414\n",
      "epoch : 318/400, val_loss = 0.596450\n",
      "epoch : 319/400, train_loss = 0.239391\n",
      "epoch : 319/400, val_loss = 0.600995\n",
      "epoch : 320/400, train_loss = 0.240890\n",
      "epoch : 320/400, val_loss = 0.600697\n",
      "epoch : 321/400, train_loss = 0.240986\n",
      "epoch : 321/400, val_loss = 0.598594\n",
      "epoch : 322/400, train_loss = 0.240990\n",
      "epoch : 322/400, val_loss = 0.596816\n",
      "epoch : 323/400, train_loss = 0.242259\n",
      "epoch : 323/400, val_loss = 0.598928\n",
      "epoch : 324/400, train_loss = 0.241653\n",
      "epoch : 324/400, val_loss = 0.602091\n",
      "epoch : 325/400, train_loss = 0.241796\n",
      "epoch : 325/400, val_loss = 0.600326\n",
      "epoch : 326/400, train_loss = 0.241203\n",
      "epoch : 326/400, val_loss = 0.595953\n",
      "epoch : 327/400, train_loss = 0.240572\n",
      "epoch : 327/400, val_loss = 0.597653\n",
      "epoch : 328/400, train_loss = 0.241225\n",
      "epoch : 328/400, val_loss = 0.597663\n",
      "epoch : 329/400, train_loss = 0.240292\n",
      "epoch : 329/400, val_loss = 0.597529\n",
      "epoch : 330/400, train_loss = 0.239890\n",
      "epoch : 330/400, val_loss = 0.595993\n",
      "epoch : 331/400, train_loss = 0.239013\n",
      "epoch : 331/400, val_loss = 0.599560\n",
      "epoch : 332/400, train_loss = 0.238640\n",
      "epoch : 332/400, val_loss = 0.596777\n",
      "epoch : 333/400, train_loss = 0.238218\n",
      "epoch : 333/400, val_loss = 0.594777\n",
      "epoch : 334/400, train_loss = 0.237662\n",
      "epoch : 334/400, val_loss = 0.593713\n",
      "epoch : 335/400, train_loss = 0.237068\n",
      "epoch : 335/400, val_loss = 0.592229\n",
      "epoch : 336/400, train_loss = 0.236738\n",
      "epoch : 336/400, val_loss = 0.595814\n",
      "epoch : 337/400, train_loss = 0.236139\n",
      "epoch : 337/400, val_loss = 0.602409\n",
      "epoch : 338/400, train_loss = 0.236496\n",
      "epoch : 338/400, val_loss = 0.596288\n",
      "epoch : 339/400, train_loss = 0.236727\n",
      "epoch : 339/400, val_loss = 0.592670\n",
      "epoch : 340/400, train_loss = 0.237459\n",
      "epoch : 340/400, val_loss = 0.596147\n",
      "epoch : 341/400, train_loss = 0.238432\n",
      "epoch : 341/400, val_loss = 0.600719\n",
      "epoch : 342/400, train_loss = 0.239923\n",
      "epoch : 342/400, val_loss = 0.605627\n",
      "epoch : 343/400, train_loss = 0.241460\n",
      "epoch : 343/400, val_loss = 0.603989\n",
      "epoch : 344/400, train_loss = 0.242326\n",
      "epoch : 344/400, val_loss = 0.601973\n",
      "epoch : 345/400, train_loss = 0.243350\n",
      "epoch : 345/400, val_loss = 0.593489\n",
      "epoch : 346/400, train_loss = 0.243210\n",
      "epoch : 346/400, val_loss = 0.602605\n",
      "epoch : 347/400, train_loss = 0.243713\n",
      "epoch : 347/400, val_loss = 0.610699\n",
      "epoch : 348/400, train_loss = 0.243757\n",
      "epoch : 348/400, val_loss = 0.601433\n",
      "epoch : 349/400, train_loss = 0.242207\n",
      "epoch : 349/400, val_loss = 0.596791\n",
      "epoch : 350/400, train_loss = 0.242092\n",
      "epoch : 350/400, val_loss = 0.590884\n",
      "epoch : 351/400, train_loss = 0.240007\n",
      "epoch : 351/400, val_loss = 0.599667\n",
      "epoch : 352/400, train_loss = 0.240113\n",
      "epoch : 352/400, val_loss = 0.600624\n",
      "epoch : 353/400, train_loss = 0.238859\n",
      "epoch : 353/400, val_loss = 0.598553\n",
      "epoch : 354/400, train_loss = 0.237993\n",
      "epoch : 354/400, val_loss = 0.595093\n",
      "epoch : 355/400, train_loss = 0.237813\n",
      "epoch : 355/400, val_loss = 0.593177\n",
      "epoch : 356/400, train_loss = 0.237151\n",
      "epoch : 356/400, val_loss = 0.597891\n",
      "epoch : 357/400, train_loss = 0.237295\n",
      "epoch : 357/400, val_loss = 0.595979\n",
      "epoch : 358/400, train_loss = 0.237356\n",
      "epoch : 358/400, val_loss = 0.597738\n",
      "epoch : 359/400, train_loss = 0.237666\n",
      "epoch : 359/400, val_loss = 0.601623\n",
      "epoch : 360/400, train_loss = 0.236837\n",
      "epoch : 360/400, val_loss = 0.598916\n",
      "epoch : 361/400, train_loss = 0.236828\n",
      "epoch : 361/400, val_loss = 0.594238\n",
      "epoch : 362/400, train_loss = 0.236930\n",
      "epoch : 362/400, val_loss = 0.594937\n",
      "epoch : 363/400, train_loss = 0.237392\n",
      "epoch : 363/400, val_loss = 0.602922\n",
      "epoch : 364/400, train_loss = 0.238122\n",
      "epoch : 364/400, val_loss = 0.602178\n",
      "epoch : 365/400, train_loss = 0.238318\n",
      "epoch : 365/400, val_loss = 0.598149\n",
      "epoch : 366/400, train_loss = 0.238253\n",
      "epoch : 366/400, val_loss = 0.598569\n",
      "epoch : 367/400, train_loss = 0.238611\n",
      "epoch : 367/400, val_loss = 0.598549\n",
      "epoch : 368/400, train_loss = 0.238740\n",
      "epoch : 368/400, val_loss = 0.598916\n",
      "epoch : 369/400, train_loss = 0.238328\n",
      "epoch : 369/400, val_loss = 0.601127\n",
      "epoch : 370/400, train_loss = 0.238810\n",
      "epoch : 370/400, val_loss = 0.601374\n",
      "epoch : 371/400, train_loss = 0.238546\n",
      "epoch : 371/400, val_loss = 0.601092\n",
      "epoch : 372/400, train_loss = 0.238018\n",
      "epoch : 372/400, val_loss = 0.600675\n",
      "epoch : 373/400, train_loss = 0.238659\n",
      "epoch : 373/400, val_loss = 0.605065\n",
      "epoch : 374/400, train_loss = 0.238540\n",
      "epoch : 374/400, val_loss = 0.600077\n",
      "epoch : 375/400, train_loss = 0.238670\n",
      "epoch : 375/400, val_loss = 0.602578\n",
      "epoch : 376/400, train_loss = 0.239172\n",
      "epoch : 376/400, val_loss = 0.595511\n",
      "epoch : 377/400, train_loss = 0.238143\n",
      "epoch : 377/400, val_loss = 0.598534\n",
      "epoch : 378/400, train_loss = 0.237919\n",
      "epoch : 378/400, val_loss = 0.605832\n",
      "epoch : 379/400, train_loss = 0.238148\n",
      "epoch : 379/400, val_loss = 0.599305\n",
      "epoch : 380/400, train_loss = 0.237114\n",
      "epoch : 380/400, val_loss = 0.597059\n",
      "epoch : 381/400, train_loss = 0.237129\n",
      "epoch : 381/400, val_loss = 0.594513\n",
      "epoch : 382/400, train_loss = 0.236605\n",
      "epoch : 382/400, val_loss = 0.599424\n",
      "epoch : 383/400, train_loss = 0.236371\n",
      "epoch : 383/400, val_loss = 0.604159\n",
      "epoch : 384/400, train_loss = 0.235976\n",
      "epoch : 384/400, val_loss = 0.599734\n",
      "epoch : 385/400, train_loss = 0.236030\n",
      "epoch : 385/400, val_loss = 0.611823\n",
      "epoch : 386/400, train_loss = 0.235714\n",
      "epoch : 386/400, val_loss = 0.598092\n",
      "epoch : 387/400, train_loss = 0.235714\n",
      "epoch : 387/400, val_loss = 0.597917\n",
      "epoch : 388/400, train_loss = 0.235657\n",
      "epoch : 388/400, val_loss = 0.599876\n",
      "epoch : 389/400, train_loss = 0.235759\n",
      "epoch : 389/400, val_loss = 0.600548\n",
      "epoch : 390/400, train_loss = 0.235319\n",
      "epoch : 390/400, val_loss = 0.597912\n",
      "epoch : 391/400, train_loss = 0.235432\n",
      "epoch : 391/400, val_loss = 0.595547\n",
      "epoch : 392/400, train_loss = 0.236505\n",
      "epoch : 392/400, val_loss = 0.595111\n",
      "epoch : 393/400, train_loss = 0.236052\n",
      "epoch : 393/400, val_loss = 0.600554\n",
      "epoch : 394/400, train_loss = 0.234897\n",
      "epoch : 394/400, val_loss = 0.598990\n",
      "epoch : 395/400, train_loss = 0.234487\n",
      "epoch : 395/400, val_loss = 0.594891\n",
      "epoch : 396/400, train_loss = 0.234080\n",
      "epoch : 396/400, val_loss = 0.593643\n",
      "epoch : 397/400, train_loss = 0.233742\n",
      "epoch : 397/400, val_loss = 0.597333\n",
      "epoch : 398/400, train_loss = 0.233272\n",
      "epoch : 398/400, val_loss = 0.599730\n",
      "epoch : 399/400, train_loss = 0.233039\n",
      "epoch : 399/400, val_loss = 0.598996\n",
      "epoch : 400/400, train_loss = 0.233038\n",
      "epoch : 400/400, val_loss = 0.594779\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "epochs=400\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        lol, outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train.append(loss)\n",
    "\n",
    "\n",
    "    #For Valid Loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            code, outputs = model(batch)\n",
    "            loss_val =criterion(outputs, batch)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    losses_val.append(val_loss)\n",
    "\n",
    "\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    print(\"epoch : {}/{}, val_loss = {:.6f}\".format(epoch + 1, epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train, label = 'train_loss')\n",
    "plt.plot(losses_val, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"../../Figures/AE_gene_cnv.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Gene Autoencoder Taining\n",
    "\n",
    "data_full = MyDataset(X_full_sc)\n",
    "full_loader = DataLoader(data_full, batch_size=50)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE(input_shape = len(X_full_sc[0])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/80, train_loss = 1.785536\n",
      "epoch : 2/80, train_loss = 1.304665\n",
      "epoch : 3/80, train_loss = 1.176756\n",
      "epoch : 4/80, train_loss = 1.093735\n",
      "epoch : 5/80, train_loss = 1.020123\n",
      "epoch : 6/80, train_loss = 0.955296\n",
      "epoch : 7/80, train_loss = 0.900209\n",
      "epoch : 8/80, train_loss = 0.856856\n",
      "epoch : 9/80, train_loss = 0.821656\n",
      "epoch : 10/80, train_loss = 0.792760\n",
      "epoch : 11/80, train_loss = 0.764800\n",
      "epoch : 12/80, train_loss = 0.740721\n",
      "epoch : 13/80, train_loss = 0.719580\n",
      "epoch : 14/80, train_loss = 0.700032\n",
      "epoch : 15/80, train_loss = 0.681254\n",
      "epoch : 16/80, train_loss = 0.665278\n",
      "epoch : 17/80, train_loss = 0.651479\n",
      "epoch : 18/80, train_loss = 0.636396\n",
      "epoch : 19/80, train_loss = 0.622839\n",
      "epoch : 20/80, train_loss = 0.611078\n",
      "epoch : 21/80, train_loss = 0.599934\n",
      "epoch : 22/80, train_loss = 0.589722\n",
      "epoch : 23/80, train_loss = 0.579130\n",
      "epoch : 24/80, train_loss = 0.570061\n",
      "epoch : 25/80, train_loss = 0.561771\n",
      "epoch : 26/80, train_loss = 0.554958\n",
      "epoch : 27/80, train_loss = 0.546107\n",
      "epoch : 28/80, train_loss = 0.540787\n",
      "epoch : 29/80, train_loss = 0.535441\n",
      "epoch : 30/80, train_loss = 0.527312\n",
      "epoch : 31/80, train_loss = 0.520244\n",
      "epoch : 32/80, train_loss = 0.516281\n",
      "epoch : 33/80, train_loss = 0.512740\n",
      "epoch : 34/80, train_loss = 0.510365\n",
      "epoch : 35/80, train_loss = 0.508741\n",
      "epoch : 36/80, train_loss = 0.505621\n",
      "epoch : 37/80, train_loss = 0.501635\n",
      "epoch : 38/80, train_loss = 0.497924\n",
      "epoch : 39/80, train_loss = 0.493207\n",
      "epoch : 40/80, train_loss = 0.490086\n",
      "epoch : 41/80, train_loss = 0.485422\n",
      "epoch : 42/80, train_loss = 0.481055\n",
      "epoch : 43/80, train_loss = 0.476709\n",
      "epoch : 44/80, train_loss = 0.471268\n",
      "epoch : 45/80, train_loss = 0.467385\n",
      "epoch : 46/80, train_loss = 0.463407\n",
      "epoch : 47/80, train_loss = 0.459324\n",
      "epoch : 48/80, train_loss = 0.456537\n",
      "epoch : 49/80, train_loss = 0.452563\n",
      "epoch : 50/80, train_loss = 0.448576\n",
      "epoch : 51/80, train_loss = 0.446870\n",
      "epoch : 52/80, train_loss = 0.445526\n",
      "epoch : 53/80, train_loss = 0.443972\n",
      "epoch : 54/80, train_loss = 0.443720\n",
      "epoch : 55/80, train_loss = 0.443885\n",
      "epoch : 56/80, train_loss = 0.443122\n",
      "epoch : 57/80, train_loss = 0.443924\n",
      "epoch : 58/80, train_loss = 0.444316\n",
      "epoch : 59/80, train_loss = 0.442093\n",
      "epoch : 60/80, train_loss = 0.438929\n",
      "epoch : 61/80, train_loss = 0.436722\n",
      "epoch : 62/80, train_loss = 0.435335\n",
      "epoch : 63/80, train_loss = 0.432459\n",
      "epoch : 64/80, train_loss = 0.427069\n",
      "epoch : 65/80, train_loss = 0.422202\n",
      "epoch : 66/80, train_loss = 0.416083\n",
      "epoch : 67/80, train_loss = 0.412462\n",
      "epoch : 68/80, train_loss = 0.409055\n",
      "epoch : 69/80, train_loss = 0.405403\n",
      "epoch : 70/80, train_loss = 0.403816\n",
      "epoch : 71/80, train_loss = 0.400866\n",
      "epoch : 72/80, train_loss = 0.398523\n",
      "epoch : 73/80, train_loss = 0.397160\n",
      "epoch : 74/80, train_loss = 0.396279\n",
      "epoch : 75/80, train_loss = 0.396550\n",
      "epoch : 76/80, train_loss = 0.395710\n",
      "epoch : 77/80, train_loss = 0.395265\n",
      "epoch : 78/80, train_loss = 0.394616\n",
      "epoch : 79/80, train_loss = 0.395557\n",
      "epoch : 80/80, train_loss = 0.394030\n"
     ]
    }
   ],
   "source": [
    "losses_train_final = []\n",
    "\n",
    "epochs=80\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in full_loader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        code, outputs = model(batch_features)\n",
    "        \n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train_final.append(loss)\n",
    "\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(X_full_sc,dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out,out2 = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-33-4582</td>\n",
       "      <td>-0.538320</td>\n",
       "      <td>1.032815</td>\n",
       "      <td>-0.207708</td>\n",
       "      <td>0.828670</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>1.257552</td>\n",
       "      <td>-0.617863</td>\n",
       "      <td>1.119022</td>\n",
       "      <td>-0.152546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>-0.698807</td>\n",
       "      <td>1.450487</td>\n",
       "      <td>-0.447062</td>\n",
       "      <td>-0.620233</td>\n",
       "      <td>0.452437</td>\n",
       "      <td>-0.694994</td>\n",
       "      <td>-0.713119</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-LA-A7SW</td>\n",
       "      <td>1.959446</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>-0.752460</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>2.197323</td>\n",
       "      <td>-0.617863</td>\n",
       "      <td>-0.551743</td>\n",
       "      <td>-0.643232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>-0.698807</td>\n",
       "      <td>4.055389</td>\n",
       "      <td>1.378998</td>\n",
       "      <td>-0.067373</td>\n",
       "      <td>2.029083</td>\n",
       "      <td>2.765881</td>\n",
       "      <td>-0.707433</td>\n",
       "      <td>-0.304322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-43-5670</td>\n",
       "      <td>0.832612</td>\n",
       "      <td>1.379605</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>-0.446544</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.724448</td>\n",
       "      <td>-0.617863</td>\n",
       "      <td>1.812993</td>\n",
       "      <td>0.738104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411734</td>\n",
       "      <td>-0.297049</td>\n",
       "      <td>1.678690</td>\n",
       "      <td>1.570264</td>\n",
       "      <td>0.516068</td>\n",
       "      <td>0.069641</td>\n",
       "      <td>1.477633</td>\n",
       "      <td>0.332023</td>\n",
       "      <td>-0.713119</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-21-5782</td>\n",
       "      <td>6.541538</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>-0.752460</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.738057</td>\n",
       "      <td>-0.239469</td>\n",
       "      <td>4.893093</td>\n",
       "      <td>3.645806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>0.407032</td>\n",
       "      <td>-0.765153</td>\n",
       "      <td>-0.695914</td>\n",
       "      <td>-0.620233</td>\n",
       "      <td>3.810906</td>\n",
       "      <td>-0.694994</td>\n",
       "      <td>-0.713119</td>\n",
       "      <td>-0.088886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-22-4601</td>\n",
       "      <td>0.420577</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>-0.752460</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.738057</td>\n",
       "      <td>1.990268</td>\n",
       "      <td>-0.394083</td>\n",
       "      <td>-0.643232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595246</td>\n",
       "      <td>-0.676838</td>\n",
       "      <td>1.811497</td>\n",
       "      <td>-0.765153</td>\n",
       "      <td>-0.695914</td>\n",
       "      <td>-0.620233</td>\n",
       "      <td>-0.754544</td>\n",
       "      <td>0.549223</td>\n",
       "      <td>0.589029</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>TCGA-58-8388</td>\n",
       "      <td>4.492478</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>1.445392</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.738057</td>\n",
       "      <td>2.077010</td>\n",
       "      <td>2.291249</td>\n",
       "      <td>-0.643232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>6.711696</td>\n",
       "      <td>-0.765153</td>\n",
       "      <td>-0.695914</td>\n",
       "      <td>-0.620233</td>\n",
       "      <td>-0.754544</td>\n",
       "      <td>1.866920</td>\n",
       "      <td>2.036840</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>TCGA-77-A5GB</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>-0.752460</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.738057</td>\n",
       "      <td>-0.617863</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>-0.643232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>-0.698807</td>\n",
       "      <td>-0.765153</td>\n",
       "      <td>-0.380807</td>\n",
       "      <td>-0.620233</td>\n",
       "      <td>-0.581701</td>\n",
       "      <td>-0.694994</td>\n",
       "      <td>0.200771</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>TCGA-55-6543</td>\n",
       "      <td>-0.347010</td>\n",
       "      <td>-0.216690</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>-0.266531</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.738057</td>\n",
       "      <td>-0.617863</td>\n",
       "      <td>0.505401</td>\n",
       "      <td>0.980330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>-0.698807</td>\n",
       "      <td>-0.126764</td>\n",
       "      <td>-0.695914</td>\n",
       "      <td>0.260744</td>\n",
       "      <td>1.484166</td>\n",
       "      <td>0.442391</td>\n",
       "      <td>-0.307955</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>TCGA-77-A5G3</td>\n",
       "      <td>-0.538320</td>\n",
       "      <td>0.039443</td>\n",
       "      <td>1.119696</td>\n",
       "      <td>2.052179</td>\n",
       "      <td>-0.088726</td>\n",
       "      <td>2.116628</td>\n",
       "      <td>-0.617863</td>\n",
       "      <td>-0.778814</td>\n",
       "      <td>2.570894</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063605</td>\n",
       "      <td>0.958087</td>\n",
       "      <td>-0.698807</td>\n",
       "      <td>1.971468</td>\n",
       "      <td>0.556211</td>\n",
       "      <td>-0.179993</td>\n",
       "      <td>-0.754544</td>\n",
       "      <td>2.736202</td>\n",
       "      <td>-0.713119</td>\n",
       "      <td>1.657057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>TCGA-NC-A5HD</td>\n",
       "      <td>2.349193</td>\n",
       "      <td>-0.998570</td>\n",
       "      <td>-0.849680</td>\n",
       "      <td>-0.113716</td>\n",
       "      <td>-0.533079</td>\n",
       "      <td>-0.738057</td>\n",
       "      <td>3.240299</td>\n",
       "      <td>2.163172</td>\n",
       "      <td>-0.643232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655609</td>\n",
       "      <td>-0.711586</td>\n",
       "      <td>1.336786</td>\n",
       "      <td>-0.765153</td>\n",
       "      <td>-0.695914</td>\n",
       "      <td>-0.620233</td>\n",
       "      <td>-0.754544</td>\n",
       "      <td>-0.694994</td>\n",
       "      <td>1.967589</td>\n",
       "      <td>-0.344749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          case_id         0         1         2         3         4         5  \\\n",
       "0    TCGA-33-4582 -0.538320  1.032815 -0.207708  0.828670 -0.533079  1.257552   \n",
       "1    TCGA-LA-A7SW  1.959446 -0.998570  0.372400 -0.752460 -0.533079  2.197323   \n",
       "2    TCGA-43-5670  0.832612  1.379605 -0.849680 -0.446544 -0.533079 -0.724448   \n",
       "3    TCGA-21-5782  6.541538 -0.998570 -0.849680 -0.752460 -0.533079 -0.738057   \n",
       "4    TCGA-22-4601  0.420577 -0.998570 -0.849680 -0.752460 -0.533079 -0.738057   \n",
       "..            ...       ...       ...       ...       ...       ...       ...   \n",
       "949  TCGA-58-8388  4.492478 -0.998570 -0.849680  1.445392 -0.533079 -0.738057   \n",
       "950  TCGA-77-A5GB -0.004110 -0.998570 -0.849680 -0.752460 -0.533079 -0.738057   \n",
       "951  TCGA-55-6543 -0.347010 -0.216690 -0.849680 -0.266531 -0.533079 -0.738057   \n",
       "952  TCGA-77-A5G3 -0.538320  0.039443  1.119696  2.052179 -0.088726  2.116628   \n",
       "953  TCGA-NC-A5HD  2.349193 -0.998570 -0.849680 -0.113716 -0.533079 -0.738057   \n",
       "\n",
       "            6         7         8  ...       118       119       120  \\\n",
       "0   -0.617863  1.119022 -0.152546  ... -0.655609 -0.711586 -0.698807   \n",
       "1   -0.617863 -0.551743 -0.643232  ... -0.655609 -0.711586 -0.698807   \n",
       "2   -0.617863  1.812993  0.738104  ... -0.411734 -0.297049  1.678690   \n",
       "3   -0.239469  4.893093  3.645806  ... -0.655609 -0.711586  0.407032   \n",
       "4    1.990268 -0.394083 -0.643232  ... -0.595246 -0.676838  1.811497   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "949  2.077010  2.291249 -0.643232  ... -0.655609 -0.711586  6.711696   \n",
       "950 -0.617863  0.256559 -0.643232  ... -0.655609 -0.711586 -0.698807   \n",
       "951 -0.617863  0.505401  0.980330  ... -0.655609 -0.711586 -0.698807   \n",
       "952 -0.617863 -0.778814  2.570894  ...  2.063605  0.958087 -0.698807   \n",
       "953  3.240299  2.163172 -0.643232  ... -0.655609 -0.711586  1.336786   \n",
       "\n",
       "          121       122       123       124       125       126       127  \n",
       "0    1.450487 -0.447062 -0.620233  0.452437 -0.694994 -0.713119 -0.344749  \n",
       "1    4.055389  1.378998 -0.067373  2.029083  2.765881 -0.707433 -0.304322  \n",
       "2    1.570264  0.516068  0.069641  1.477633  0.332023 -0.713119 -0.344749  \n",
       "3   -0.765153 -0.695914 -0.620233  3.810906 -0.694994 -0.713119 -0.088886  \n",
       "4   -0.765153 -0.695914 -0.620233 -0.754544  0.549223  0.589029 -0.344749  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "949 -0.765153 -0.695914 -0.620233 -0.754544  1.866920  2.036840 -0.344749  \n",
       "950 -0.765153 -0.380807 -0.620233 -0.581701 -0.694994  0.200771 -0.344749  \n",
       "951 -0.126764 -0.695914  0.260744  1.484166  0.442391 -0.307955 -0.344749  \n",
       "952  1.971468  0.556211 -0.179993 -0.754544  2.736202 -0.713119  1.657057  \n",
       "953 -0.765153 -0.695914 -0.620233 -0.754544 -0.694994  1.967589 -0.344749  \n",
       "\n",
       "[954 rows x 129 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df = pd.DataFrame(out)\n",
    "latent_df.insert(0,'case_id',features_df['case_id'])\n",
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"../../data/gene_cnv_df_128.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ef9037651b8fb300183737a1adf54e758a8413bef4becc8f06877b013d9a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
