{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type = Ignore\n",
    "#Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from AEModel import AE, MyDataset,AE_T\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "gene_df= pd.read_csv(\"../../data/master_gene_df.csv\",encoding = \"UTF-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSPYL1</th>\n",
       "      <th>TRIM40</th>\n",
       "      <th>PCGF2</th>\n",
       "      <th>GLT8D2</th>\n",
       "      <th>MLKL</th>\n",
       "      <th>TRPC1</th>\n",
       "      <th>NOP14</th>\n",
       "      <th>PTK7</th>\n",
       "      <th>SAAL1</th>\n",
       "      <th>LRRC27</th>\n",
       "      <th>...</th>\n",
       "      <th>DHX34</th>\n",
       "      <th>XAGE1B</th>\n",
       "      <th>ARL8A</th>\n",
       "      <th>KCTD8</th>\n",
       "      <th>SLX1B</th>\n",
       "      <th>CPNE6</th>\n",
       "      <th>XYLB</th>\n",
       "      <th>PKN3</th>\n",
       "      <th>RGS1</th>\n",
       "      <th>PGK1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.4909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.6829</td>\n",
       "      <td>6.5735</td>\n",
       "      <td>11.8354</td>\n",
       "      <td>4.3273</td>\n",
       "      <td>17.0597</td>\n",
       "      <td>76.0102</td>\n",
       "      <td>13.5971</td>\n",
       "      <td>4.1564</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1412</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>114.5090</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.3251</td>\n",
       "      <td>12.6058</td>\n",
       "      <td>84.0652</td>\n",
       "      <td>515.9193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.3660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.3977</td>\n",
       "      <td>12.3956</td>\n",
       "      <td>6.7753</td>\n",
       "      <td>3.4088</td>\n",
       "      <td>26.2333</td>\n",
       "      <td>35.2884</td>\n",
       "      <td>11.8437</td>\n",
       "      <td>1.0874</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5863</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>93.0715</td>\n",
       "      <td>0.3294</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>2.6886</td>\n",
       "      <td>11.6752</td>\n",
       "      <td>82.8888</td>\n",
       "      <td>1444.9458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.5962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0650</td>\n",
       "      <td>5.2642</td>\n",
       "      <td>4.2537</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>25.3952</td>\n",
       "      <td>64.0538</td>\n",
       "      <td>34.0402</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>...</td>\n",
       "      <td>28.4462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>82.7308</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.1222</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.2784</td>\n",
       "      <td>7.2688</td>\n",
       "      <td>39.6819</td>\n",
       "      <td>632.6572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.8982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.7856</td>\n",
       "      <td>6.6122</td>\n",
       "      <td>5.0003</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>30.4176</td>\n",
       "      <td>69.6955</td>\n",
       "      <td>33.3931</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>...</td>\n",
       "      <td>20.8353</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>168.9003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>7.0244</td>\n",
       "      <td>15.3454</td>\n",
       "      <td>26.6279</td>\n",
       "      <td>682.4480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.1942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5414</td>\n",
       "      <td>5.0653</td>\n",
       "      <td>9.2886</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>30.6175</td>\n",
       "      <td>83.9273</td>\n",
       "      <td>10.7675</td>\n",
       "      <td>2.1584</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3766</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>106.8909</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>1.1573</td>\n",
       "      <td>3.3286</td>\n",
       "      <td>154.2987</td>\n",
       "      <td>436.9842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TSPYL1  TRIM40    PCGF2   GLT8D2     MLKL   TRPC1    NOP14     PTK7  \\\n",
       "0  24.4909     0.0  21.6829   6.5735  11.8354  4.3273  17.0597  76.0102   \n",
       "1  12.3660     0.0  24.3977  12.3956   6.7753  3.4088  26.2333  35.2884   \n",
       "2  41.5962     0.0  22.0650   5.2642   4.2537  0.8289  25.3952  64.0538   \n",
       "3  30.8982     0.0  14.7856   6.6122   5.0003  0.9631  30.4176  69.6955   \n",
       "4  14.1942     0.0   6.5414   5.0653   9.2886  0.9809  30.6175  83.9273   \n",
       "\n",
       "     SAAL1  LRRC27  ...    DHX34  XAGE1B     ARL8A   KCTD8   SLX1B   CPNE6  \\\n",
       "0  13.5971  4.1564  ...  13.1412  0.0000  114.5090  0.0000  0.0000  0.0000   \n",
       "1  11.8437  1.0874  ...   7.5863  0.1099   93.0715  0.3294  0.0000  0.0065   \n",
       "2  34.0402  0.9742  ...  28.4462  0.0000   82.7308  0.0277  0.1222  0.0000   \n",
       "3  33.3931  0.7268  ...  20.8353  0.0000  168.9003  0.0000  0.0229  0.0471   \n",
       "4  10.7675  2.1584  ...  15.3766  0.0000  106.8909  0.0000  0.0000  0.0414   \n",
       "\n",
       "     XYLB     PKN3      RGS1       PGK1  \n",
       "0  3.3251  12.6058   84.0652   515.9193  \n",
       "1  2.6886  11.6752   82.8888  1444.9458  \n",
       "2  4.2784   7.2688   39.6819   632.6572  \n",
       "3  7.0244  15.3454   26.6279   682.4480  \n",
       "4  1.1573   3.3286  154.2987   436.9842  \n",
       "\n",
       "[5 rows x 19188 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(gene_df.iloc[:,1:].shape[1])\n",
    "gene_df.iloc[:,1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is used to load the dataset for training\n",
    "pd_train_dataset, pd_test_dataset = train_test_split(gene_df.iloc[:,1:], test_size=0.2)\n",
    "\n",
    "X_train_sc = StandardScaler().fit_transform(pd_train_dataset)\n",
    "\n",
    "X_test_sc = StandardScaler().fit_transform(pd_test_dataset)\n",
    "\n",
    "X_full_sc = StandardScaler().fit_transform(gene_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MyDataset(X_train_sc)\n",
    "data_test = MyDataset(X_test_sc)\n",
    "data_full = MyDataset(X_full_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(data_train, batch_size=50, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE_T(input_shape = len(X_train_sc[0])).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/500, train_loss = 1.824395\n",
      "epoch : 1/500, val_loss = 3.987213\n",
      "epoch : 2/500, train_loss = 1.757535\n",
      "epoch : 2/500, val_loss = 2.136973\n",
      "epoch : 3/500, train_loss = 1.707388\n",
      "epoch : 3/500, val_loss = 1.900142\n",
      "epoch : 4/500, train_loss = 1.662113\n",
      "epoch : 4/500, val_loss = 1.799905\n",
      "epoch : 5/500, train_loss = 1.619903\n",
      "epoch : 5/500, val_loss = 1.731619\n",
      "epoch : 6/500, train_loss = 1.579950\n",
      "epoch : 6/500, val_loss = 1.677388\n",
      "epoch : 7/500, train_loss = 1.541862\n",
      "epoch : 7/500, val_loss = 1.631017\n",
      "epoch : 8/500, train_loss = 1.505302\n",
      "epoch : 8/500, val_loss = 1.588291\n",
      "epoch : 9/500, train_loss = 1.470164\n",
      "epoch : 9/500, val_loss = 1.548403\n",
      "epoch : 10/500, train_loss = 1.436386\n",
      "epoch : 10/500, val_loss = 1.510835\n",
      "epoch : 11/500, train_loss = 1.403810\n",
      "epoch : 11/500, val_loss = 1.475304\n",
      "epoch : 12/500, train_loss = 1.372384\n",
      "epoch : 12/500, val_loss = 1.441194\n",
      "epoch : 13/500, train_loss = 1.342087\n",
      "epoch : 13/500, val_loss = 1.407299\n",
      "epoch : 14/500, train_loss = 1.312897\n",
      "epoch : 14/500, val_loss = 1.375102\n",
      "epoch : 15/500, train_loss = 1.284790\n",
      "epoch : 15/500, val_loss = 1.343708\n",
      "epoch : 16/500, train_loss = 1.257810\n",
      "epoch : 16/500, val_loss = 1.313972\n",
      "epoch : 17/500, train_loss = 1.231958\n",
      "epoch : 17/500, val_loss = 1.285593\n",
      "epoch : 18/500, train_loss = 1.207275\n",
      "epoch : 18/500, val_loss = 1.258419\n",
      "epoch : 19/500, train_loss = 1.183783\n",
      "epoch : 19/500, val_loss = 1.232604\n",
      "epoch : 20/500, train_loss = 1.161452\n",
      "epoch : 20/500, val_loss = 1.208014\n",
      "epoch : 21/500, train_loss = 1.140280\n",
      "epoch : 21/500, val_loss = 1.184247\n",
      "epoch : 22/500, train_loss = 1.120235\n",
      "epoch : 22/500, val_loss = 1.161601\n",
      "epoch : 23/500, train_loss = 1.101284\n",
      "epoch : 23/500, val_loss = 1.140376\n",
      "epoch : 24/500, train_loss = 1.083394\n",
      "epoch : 24/500, val_loss = 1.120399\n",
      "epoch : 25/500, train_loss = 1.066500\n",
      "epoch : 25/500, val_loss = 1.101462\n",
      "epoch : 26/500, train_loss = 1.050514\n",
      "epoch : 26/500, val_loss = 1.083685\n",
      "epoch : 27/500, train_loss = 1.035397\n",
      "epoch : 27/500, val_loss = 1.066999\n",
      "epoch : 28/500, train_loss = 1.021115\n",
      "epoch : 28/500, val_loss = 1.051254\n",
      "epoch : 29/500, train_loss = 1.007627\n",
      "epoch : 29/500, val_loss = 1.036407\n",
      "epoch : 30/500, train_loss = 0.994894\n",
      "epoch : 30/500, val_loss = 1.022464\n",
      "epoch : 31/500, train_loss = 0.982865\n",
      "epoch : 31/500, val_loss = 1.009238\n",
      "epoch : 32/500, train_loss = 0.971478\n",
      "epoch : 32/500, val_loss = 0.996835\n",
      "epoch : 33/500, train_loss = 0.960712\n",
      "epoch : 33/500, val_loss = 0.985197\n",
      "epoch : 34/500, train_loss = 0.950534\n",
      "epoch : 34/500, val_loss = 0.974137\n",
      "epoch : 35/500, train_loss = 0.940897\n",
      "epoch : 35/500, val_loss = 0.963697\n",
      "epoch : 36/500, train_loss = 0.931771\n",
      "epoch : 36/500, val_loss = 0.953835\n",
      "epoch : 37/500, train_loss = 0.923123\n",
      "epoch : 37/500, val_loss = 0.944537\n",
      "epoch : 38/500, train_loss = 0.914921\n",
      "epoch : 38/500, val_loss = 0.935726\n",
      "epoch : 39/500, train_loss = 0.907135\n",
      "epoch : 39/500, val_loss = 0.927476\n",
      "epoch : 40/500, train_loss = 0.899732\n",
      "epoch : 40/500, val_loss = 0.919654\n",
      "epoch : 41/500, train_loss = 0.892689\n",
      "epoch : 41/500, val_loss = 0.912247\n",
      "epoch : 42/500, train_loss = 0.885999\n",
      "epoch : 42/500, val_loss = 0.905288\n",
      "epoch : 43/500, train_loss = 0.879609\n",
      "epoch : 43/500, val_loss = 0.898575\n",
      "epoch : 44/500, train_loss = 0.873510\n",
      "epoch : 44/500, val_loss = 0.892275\n",
      "epoch : 45/500, train_loss = 0.867689\n",
      "epoch : 45/500, val_loss = 0.886302\n",
      "epoch : 46/500, train_loss = 0.862132\n",
      "epoch : 46/500, val_loss = 0.880645\n",
      "epoch : 47/500, train_loss = 0.856815\n",
      "epoch : 47/500, val_loss = 0.875235\n",
      "epoch : 48/500, train_loss = 0.851726\n",
      "epoch : 48/500, val_loss = 0.870108\n",
      "epoch : 49/500, train_loss = 0.846852\n",
      "epoch : 49/500, val_loss = 0.865178\n",
      "epoch : 50/500, train_loss = 0.842177\n",
      "epoch : 50/500, val_loss = 0.860456\n",
      "epoch : 51/500, train_loss = 0.837698\n",
      "epoch : 51/500, val_loss = 0.856004\n",
      "epoch : 52/500, train_loss = 0.833416\n",
      "epoch : 52/500, val_loss = 0.851725\n",
      "epoch : 53/500, train_loss = 0.829291\n",
      "epoch : 53/500, val_loss = 0.847626\n",
      "epoch : 54/500, train_loss = 0.825319\n",
      "epoch : 54/500, val_loss = 0.843691\n",
      "epoch : 55/500, train_loss = 0.821555\n",
      "epoch : 55/500, val_loss = 0.840082\n",
      "epoch : 56/500, train_loss = 0.817979\n",
      "epoch : 56/500, val_loss = 0.836488\n",
      "epoch : 57/500, train_loss = 0.814593\n",
      "epoch : 57/500, val_loss = 0.833167\n",
      "epoch : 58/500, train_loss = 0.811484\n",
      "epoch : 58/500, val_loss = 0.829965\n",
      "epoch : 59/500, train_loss = 0.808105\n",
      "epoch : 59/500, val_loss = 0.826744\n",
      "epoch : 60/500, train_loss = 0.804841\n",
      "epoch : 60/500, val_loss = 0.823638\n",
      "epoch : 61/500, train_loss = 0.801755\n",
      "epoch : 61/500, val_loss = 0.820660\n",
      "epoch : 62/500, train_loss = 0.798784\n",
      "epoch : 62/500, val_loss = 0.817841\n",
      "epoch : 63/500, train_loss = 0.795911\n",
      "epoch : 63/500, val_loss = 0.815119\n",
      "epoch : 64/500, train_loss = 0.793130\n",
      "epoch : 64/500, val_loss = 0.812460\n",
      "epoch : 65/500, train_loss = 0.790438\n",
      "epoch : 65/500, val_loss = 0.810162\n",
      "epoch : 66/500, train_loss = 0.787866\n",
      "epoch : 66/500, val_loss = 0.807659\n",
      "epoch : 67/500, train_loss = 0.785304\n",
      "epoch : 67/500, val_loss = 0.805197\n",
      "epoch : 68/500, train_loss = 0.782783\n",
      "epoch : 68/500, val_loss = 0.802923\n",
      "epoch : 69/500, train_loss = 0.780427\n",
      "epoch : 69/500, val_loss = 0.800728\n",
      "epoch : 70/500, train_loss = 0.778022\n",
      "epoch : 70/500, val_loss = 0.798532\n",
      "epoch : 71/500, train_loss = 0.775663\n",
      "epoch : 71/500, val_loss = 0.796356\n",
      "epoch : 72/500, train_loss = 0.773371\n",
      "epoch : 72/500, val_loss = 0.794315\n",
      "epoch : 73/500, train_loss = 0.771170\n",
      "epoch : 73/500, val_loss = 0.792270\n",
      "epoch : 74/500, train_loss = 0.768984\n",
      "epoch : 74/500, val_loss = 0.790182\n",
      "epoch : 75/500, train_loss = 0.766922\n",
      "epoch : 75/500, val_loss = 0.788288\n",
      "epoch : 76/500, train_loss = 0.764832\n",
      "epoch : 76/500, val_loss = 0.786385\n",
      "epoch : 77/500, train_loss = 0.763044\n",
      "epoch : 77/500, val_loss = 0.784737\n",
      "epoch : 78/500, train_loss = 0.761258\n",
      "epoch : 78/500, val_loss = 0.783059\n",
      "epoch : 79/500, train_loss = 0.759248\n",
      "epoch : 79/500, val_loss = 0.781236\n",
      "epoch : 80/500, train_loss = 0.757266\n",
      "epoch : 80/500, val_loss = 0.779396\n",
      "epoch : 81/500, train_loss = 0.755265\n",
      "epoch : 81/500, val_loss = 0.777665\n",
      "epoch : 82/500, train_loss = 0.753354\n",
      "epoch : 82/500, val_loss = 0.775969\n",
      "epoch : 83/500, train_loss = 0.751551\n",
      "epoch : 83/500, val_loss = 0.774363\n",
      "epoch : 84/500, train_loss = 0.749914\n",
      "epoch : 84/500, val_loss = 0.772868\n",
      "epoch : 85/500, train_loss = 0.748087\n",
      "epoch : 85/500, val_loss = 0.771261\n",
      "epoch : 86/500, train_loss = 0.746272\n",
      "epoch : 86/500, val_loss = 0.769662\n",
      "epoch : 87/500, train_loss = 0.744499\n",
      "epoch : 87/500, val_loss = 0.768179\n",
      "epoch : 88/500, train_loss = 0.742776\n",
      "epoch : 88/500, val_loss = 0.766710\n",
      "epoch : 89/500, train_loss = 0.741112\n",
      "epoch : 89/500, val_loss = 0.765129\n",
      "epoch : 90/500, train_loss = 0.739447\n",
      "epoch : 90/500, val_loss = 0.763979\n",
      "epoch : 91/500, train_loss = 0.737843\n",
      "epoch : 91/500, val_loss = 0.762607\n",
      "epoch : 92/500, train_loss = 0.736132\n",
      "epoch : 92/500, val_loss = 0.761148\n",
      "epoch : 93/500, train_loss = 0.734603\n",
      "epoch : 93/500, val_loss = 0.759797\n",
      "epoch : 94/500, train_loss = 0.733053\n",
      "epoch : 94/500, val_loss = 0.758399\n",
      "epoch : 95/500, train_loss = 0.731465\n",
      "epoch : 95/500, val_loss = 0.757074\n",
      "epoch : 96/500, train_loss = 0.729921\n",
      "epoch : 96/500, val_loss = 0.755753\n",
      "epoch : 97/500, train_loss = 0.728394\n",
      "epoch : 97/500, val_loss = 0.754340\n",
      "epoch : 98/500, train_loss = 0.727019\n",
      "epoch : 98/500, val_loss = 0.753161\n",
      "epoch : 99/500, train_loss = 0.725478\n",
      "epoch : 99/500, val_loss = 0.752171\n",
      "epoch : 100/500, train_loss = 0.724077\n",
      "epoch : 100/500, val_loss = 0.751092\n",
      "epoch : 101/500, train_loss = 0.723019\n",
      "epoch : 101/500, val_loss = 0.750103\n",
      "epoch : 102/500, train_loss = 0.721769\n",
      "epoch : 102/500, val_loss = 0.748823\n",
      "epoch : 103/500, train_loss = 0.720353\n",
      "epoch : 103/500, val_loss = 0.747311\n",
      "epoch : 104/500, train_loss = 0.718798\n",
      "epoch : 104/500, val_loss = 0.746215\n",
      "epoch : 105/500, train_loss = 0.717221\n",
      "epoch : 105/500, val_loss = 0.745105\n",
      "epoch : 106/500, train_loss = 0.715704\n",
      "epoch : 106/500, val_loss = 0.744166\n",
      "epoch : 107/500, train_loss = 0.714449\n",
      "epoch : 107/500, val_loss = 0.743060\n",
      "epoch : 108/500, train_loss = 0.713086\n",
      "epoch : 108/500, val_loss = 0.741885\n",
      "epoch : 109/500, train_loss = 0.711648\n",
      "epoch : 109/500, val_loss = 0.740709\n",
      "epoch : 110/500, train_loss = 0.710247\n",
      "epoch : 110/500, val_loss = 0.740266\n",
      "epoch : 111/500, train_loss = 0.708924\n",
      "epoch : 111/500, val_loss = 0.739036\n",
      "epoch : 112/500, train_loss = 0.708195\n",
      "epoch : 112/500, val_loss = 0.741429\n",
      "epoch : 113/500, train_loss = 0.707515\n",
      "epoch : 113/500, val_loss = 0.736930\n",
      "epoch : 114/500, train_loss = 0.706426\n",
      "epoch : 114/500, val_loss = 0.735877\n",
      "epoch : 115/500, train_loss = 0.705130\n",
      "epoch : 115/500, val_loss = 0.734975\n",
      "epoch : 116/500, train_loss = 0.703843\n",
      "epoch : 116/500, val_loss = 0.733888\n",
      "epoch : 117/500, train_loss = 0.702393\n",
      "epoch : 117/500, val_loss = 0.732834\n",
      "epoch : 118/500, train_loss = 0.701038\n",
      "epoch : 118/500, val_loss = 0.732258\n",
      "epoch : 119/500, train_loss = 0.699724\n",
      "epoch : 119/500, val_loss = 0.732050\n",
      "epoch : 120/500, train_loss = 0.698492\n",
      "epoch : 120/500, val_loss = 0.731695\n",
      "epoch : 121/500, train_loss = 0.697329\n",
      "epoch : 121/500, val_loss = 0.732649\n",
      "epoch : 122/500, train_loss = 0.696123\n",
      "epoch : 122/500, val_loss = 0.735898\n",
      "epoch : 123/500, train_loss = 0.694930\n",
      "epoch : 123/500, val_loss = 0.738974\n",
      "epoch : 124/500, train_loss = 0.693866\n",
      "epoch : 124/500, val_loss = 0.740035\n",
      "epoch : 125/500, train_loss = 0.692738\n",
      "epoch : 125/500, val_loss = 0.739348\n",
      "epoch : 126/500, train_loss = 0.691544\n",
      "epoch : 126/500, val_loss = 0.737222\n",
      "epoch : 127/500, train_loss = 0.690375\n",
      "epoch : 127/500, val_loss = 0.735952\n",
      "epoch : 128/500, train_loss = 0.689237\n",
      "epoch : 128/500, val_loss = 0.733171\n",
      "epoch : 129/500, train_loss = 0.688154\n",
      "epoch : 129/500, val_loss = 0.734164\n",
      "epoch : 130/500, train_loss = 0.687087\n",
      "epoch : 130/500, val_loss = 0.734206\n",
      "epoch : 131/500, train_loss = 0.685986\n",
      "epoch : 131/500, val_loss = 0.731515\n",
      "epoch : 132/500, train_loss = 0.684880\n",
      "epoch : 132/500, val_loss = 0.734370\n",
      "epoch : 133/500, train_loss = 0.683815\n",
      "epoch : 133/500, val_loss = 0.730265\n",
      "epoch : 134/500, train_loss = 0.682872\n",
      "epoch : 134/500, val_loss = 0.728606\n",
      "epoch : 135/500, train_loss = 0.681802\n",
      "epoch : 135/500, val_loss = 0.729474\n",
      "epoch : 136/500, train_loss = 0.680750\n",
      "epoch : 136/500, val_loss = 0.726571\n",
      "epoch : 137/500, train_loss = 0.679722\n",
      "epoch : 137/500, val_loss = 0.724167\n",
      "epoch : 138/500, train_loss = 0.678775\n",
      "epoch : 138/500, val_loss = 0.722809\n",
      "epoch : 139/500, train_loss = 0.677909\n",
      "epoch : 139/500, val_loss = 0.730522\n",
      "epoch : 140/500, train_loss = 0.676880\n",
      "epoch : 140/500, val_loss = 0.717467\n",
      "epoch : 141/500, train_loss = 0.675815\n",
      "epoch : 141/500, val_loss = 0.716948\n",
      "epoch : 142/500, train_loss = 0.675119\n",
      "epoch : 142/500, val_loss = 0.718906\n",
      "epoch : 143/500, train_loss = 0.674184\n",
      "epoch : 143/500, val_loss = 0.717122\n",
      "epoch : 144/500, train_loss = 0.673277\n",
      "epoch : 144/500, val_loss = 0.715349\n",
      "epoch : 145/500, train_loss = 0.672334\n",
      "epoch : 145/500, val_loss = 0.715520\n",
      "epoch : 146/500, train_loss = 0.671632\n",
      "epoch : 146/500, val_loss = 0.713697\n",
      "epoch : 147/500, train_loss = 0.670620\n",
      "epoch : 147/500, val_loss = 0.712387\n",
      "epoch : 148/500, train_loss = 0.669584\n",
      "epoch : 148/500, val_loss = 0.712123\n",
      "epoch : 149/500, train_loss = 0.668600\n",
      "epoch : 149/500, val_loss = 0.711386\n",
      "epoch : 150/500, train_loss = 0.667559\n",
      "epoch : 150/500, val_loss = 0.713403\n",
      "epoch : 151/500, train_loss = 0.666637\n",
      "epoch : 151/500, val_loss = 0.731541\n",
      "epoch : 152/500, train_loss = 0.665848\n",
      "epoch : 152/500, val_loss = 0.811709\n",
      "epoch : 153/500, train_loss = 0.664889\n",
      "epoch : 153/500, val_loss = 1.089977\n",
      "epoch : 154/500, train_loss = 0.663949\n",
      "epoch : 154/500, val_loss = 1.404237\n",
      "epoch : 155/500, train_loss = 0.662975\n",
      "epoch : 155/500, val_loss = 1.572554\n",
      "epoch : 156/500, train_loss = 0.662007\n",
      "epoch : 156/500, val_loss = 1.571108\n",
      "epoch : 157/500, train_loss = 0.661127\n",
      "epoch : 157/500, val_loss = 1.520749\n",
      "epoch : 158/500, train_loss = 0.660337\n",
      "epoch : 158/500, val_loss = 1.525994\n",
      "epoch : 159/500, train_loss = 0.659417\n",
      "epoch : 159/500, val_loss = 1.560447\n",
      "epoch : 160/500, train_loss = 0.658592\n",
      "epoch : 160/500, val_loss = 1.568055\n",
      "epoch : 161/500, train_loss = 0.657782\n",
      "epoch : 161/500, val_loss = 1.649359\n",
      "epoch : 162/500, train_loss = 0.656888\n",
      "epoch : 162/500, val_loss = 1.688819\n",
      "epoch : 163/500, train_loss = 0.656052\n",
      "epoch : 163/500, val_loss = 1.628772\n",
      "epoch : 164/500, train_loss = 0.655530\n",
      "epoch : 164/500, val_loss = 1.618870\n",
      "epoch : 165/500, train_loss = 0.654838\n",
      "epoch : 165/500, val_loss = 1.645820\n",
      "epoch : 166/500, train_loss = 0.653934\n",
      "epoch : 166/500, val_loss = 1.813110\n",
      "epoch : 167/500, train_loss = 0.653074\n",
      "epoch : 167/500, val_loss = 1.812168\n",
      "epoch : 168/500, train_loss = 0.652247\n",
      "epoch : 168/500, val_loss = 1.822762\n",
      "epoch : 169/500, train_loss = 0.651473\n",
      "epoch : 169/500, val_loss = 1.758119\n",
      "epoch : 170/500, train_loss = 0.650616\n",
      "epoch : 170/500, val_loss = 1.689538\n",
      "epoch : 171/500, train_loss = 0.649777\n",
      "epoch : 171/500, val_loss = 1.540537\n",
      "epoch : 172/500, train_loss = 0.649254\n",
      "epoch : 172/500, val_loss = 1.402286\n",
      "epoch : 173/500, train_loss = 0.648648\n",
      "epoch : 173/500, val_loss = 1.449949\n",
      "epoch : 174/500, train_loss = 0.648077\n",
      "epoch : 174/500, val_loss = 1.474413\n",
      "epoch : 175/500, train_loss = 0.647380\n",
      "epoch : 175/500, val_loss = 1.485821\n",
      "epoch : 176/500, train_loss = 0.646436\n",
      "epoch : 176/500, val_loss = 1.498700\n",
      "epoch : 177/500, train_loss = 0.645698\n",
      "epoch : 177/500, val_loss = 1.866597\n",
      "epoch : 178/500, train_loss = 0.645066\n",
      "epoch : 178/500, val_loss = 1.665031\n",
      "epoch : 179/500, train_loss = 0.644457\n",
      "epoch : 179/500, val_loss = 1.652607\n",
      "epoch : 180/500, train_loss = 0.643631\n",
      "epoch : 180/500, val_loss = 1.625120\n",
      "epoch : 181/500, train_loss = 0.642832\n",
      "epoch : 181/500, val_loss = 1.523716\n",
      "epoch : 182/500, train_loss = 0.642033\n",
      "epoch : 182/500, val_loss = 1.532859\n",
      "epoch : 183/500, train_loss = 0.641620\n",
      "epoch : 183/500, val_loss = 1.525701\n",
      "epoch : 184/500, train_loss = 0.641019\n",
      "epoch : 184/500, val_loss = 1.610380\n",
      "epoch : 185/500, train_loss = 0.640126\n",
      "epoch : 185/500, val_loss = 1.834042\n",
      "epoch : 186/500, train_loss = 0.639346\n",
      "epoch : 186/500, val_loss = 2.134767\n",
      "epoch : 187/500, train_loss = 0.638612\n",
      "epoch : 187/500, val_loss = 2.245647\n",
      "epoch : 188/500, train_loss = 0.637877\n",
      "epoch : 188/500, val_loss = 2.206088\n",
      "epoch : 189/500, train_loss = 0.637049\n",
      "epoch : 189/500, val_loss = 2.092637\n",
      "epoch : 190/500, train_loss = 0.636383\n",
      "epoch : 190/500, val_loss = 2.383715\n",
      "epoch : 191/500, train_loss = 0.636265\n",
      "epoch : 191/500, val_loss = 0.712902\n",
      "epoch : 192/500, train_loss = 0.635648\n",
      "epoch : 192/500, val_loss = 0.747230\n",
      "epoch : 193/500, train_loss = 0.634964\n",
      "epoch : 193/500, val_loss = 0.788798\n",
      "epoch : 194/500, train_loss = 0.634285\n",
      "epoch : 194/500, val_loss = 0.818253\n",
      "epoch : 195/500, train_loss = 0.633821\n",
      "epoch : 195/500, val_loss = 0.832522\n",
      "epoch : 196/500, train_loss = 0.633108\n",
      "epoch : 196/500, val_loss = 0.841578\n",
      "epoch : 197/500, train_loss = 0.632370\n",
      "epoch : 197/500, val_loss = 0.843416\n",
      "epoch : 198/500, train_loss = 0.631511\n",
      "epoch : 198/500, val_loss = 0.864231\n",
      "epoch : 199/500, train_loss = 0.630686\n",
      "epoch : 199/500, val_loss = 0.815754\n",
      "epoch : 200/500, train_loss = 0.629944\n",
      "epoch : 200/500, val_loss = 0.812462\n",
      "epoch : 201/500, train_loss = 0.629313\n",
      "epoch : 201/500, val_loss = 0.798154\n",
      "epoch : 202/500, train_loss = 0.628628\n",
      "epoch : 202/500, val_loss = 0.808194\n",
      "epoch : 203/500, train_loss = 0.627896\n",
      "epoch : 203/500, val_loss = 0.845630\n",
      "epoch : 204/500, train_loss = 0.627154\n",
      "epoch : 204/500, val_loss = 0.934126\n",
      "epoch : 205/500, train_loss = 0.626439\n",
      "epoch : 205/500, val_loss = 1.039793\n",
      "epoch : 206/500, train_loss = 0.625822\n",
      "epoch : 206/500, val_loss = 0.892591\n",
      "epoch : 207/500, train_loss = 0.625234\n",
      "epoch : 207/500, val_loss = 0.862179\n",
      "epoch : 208/500, train_loss = 0.624614\n",
      "epoch : 208/500, val_loss = 0.881336\n",
      "epoch : 209/500, train_loss = 0.623961\n",
      "epoch : 209/500, val_loss = 0.862079\n",
      "epoch : 210/500, train_loss = 0.623346\n",
      "epoch : 210/500, val_loss = 0.863522\n",
      "epoch : 211/500, train_loss = 0.622889\n",
      "epoch : 211/500, val_loss = 0.893414\n",
      "epoch : 212/500, train_loss = 0.622319\n",
      "epoch : 212/500, val_loss = 0.870213\n",
      "epoch : 213/500, train_loss = 0.621639\n",
      "epoch : 213/500, val_loss = 0.846264\n",
      "epoch : 214/500, train_loss = 0.620957\n",
      "epoch : 214/500, val_loss = 0.813935\n",
      "epoch : 215/500, train_loss = 0.620496\n",
      "epoch : 215/500, val_loss = 1.056437\n",
      "epoch : 216/500, train_loss = 0.619939\n",
      "epoch : 216/500, val_loss = 0.798193\n",
      "epoch : 217/500, train_loss = 0.619314\n",
      "epoch : 217/500, val_loss = 0.825314\n",
      "epoch : 218/500, train_loss = 0.618687\n",
      "epoch : 218/500, val_loss = 0.836798\n",
      "epoch : 219/500, train_loss = 0.618251\n",
      "epoch : 219/500, val_loss = 0.702822\n",
      "epoch : 220/500, train_loss = 0.617861\n",
      "epoch : 220/500, val_loss = 0.703878\n",
      "epoch : 221/500, train_loss = 0.617202\n",
      "epoch : 221/500, val_loss = 0.718513\n",
      "epoch : 222/500, train_loss = 0.616503\n",
      "epoch : 222/500, val_loss = 0.783735\n",
      "epoch : 223/500, train_loss = 0.615827\n",
      "epoch : 223/500, val_loss = 1.069647\n",
      "epoch : 224/500, train_loss = 0.615160\n",
      "epoch : 224/500, val_loss = 1.714871\n",
      "epoch : 225/500, train_loss = 0.614561\n",
      "epoch : 225/500, val_loss = 2.273897\n",
      "epoch : 226/500, train_loss = 0.614023\n",
      "epoch : 226/500, val_loss = 2.557281\n",
      "epoch : 227/500, train_loss = 0.613426\n",
      "epoch : 227/500, val_loss = 2.604247\n",
      "epoch : 228/500, train_loss = 0.612790\n",
      "epoch : 228/500, val_loss = 2.587907\n",
      "epoch : 229/500, train_loss = 0.612269\n",
      "epoch : 229/500, val_loss = 2.650773\n",
      "epoch : 230/500, train_loss = 0.611716\n",
      "epoch : 230/500, val_loss = 2.780082\n",
      "epoch : 231/500, train_loss = 0.611122\n",
      "epoch : 231/500, val_loss = 2.413984\n",
      "epoch : 232/500, train_loss = 0.610537\n",
      "epoch : 232/500, val_loss = 2.257324\n",
      "epoch : 233/500, train_loss = 0.609927\n",
      "epoch : 233/500, val_loss = 2.238040\n",
      "epoch : 234/500, train_loss = 0.609354\n",
      "epoch : 234/500, val_loss = 2.075237\n",
      "epoch : 235/500, train_loss = 0.608774\n",
      "epoch : 235/500, val_loss = 1.953888\n",
      "epoch : 236/500, train_loss = 0.608204\n",
      "epoch : 236/500, val_loss = 2.157122\n",
      "epoch : 237/500, train_loss = 0.608069\n",
      "epoch : 237/500, val_loss = 0.806247\n",
      "epoch : 238/500, train_loss = 0.607572\n",
      "epoch : 238/500, val_loss = 0.806436\n",
      "epoch : 239/500, train_loss = 0.607126\n",
      "epoch : 239/500, val_loss = 0.813995\n",
      "epoch : 240/500, train_loss = 0.606591\n",
      "epoch : 240/500, val_loss = 0.831637\n",
      "epoch : 241/500, train_loss = 0.606127\n",
      "epoch : 241/500, val_loss = 0.881749\n",
      "epoch : 242/500, train_loss = 0.605526\n",
      "epoch : 242/500, val_loss = 0.858144\n",
      "epoch : 243/500, train_loss = 0.604982\n",
      "epoch : 243/500, val_loss = 0.846877\n",
      "epoch : 244/500, train_loss = 0.604487\n",
      "epoch : 244/500, val_loss = 0.866292\n",
      "epoch : 245/500, train_loss = 0.603910\n",
      "epoch : 245/500, val_loss = 0.856920\n",
      "epoch : 246/500, train_loss = 0.603240\n",
      "epoch : 246/500, val_loss = 0.857773\n",
      "epoch : 247/500, train_loss = 0.602678\n",
      "epoch : 247/500, val_loss = 0.868694\n",
      "epoch : 248/500, train_loss = 0.602055\n",
      "epoch : 248/500, val_loss = 0.867509\n",
      "epoch : 249/500, train_loss = 0.601557\n",
      "epoch : 249/500, val_loss = 0.909935\n",
      "epoch : 250/500, train_loss = 0.601016\n",
      "epoch : 250/500, val_loss = 0.980928\n",
      "epoch : 251/500, train_loss = 0.600647\n",
      "epoch : 251/500, val_loss = 0.931419\n",
      "epoch : 252/500, train_loss = 0.600233\n",
      "epoch : 252/500, val_loss = 1.093816\n",
      "epoch : 253/500, train_loss = 0.599745\n",
      "epoch : 253/500, val_loss = 1.116570\n",
      "epoch : 254/500, train_loss = 0.599153\n",
      "epoch : 254/500, val_loss = 1.098113\n",
      "epoch : 255/500, train_loss = 0.598537\n",
      "epoch : 255/500, val_loss = 1.058737\n",
      "epoch : 256/500, train_loss = 0.598042\n",
      "epoch : 256/500, val_loss = 1.025063\n",
      "epoch : 257/500, train_loss = 0.597936\n",
      "epoch : 257/500, val_loss = 1.126663\n",
      "epoch : 258/500, train_loss = 0.597526\n",
      "epoch : 258/500, val_loss = 1.195601\n",
      "epoch : 259/500, train_loss = 0.597075\n",
      "epoch : 259/500, val_loss = 1.165989\n",
      "epoch : 260/500, train_loss = 0.596979\n",
      "epoch : 260/500, val_loss = 1.252817\n",
      "epoch : 261/500, train_loss = 0.596669\n",
      "epoch : 261/500, val_loss = 1.214928\n",
      "epoch : 262/500, train_loss = 0.596079\n",
      "epoch : 262/500, val_loss = 1.251902\n",
      "epoch : 263/500, train_loss = 0.595406\n",
      "epoch : 263/500, val_loss = 1.269455\n",
      "epoch : 264/500, train_loss = 0.594674\n",
      "epoch : 264/500, val_loss = 1.253806\n",
      "epoch : 265/500, train_loss = 0.594120\n",
      "epoch : 265/500, val_loss = 1.278360\n",
      "epoch : 266/500, train_loss = 0.593439\n",
      "epoch : 266/500, val_loss = 1.189754\n",
      "epoch : 267/500, train_loss = 0.592899\n",
      "epoch : 267/500, val_loss = 1.102978\n",
      "epoch : 268/500, train_loss = 0.592663\n",
      "epoch : 268/500, val_loss = 1.057774\n",
      "epoch : 269/500, train_loss = 0.592229\n",
      "epoch : 269/500, val_loss = 1.070511\n",
      "epoch : 270/500, train_loss = 0.591628\n",
      "epoch : 270/500, val_loss = 1.045488\n",
      "epoch : 271/500, train_loss = 0.591306\n",
      "epoch : 271/500, val_loss = 1.334116\n",
      "epoch : 272/500, train_loss = 0.591193\n",
      "epoch : 272/500, val_loss = 1.050714\n",
      "epoch : 273/500, train_loss = 0.590673\n",
      "epoch : 273/500, val_loss = 0.823952\n",
      "epoch : 274/500, train_loss = 0.590026\n",
      "epoch : 274/500, val_loss = 0.846352\n",
      "epoch : 275/500, train_loss = 0.589513\n",
      "epoch : 275/500, val_loss = 0.825692\n",
      "epoch : 276/500, train_loss = 0.588975\n",
      "epoch : 276/500, val_loss = 0.805579\n",
      "epoch : 277/500, train_loss = 0.588447\n",
      "epoch : 277/500, val_loss = 0.838825\n",
      "epoch : 278/500, train_loss = 0.588164\n",
      "epoch : 278/500, val_loss = 1.061141\n",
      "epoch : 279/500, train_loss = 0.587710\n",
      "epoch : 279/500, val_loss = 1.161298\n",
      "epoch : 280/500, train_loss = 0.587325\n",
      "epoch : 280/500, val_loss = 1.170777\n",
      "epoch : 281/500, train_loss = 0.586838\n",
      "epoch : 281/500, val_loss = 1.210188\n",
      "epoch : 282/500, train_loss = 0.586267\n",
      "epoch : 282/500, val_loss = 1.124780\n",
      "epoch : 283/500, train_loss = 0.585811\n",
      "epoch : 283/500, val_loss = 1.057071\n",
      "epoch : 284/500, train_loss = 0.585323\n",
      "epoch : 284/500, val_loss = 0.987534\n",
      "epoch : 285/500, train_loss = 0.584799\n",
      "epoch : 285/500, val_loss = 1.077122\n",
      "epoch : 286/500, train_loss = 0.584293\n",
      "epoch : 286/500, val_loss = 1.257133\n",
      "epoch : 287/500, train_loss = 0.583771\n",
      "epoch : 287/500, val_loss = 1.300800\n",
      "epoch : 288/500, train_loss = 0.583326\n",
      "epoch : 288/500, val_loss = 1.140213\n",
      "epoch : 289/500, train_loss = 0.582964\n",
      "epoch : 289/500, val_loss = 1.101609\n",
      "epoch : 290/500, train_loss = 0.582492\n",
      "epoch : 290/500, val_loss = 1.110185\n",
      "epoch : 291/500, train_loss = 0.582097\n",
      "epoch : 291/500, val_loss = 1.088802\n",
      "epoch : 292/500, train_loss = 0.581699\n",
      "epoch : 292/500, val_loss = 1.092137\n",
      "epoch : 293/500, train_loss = 0.581243\n",
      "epoch : 293/500, val_loss = 1.205892\n",
      "epoch : 294/500, train_loss = 0.581027\n",
      "epoch : 294/500, val_loss = 1.272354\n",
      "epoch : 295/500, train_loss = 0.580782\n",
      "epoch : 295/500, val_loss = 1.328020\n",
      "epoch : 296/500, train_loss = 0.580412\n",
      "epoch : 296/500, val_loss = 1.302689\n",
      "epoch : 297/500, train_loss = 0.580238\n",
      "epoch : 297/500, val_loss = 1.426247\n",
      "epoch : 298/500, train_loss = 0.580585\n",
      "epoch : 298/500, val_loss = 1.837287\n",
      "epoch : 299/500, train_loss = 0.580392\n",
      "epoch : 299/500, val_loss = 1.979400\n",
      "epoch : 300/500, train_loss = 0.579638\n",
      "epoch : 300/500, val_loss = 2.026073\n",
      "epoch : 301/500, train_loss = 0.578844\n",
      "epoch : 301/500, val_loss = 2.320156\n",
      "epoch : 302/500, train_loss = 0.578216\n",
      "epoch : 302/500, val_loss = 3.108631\n",
      "epoch : 303/500, train_loss = 0.577651\n",
      "epoch : 303/500, val_loss = 3.777804\n",
      "epoch : 304/500, train_loss = 0.577108\n",
      "epoch : 304/500, val_loss = 3.681305\n",
      "epoch : 305/500, train_loss = 0.576685\n",
      "epoch : 305/500, val_loss = 3.411672\n",
      "epoch : 306/500, train_loss = 0.576166\n",
      "epoch : 306/500, val_loss = 3.528549\n",
      "epoch : 307/500, train_loss = 0.575718\n",
      "epoch : 307/500, val_loss = 3.626922\n",
      "epoch : 308/500, train_loss = 0.575440\n",
      "epoch : 308/500, val_loss = 3.986396\n",
      "epoch : 309/500, train_loss = 0.575031\n",
      "epoch : 309/500, val_loss = 3.844597\n",
      "epoch : 310/500, train_loss = 0.574597\n",
      "epoch : 310/500, val_loss = 3.810729\n",
      "epoch : 311/500, train_loss = 0.574102\n",
      "epoch : 311/500, val_loss = 3.679088\n",
      "epoch : 312/500, train_loss = 0.573610\n",
      "epoch : 312/500, val_loss = 3.631877\n",
      "epoch : 313/500, train_loss = 0.573140\n",
      "epoch : 313/500, val_loss = 3.654006\n",
      "epoch : 314/500, train_loss = 0.572758\n",
      "epoch : 314/500, val_loss = 3.664625\n",
      "epoch : 315/500, train_loss = 0.572306\n",
      "epoch : 315/500, val_loss = 3.582544\n",
      "epoch : 316/500, train_loss = 0.571873\n",
      "epoch : 316/500, val_loss = 2.966671\n",
      "epoch : 317/500, train_loss = 0.571425\n",
      "epoch : 317/500, val_loss = 2.800394\n",
      "epoch : 318/500, train_loss = 0.571044\n",
      "epoch : 318/500, val_loss = 2.904255\n",
      "epoch : 319/500, train_loss = 0.570668\n",
      "epoch : 319/500, val_loss = 2.954059\n",
      "epoch : 320/500, train_loss = 0.570228\n",
      "epoch : 320/500, val_loss = 2.851709\n",
      "epoch : 321/500, train_loss = 0.570021\n",
      "epoch : 321/500, val_loss = 2.602360\n",
      "epoch : 322/500, train_loss = 0.569686\n",
      "epoch : 322/500, val_loss = 2.644536\n",
      "epoch : 323/500, train_loss = 0.569254\n",
      "epoch : 323/500, val_loss = 2.218618\n",
      "epoch : 324/500, train_loss = 0.568865\n",
      "epoch : 324/500, val_loss = 2.145301\n",
      "epoch : 325/500, train_loss = 0.568559\n",
      "epoch : 325/500, val_loss = 2.156457\n",
      "epoch : 326/500, train_loss = 0.568411\n",
      "epoch : 326/500, val_loss = 2.009584\n",
      "epoch : 327/500, train_loss = 0.568303\n",
      "epoch : 327/500, val_loss = 2.494089\n",
      "epoch : 328/500, train_loss = 0.568494\n",
      "epoch : 328/500, val_loss = 2.630161\n",
      "epoch : 329/500, train_loss = 0.568397\n",
      "epoch : 329/500, val_loss = 2.841780\n",
      "epoch : 330/500, train_loss = 0.567835\n",
      "epoch : 330/500, val_loss = 1.445297\n",
      "epoch : 331/500, train_loss = 0.567338\n",
      "epoch : 331/500, val_loss = 2.019132\n",
      "epoch : 332/500, train_loss = 0.566759\n",
      "epoch : 332/500, val_loss = 2.601007\n",
      "epoch : 333/500, train_loss = 0.566283\n",
      "epoch : 333/500, val_loss = 2.832594\n",
      "epoch : 334/500, train_loss = 0.565852\n",
      "epoch : 334/500, val_loss = 2.624874\n",
      "epoch : 335/500, train_loss = 0.565388\n",
      "epoch : 335/500, val_loss = 2.592968\n",
      "epoch : 336/500, train_loss = 0.564964\n",
      "epoch : 336/500, val_loss = 2.593721\n",
      "epoch : 337/500, train_loss = 0.564525\n",
      "epoch : 337/500, val_loss = 2.424615\n",
      "epoch : 338/500, train_loss = 0.564281\n",
      "epoch : 338/500, val_loss = 2.654769\n",
      "epoch : 339/500, train_loss = 0.563778\n",
      "epoch : 339/500, val_loss = 2.681019\n",
      "epoch : 340/500, train_loss = 0.563325\n",
      "epoch : 340/500, val_loss = 2.818912\n",
      "epoch : 341/500, train_loss = 0.562860\n",
      "epoch : 341/500, val_loss = 2.849902\n",
      "epoch : 342/500, train_loss = 0.562415\n",
      "epoch : 342/500, val_loss = 2.792641\n",
      "epoch : 343/500, train_loss = 0.561986\n",
      "epoch : 343/500, val_loss = 2.669099\n",
      "epoch : 344/500, train_loss = 0.561674\n",
      "epoch : 344/500, val_loss = 2.740338\n",
      "epoch : 345/500, train_loss = 0.561637\n",
      "epoch : 345/500, val_loss = 2.758476\n",
      "epoch : 346/500, train_loss = 0.561359\n",
      "epoch : 346/500, val_loss = 2.505235\n",
      "epoch : 347/500, train_loss = 0.560818\n",
      "epoch : 347/500, val_loss = 2.383163\n",
      "epoch : 348/500, train_loss = 0.560446\n",
      "epoch : 348/500, val_loss = 2.313482\n",
      "epoch : 349/500, train_loss = 0.559987\n",
      "epoch : 349/500, val_loss = 2.351443\n",
      "epoch : 350/500, train_loss = 0.559558\n",
      "epoch : 350/500, val_loss = 2.387055\n",
      "epoch : 351/500, train_loss = 0.559231\n",
      "epoch : 351/500, val_loss = 2.372968\n",
      "epoch : 352/500, train_loss = 0.558881\n",
      "epoch : 352/500, val_loss = 2.040697\n",
      "epoch : 353/500, train_loss = 0.558629\n",
      "epoch : 353/500, val_loss = 2.005178\n",
      "epoch : 354/500, train_loss = 0.558216\n",
      "epoch : 354/500, val_loss = 2.007464\n",
      "epoch : 355/500, train_loss = 0.557950\n",
      "epoch : 355/500, val_loss = 1.874366\n",
      "epoch : 356/500, train_loss = 0.557558\n",
      "epoch : 356/500, val_loss = 1.953838\n",
      "epoch : 357/500, train_loss = 0.557185\n",
      "epoch : 357/500, val_loss = 2.366933\n",
      "epoch : 358/500, train_loss = 0.557016\n",
      "epoch : 358/500, val_loss = 2.886756\n",
      "epoch : 359/500, train_loss = 0.556660\n",
      "epoch : 359/500, val_loss = 2.833131\n",
      "epoch : 360/500, train_loss = 0.556161\n",
      "epoch : 360/500, val_loss = 2.773249\n",
      "epoch : 361/500, train_loss = 0.555763\n",
      "epoch : 361/500, val_loss = 2.407980\n",
      "epoch : 362/500, train_loss = 0.555525\n",
      "epoch : 362/500, val_loss = 2.355365\n",
      "epoch : 363/500, train_loss = 0.555410\n",
      "epoch : 363/500, val_loss = 2.576890\n",
      "epoch : 364/500, train_loss = 0.555292\n",
      "epoch : 364/500, val_loss = 1.475379\n",
      "epoch : 365/500, train_loss = 0.554993\n",
      "epoch : 365/500, val_loss = 1.746946\n",
      "epoch : 366/500, train_loss = 0.554670\n",
      "epoch : 366/500, val_loss = 1.996653\n",
      "epoch : 367/500, train_loss = 0.554357\n",
      "epoch : 367/500, val_loss = 2.125994\n",
      "epoch : 368/500, train_loss = 0.554335\n",
      "epoch : 368/500, val_loss = 2.886856\n",
      "epoch : 369/500, train_loss = 0.553979\n",
      "epoch : 369/500, val_loss = 1.860609\n",
      "epoch : 370/500, train_loss = 0.553484\n",
      "epoch : 370/500, val_loss = 1.239955\n",
      "epoch : 371/500, train_loss = 0.553014\n",
      "epoch : 371/500, val_loss = 1.259469\n",
      "epoch : 372/500, train_loss = 0.552542\n",
      "epoch : 372/500, val_loss = 1.259184\n",
      "epoch : 373/500, train_loss = 0.552152\n",
      "epoch : 373/500, val_loss = 1.289983\n",
      "epoch : 374/500, train_loss = 0.551864\n",
      "epoch : 374/500, val_loss = 1.268283\n",
      "epoch : 375/500, train_loss = 0.551516\n",
      "epoch : 375/500, val_loss = 1.215551\n",
      "epoch : 376/500, train_loss = 0.551162\n",
      "epoch : 376/500, val_loss = 1.215534\n",
      "epoch : 377/500, train_loss = 0.550757\n",
      "epoch : 377/500, val_loss = 1.204052\n",
      "epoch : 378/500, train_loss = 0.550282\n",
      "epoch : 378/500, val_loss = 1.184557\n",
      "epoch : 379/500, train_loss = 0.549872\n",
      "epoch : 379/500, val_loss = 1.172604\n",
      "epoch : 380/500, train_loss = 0.549467\n",
      "epoch : 380/500, val_loss = 1.193089\n",
      "epoch : 381/500, train_loss = 0.549085\n",
      "epoch : 381/500, val_loss = 1.199272\n",
      "epoch : 382/500, train_loss = 0.548866\n",
      "epoch : 382/500, val_loss = 1.184661\n",
      "epoch : 383/500, train_loss = 0.548639\n",
      "epoch : 383/500, val_loss = 1.129277\n",
      "epoch : 384/500, train_loss = 0.548291\n",
      "epoch : 384/500, val_loss = 1.123902\n",
      "epoch : 385/500, train_loss = 0.547977\n",
      "epoch : 385/500, val_loss = 1.116175\n",
      "epoch : 386/500, train_loss = 0.547716\n",
      "epoch : 386/500, val_loss = 1.169214\n",
      "epoch : 387/500, train_loss = 0.547547\n",
      "epoch : 387/500, val_loss = 1.185886\n",
      "epoch : 388/500, train_loss = 0.547195\n",
      "epoch : 388/500, val_loss = 1.184463\n",
      "epoch : 389/500, train_loss = 0.546856\n",
      "epoch : 389/500, val_loss = 1.113751\n",
      "epoch : 390/500, train_loss = 0.546772\n",
      "epoch : 390/500, val_loss = 1.185557\n",
      "epoch : 391/500, train_loss = 0.547058\n",
      "epoch : 391/500, val_loss = 1.207076\n",
      "epoch : 392/500, train_loss = 0.547066\n",
      "epoch : 392/500, val_loss = 1.154906\n",
      "epoch : 393/500, train_loss = 0.546543\n",
      "epoch : 393/500, val_loss = 1.097739\n",
      "epoch : 394/500, train_loss = 0.546406\n",
      "epoch : 394/500, val_loss = 1.124916\n",
      "epoch : 395/500, train_loss = 0.546156\n",
      "epoch : 395/500, val_loss = 1.319203\n",
      "epoch : 396/500, train_loss = 0.546283\n",
      "epoch : 396/500, val_loss = 1.161824\n",
      "epoch : 397/500, train_loss = 0.546140\n",
      "epoch : 397/500, val_loss = 1.181220\n",
      "epoch : 398/500, train_loss = 0.545916\n",
      "epoch : 398/500, val_loss = 1.113258\n",
      "epoch : 399/500, train_loss = 0.545739\n",
      "epoch : 399/500, val_loss = 1.095082\n",
      "epoch : 400/500, train_loss = 0.545195\n",
      "epoch : 400/500, val_loss = 1.164112\n",
      "epoch : 401/500, train_loss = 0.544773\n",
      "epoch : 401/500, val_loss = 1.233743\n",
      "epoch : 402/500, train_loss = 0.544376\n",
      "epoch : 402/500, val_loss = 1.233629\n",
      "epoch : 403/500, train_loss = 0.544064\n",
      "epoch : 403/500, val_loss = 1.250311\n",
      "epoch : 404/500, train_loss = 0.543755\n",
      "epoch : 404/500, val_loss = 1.219947\n",
      "epoch : 405/500, train_loss = 0.543202\n",
      "epoch : 405/500, val_loss = 1.205095\n",
      "epoch : 406/500, train_loss = 0.542738\n",
      "epoch : 406/500, val_loss = 1.206538\n",
      "epoch : 407/500, train_loss = 0.542307\n",
      "epoch : 407/500, val_loss = 1.215929\n",
      "epoch : 408/500, train_loss = 0.541875\n",
      "epoch : 408/500, val_loss = 1.193017\n",
      "epoch : 409/500, train_loss = 0.541506\n",
      "epoch : 409/500, val_loss = 1.173553\n",
      "epoch : 410/500, train_loss = 0.541243\n",
      "epoch : 410/500, val_loss = 1.296444\n",
      "epoch : 411/500, train_loss = 0.541184\n",
      "epoch : 411/500, val_loss = 1.202567\n",
      "epoch : 412/500, train_loss = 0.541301\n",
      "epoch : 412/500, val_loss = 1.186278\n",
      "epoch : 413/500, train_loss = 0.541018\n",
      "epoch : 413/500, val_loss = 1.212635\n",
      "epoch : 414/500, train_loss = 0.540707\n",
      "epoch : 414/500, val_loss = 1.183734\n",
      "epoch : 415/500, train_loss = 0.540228\n",
      "epoch : 415/500, val_loss = 1.248923\n",
      "epoch : 416/500, train_loss = 0.539785\n",
      "epoch : 416/500, val_loss = 1.304035\n",
      "epoch : 417/500, train_loss = 0.539374\n",
      "epoch : 417/500, val_loss = 1.304602\n",
      "epoch : 418/500, train_loss = 0.538974\n",
      "epoch : 418/500, val_loss = 1.283351\n",
      "epoch : 419/500, train_loss = 0.538611\n",
      "epoch : 419/500, val_loss = 1.269003\n",
      "epoch : 420/500, train_loss = 0.538216\n",
      "epoch : 420/500, val_loss = 1.266725\n",
      "epoch : 421/500, train_loss = 0.537827\n",
      "epoch : 421/500, val_loss = 1.260065\n",
      "epoch : 422/500, train_loss = 0.537479\n",
      "epoch : 422/500, val_loss = 1.251976\n",
      "epoch : 423/500, train_loss = 0.537267\n",
      "epoch : 423/500, val_loss = 1.226230\n",
      "epoch : 424/500, train_loss = 0.536942\n",
      "epoch : 424/500, val_loss = 1.203585\n",
      "epoch : 425/500, train_loss = 0.536576\n",
      "epoch : 425/500, val_loss = 1.208449\n",
      "epoch : 426/500, train_loss = 0.536206\n",
      "epoch : 426/500, val_loss = 1.206013\n",
      "epoch : 427/500, train_loss = 0.535898\n",
      "epoch : 427/500, val_loss = 1.184254\n",
      "epoch : 428/500, train_loss = 0.535691\n",
      "epoch : 428/500, val_loss = 1.183484\n",
      "epoch : 429/500, train_loss = 0.535426\n",
      "epoch : 429/500, val_loss = 1.209709\n",
      "epoch : 430/500, train_loss = 0.535110\n",
      "epoch : 430/500, val_loss = 1.193806\n",
      "epoch : 431/500, train_loss = 0.534817\n",
      "epoch : 431/500, val_loss = 1.170899\n",
      "epoch : 432/500, train_loss = 0.534507\n",
      "epoch : 432/500, val_loss = 1.206871\n",
      "epoch : 433/500, train_loss = 0.534254\n",
      "epoch : 433/500, val_loss = 1.249682\n",
      "epoch : 434/500, train_loss = 0.534300\n",
      "epoch : 434/500, val_loss = 1.421873\n",
      "epoch : 435/500, train_loss = 0.535799\n",
      "epoch : 435/500, val_loss = 1.852481\n",
      "epoch : 436/500, train_loss = 0.535550\n",
      "epoch : 436/500, val_loss = 1.567242\n",
      "epoch : 437/500, train_loss = 0.534972\n",
      "epoch : 437/500, val_loss = 1.381582\n",
      "epoch : 438/500, train_loss = 0.534322\n",
      "epoch : 438/500, val_loss = 1.359130\n",
      "epoch : 439/500, train_loss = 0.533713\n",
      "epoch : 439/500, val_loss = 1.342529\n",
      "epoch : 440/500, train_loss = 0.533183\n",
      "epoch : 440/500, val_loss = 1.329073\n",
      "epoch : 441/500, train_loss = 0.532673\n",
      "epoch : 441/500, val_loss = 1.386620\n",
      "epoch : 442/500, train_loss = 0.532513\n",
      "epoch : 442/500, val_loss = 1.489159\n",
      "epoch : 443/500, train_loss = 0.532285\n",
      "epoch : 443/500, val_loss = 1.226228\n",
      "epoch : 444/500, train_loss = 0.531790\n",
      "epoch : 444/500, val_loss = 1.253999\n",
      "epoch : 445/500, train_loss = 0.531403\n",
      "epoch : 445/500, val_loss = 1.298526\n",
      "epoch : 446/500, train_loss = 0.531062\n",
      "epoch : 446/500, val_loss = 1.130956\n",
      "epoch : 447/500, train_loss = 0.530923\n",
      "epoch : 447/500, val_loss = 1.133829\n",
      "epoch : 448/500, train_loss = 0.530779\n",
      "epoch : 448/500, val_loss = 1.149694\n",
      "epoch : 449/500, train_loss = 0.530545\n",
      "epoch : 449/500, val_loss = 1.238331\n",
      "epoch : 450/500, train_loss = 0.530202\n",
      "epoch : 450/500, val_loss = 1.211433\n",
      "epoch : 451/500, train_loss = 0.529973\n",
      "epoch : 451/500, val_loss = 1.092051\n",
      "epoch : 452/500, train_loss = 0.530029\n",
      "epoch : 452/500, val_loss = 1.073251\n",
      "epoch : 453/500, train_loss = 0.530402\n",
      "epoch : 453/500, val_loss = 1.056581\n",
      "epoch : 454/500, train_loss = 0.530045\n",
      "epoch : 454/500, val_loss = 1.037540\n",
      "epoch : 455/500, train_loss = 0.529749\n",
      "epoch : 455/500, val_loss = 1.012450\n",
      "epoch : 456/500, train_loss = 0.529371\n",
      "epoch : 456/500, val_loss = 1.015515\n",
      "epoch : 457/500, train_loss = 0.529368\n",
      "epoch : 457/500, val_loss = 1.018924\n",
      "epoch : 458/500, train_loss = 0.528793\n",
      "epoch : 458/500, val_loss = 1.022968\n",
      "epoch : 459/500, train_loss = 0.528571\n",
      "epoch : 459/500, val_loss = 1.074508\n",
      "epoch : 460/500, train_loss = 0.528160\n",
      "epoch : 460/500, val_loss = 1.081394\n",
      "epoch : 461/500, train_loss = 0.527691\n",
      "epoch : 461/500, val_loss = 1.077449\n",
      "epoch : 462/500, train_loss = 0.527454\n",
      "epoch : 462/500, val_loss = 1.077520\n",
      "epoch : 463/500, train_loss = 0.527089\n",
      "epoch : 463/500, val_loss = 1.091587\n",
      "epoch : 464/500, train_loss = 0.526758\n",
      "epoch : 464/500, val_loss = 1.075954\n",
      "epoch : 465/500, train_loss = 0.526430\n",
      "epoch : 465/500, val_loss = 1.116773\n",
      "epoch : 466/500, train_loss = 0.526118\n",
      "epoch : 466/500, val_loss = 0.998169\n",
      "epoch : 467/500, train_loss = 0.525752\n",
      "epoch : 467/500, val_loss = 1.112549\n",
      "epoch : 468/500, train_loss = 0.525649\n",
      "epoch : 468/500, val_loss = 1.180633\n",
      "epoch : 469/500, train_loss = 0.525367\n",
      "epoch : 469/500, val_loss = 1.141081\n",
      "epoch : 470/500, train_loss = 0.524951\n",
      "epoch : 470/500, val_loss = 1.127925\n",
      "epoch : 471/500, train_loss = 0.524576\n",
      "epoch : 471/500, val_loss = 1.105097\n",
      "epoch : 472/500, train_loss = 0.524320\n",
      "epoch : 472/500, val_loss = 1.335687\n",
      "epoch : 473/500, train_loss = 0.524354\n",
      "epoch : 473/500, val_loss = 1.364583\n",
      "epoch : 474/500, train_loss = 0.524194\n",
      "epoch : 474/500, val_loss = 1.268957\n",
      "epoch : 475/500, train_loss = 0.523929\n",
      "epoch : 475/500, val_loss = 1.146965\n",
      "epoch : 476/500, train_loss = 0.523641\n",
      "epoch : 476/500, val_loss = 1.163376\n",
      "epoch : 477/500, train_loss = 0.523474\n",
      "epoch : 477/500, val_loss = 1.313200\n",
      "epoch : 478/500, train_loss = 0.523210\n",
      "epoch : 478/500, val_loss = 1.128294\n",
      "epoch : 479/500, train_loss = 0.523129\n",
      "epoch : 479/500, val_loss = 1.132650\n",
      "epoch : 480/500, train_loss = 0.522869\n",
      "epoch : 480/500, val_loss = 1.137066\n",
      "epoch : 481/500, train_loss = 0.522468\n",
      "epoch : 481/500, val_loss = 1.116969\n",
      "epoch : 482/500, train_loss = 0.522049\n",
      "epoch : 482/500, val_loss = 1.096396\n",
      "epoch : 483/500, train_loss = 0.521692\n",
      "epoch : 483/500, val_loss = 1.075125\n",
      "epoch : 484/500, train_loss = 0.521376\n",
      "epoch : 484/500, val_loss = 1.071873\n",
      "epoch : 485/500, train_loss = 0.521085\n",
      "epoch : 485/500, val_loss = 1.048120\n",
      "epoch : 486/500, train_loss = 0.520776\n",
      "epoch : 486/500, val_loss = 1.034227\n",
      "epoch : 487/500, train_loss = 0.520490\n",
      "epoch : 487/500, val_loss = 1.020908\n",
      "epoch : 488/500, train_loss = 0.520244\n",
      "epoch : 488/500, val_loss = 1.014962\n",
      "epoch : 489/500, train_loss = 0.519932\n",
      "epoch : 489/500, val_loss = 1.003943\n",
      "epoch : 490/500, train_loss = 0.519589\n",
      "epoch : 490/500, val_loss = 1.014137\n",
      "epoch : 491/500, train_loss = 0.519380\n",
      "epoch : 491/500, val_loss = 1.949125\n",
      "epoch : 492/500, train_loss = 0.519277\n",
      "epoch : 492/500, val_loss = 1.054822\n",
      "epoch : 493/500, train_loss = 0.519394\n",
      "epoch : 493/500, val_loss = 1.071681\n",
      "epoch : 494/500, train_loss = 0.519127\n",
      "epoch : 494/500, val_loss = 1.035240\n",
      "epoch : 495/500, train_loss = 0.518858\n",
      "epoch : 495/500, val_loss = 0.984775\n",
      "epoch : 496/500, train_loss = 0.518635\n",
      "epoch : 496/500, val_loss = 0.954062\n",
      "epoch : 497/500, train_loss = 0.518772\n",
      "epoch : 497/500, val_loss = 0.948019\n",
      "epoch : 498/500, train_loss = 0.518487\n",
      "epoch : 498/500, val_loss = 0.893301\n",
      "epoch : 499/500, train_loss = 0.518155\n",
      "epoch : 499/500, val_loss = 0.904618\n",
      "epoch : 500/500, train_loss = 0.517935\n",
      "epoch : 500/500, val_loss = 0.924024\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "epochs=500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        lol, outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train.append(loss)\n",
    "\n",
    "\n",
    "    #For Valid Loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            code, outputs = model(batch)\n",
    "            loss_val =criterion(outputs, batch)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    losses_val.append(val_loss)\n",
    "\n",
    "\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    print(\"epoch : {}/{}, val_loss = {:.6f}\".format(epoch + 1, epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29601abb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAApf0lEQVR4nO3deXhU9b3H8fd3lmwEAoRVFllkUUFBUbBorXq1iFSsotiqFbW1Wq1Lba/o7aaPfZ5u17beIta61q0qbrjVutalFQ0IArKKLAGVELZsk9l+949zApOQkAATJhM+r+eZZ86cc+bMNwfyOb/8zm/OMeccIiKS/QKZLkBERNJDgS4i0k4o0EVE2gkFuohIO6FAFxFpJ0KZ+uBu3bq5AQMGZOrjRUSy0ty5czc557o3tixjgT5gwABKSkoy9fEiIlnJzNY0tUxdLiIi7YQCXUSknVCgi4i0ExnrQxeR9icWi1FaWkokEsl0KVkvLy+Pvn37Eg6HW/yeFge6mQWBEmC9c25Sg2W5wN+Ao4FyYKpzbnWLqxCRdqG0tJSOHTsyYMAAzCzT5WQt5xzl5eWUlpYycODAFr9vT7pcrgWWNLHsMmCLc+4Q4A/Ab/ZguyLSTkQiEYqLixXm+8jMKC4u3uO/dFoU6GbWFzgDuKeJVSYDD/rTs4BTTP+iIgck/eqnx97sx5a20P8I/DeQbGJ5H2AdgHMuDmwDihsp8HIzKzGzkrKysj0uFoAvP4E3boOqTXv3fhGRdqrZQDezScBG59zcff0w59zdzrkxzrkx3bs3+kWn5m1aDm//Dio37ms5IiLtSkta6OOBM81sNfB34GQze7jBOuuBfgBmFgKK8E6Opl/QP+ObjLfK5kUke23dupU777xzj983ceJEtm7dusfvmzZtGrNmzdrj97WWZgPdOXeTc66vc24AcD7whnPuwgarzQYu9qen+Ou0zq2QAv7AnGSsVTYvItmrqUCPx3ffAHzppZfo3LlzK1W1/+z1OHQzuxUocc7NBu4FHjKzlcBmvOBvHXWBnlALXaQtu+X5xXyyYXtat3nYQZ34xTcOb3L59OnT+fTTTxk1ahThcJi8vDy6dOnC0qVLWb58OWeddRbr1q0jEolw7bXXcvnllwM7ry1VWVnJ6aefzvHHH8+///1v+vTpw3PPPUd+fn6ztb3++uv8+Mc/Jh6Pc8wxxzBz5kxyc3OZPn06s2fPJhQKcdppp/H73/+eJ598kltuuYVgMEhRURFvv/12WvbPHgW6c+4t4C1/+ucp8yPAuWmpqDk7ulzUQheR+n7961+zaNEi5s+fz1tvvcUZZ5zBokWLdozlvu++++jatSs1NTUcc8wxnHPOORQX1x+/sWLFCh577DH++te/ct555/HUU09x4YUNOyXqi0QiTJs2jddff52hQ4fyne98h5kzZ3LRRRfxzDPPsHTpUsxsR7fOrbfeyiuvvEKfPn32qqunKdn3TdGA+tBFssHuWtL7y7HHHlvvizl33HEHzzzzDADr1q1jxYoVuwT6wIEDGTVqFABHH300q1evbvZzli1bxsCBAxk6dCgAF198MTNmzODqq68mLy+Pyy67jEmTJjFpkvedzPHjxzNt2jTOO+88zj777DT8pJ7su5aLulxEpIU6dOiwY/qtt97itdde4z//+Q8LFixg9OjRjX5xJzc3d8d0MBhstv99d0KhEB988AFTpkzhhRdeYMKECQDcdddd3Hbbbaxbt46jjz6a8vL0jCHJvhZ6UCdFRaRxHTt2pKKiotFl27Zto0uXLhQUFLB06VLef//9tH3usGHDWL16NStXruSQQw7hoYce4sQTT6SyspLq6momTpzI+PHjGTRoEACffvopY8eOZezYsbz88susW7dul78U9kb2BXpdl0tCgS4i9RUXFzN+/HhGjBhBfn4+PXv23LFswoQJ3HXXXRx66KEMGzaMcePGpe1z8/LyuP/++zn33HN3nBS94oor2Lx5M5MnTyYSieCc4/bbbwfgJz/5CStWrMA5xymnnMKRRx6ZljqstUYXNmfMmDFur+5YVLYMZhwLU+6HEenrexKRfbdkyRIOPfTQTJfRbjS2P81srnNuTGPrZ28fuk6KiojUk4VdLnUnRdXlIiL7x1VXXcV7771Xb961117LJZdckqGKGpd9ga5x6CKyn82YMSPTJbRI9na5qIUuIlJP9gZ6MpHZOkRE2pjsC3R1uYiINCr7Al3j0EVEGpWFga5hiyKSPoWFhU0uW716NSNGjNiP1eybLAz0oPesQBcRqSf7hi2aed0u6nIRadteng5fLEzvNnuNhNN/vdtVpk+fTr9+/bjqqqsA+OUvf0koFOLNN99ky5YtxGIxbrvtNiZPnrxHHx2JRLjyyispKSkhFApx++23c9JJJ7F48WIuueQSotEoyWSSp556ioMOOojzzjuP0tJSEokEP/vZz5g6depe/9gtlX2BDt6JUZ0UFZFGTJ06leuuu25HoD/xxBO88sorXHPNNXTq1IlNmzYxbtw4zjzzTMysxdudMWMGZsbChQtZunQpp512GsuXL+euu+7i2muv5YILLiAajZJIJHjppZc46KCDePHFFwHvwmD7Q3YGeiCky+eKtHXNtKRby+jRo9m4cSMbNmygrKyMLl260KtXL66//nrefvttAoEA69ev58svv6RXr14t3u67777LD3/4QwCGDx/OwQcfzPLlyznuuOP41a9+RWlpKWeffTZDhgxh5MiR3HDDDdx4441MmjSJE044obV+3Hqyrw8dvEBXH7qINOHcc89l1qxZPP7440ydOpVHHnmEsrIy5s6dy/z58+nZs2ej10LfG9/+9reZPXs2+fn5TJw4kTfeeIOhQ4cyb948Ro4cyU9/+lNuvfXWtHxWc7Kzha4uFxHZjalTp/K9732PTZs28a9//YsnnniCHj16EA6HefPNN1mzZs0eb/OEE07gkUce4eSTT2b58uWsXbuWYcOGsWrVKgYNGsQ111zD2rVr+fjjjxk+fDhdu3blwgsvpHPnztxzzz2t8FPuKjsDPRBWl4uINOnwww+noqKCPn360Lt3by644AK+8Y1vMHLkSMaMGcPw4cP3eJs/+MEPuPLKKxk5ciShUIgHHniA3NxcnnjiCR566CHC4TC9evXi5ptv5sMPP+QnP/kJgUCAcDjMzJkzW+Gn3FX2XQ8d4I9HQP9xcPbd6S1KRPaJroeeXmm/HrqZ5ZnZB2a2wMwWm9ktjawzzczKzGy+//juXv8ELREMqw9dRKSBlnS51AInO+cqzSwMvGtmLzvnGt6Q73Hn3NXpL7ERGocuImm0cOFCLrroonrzcnNzmTNnToYq2jvNBrrz+mQq/Zdh/5GZfpo6QY1yEWmrnHN7NL67LRg5ciTz58/PdBn17E13eIuGLZpZ0MzmAxuBV51zjR22zjGzj81slpn1a2I7l5tZiZmVlJWV7XGxO6sOqYUu0gbl5eVRXl6+V2EkOznnKC8vJy8vb4/e16JRLs65BDDKzDoDz5jZCOfcopRVngcec87Vmtn3gQeBkxvZzt3A3eCdFN2jSlMF1Icu0hb17duX0tJS9qnBJoB3cOzbt+8evWePhi0657aa2ZvABGBRyvzylNXuAX67R1XsKZ0UFWmTwuEwAwcOzHQZB6yWjHLp7rfMMbN84FRgaYN1eqe8PBNYksYad6UuFxGRXbSkhd4beNDMgngHgCeccy+Y2a1AiXNuNnCNmZ0JxIHNwLTWKhjwv/pf2fx6IiIHkJaMcvkYGN3I/J+nTN8E3JTe0nZDXS4iIrvI3otz6av/IiL1ZGeg6+JcIiK7yM5A10lREZFdZGmghyGZyHQVIiJtSnYGejCkLhcRkQayM9B1cS4RkV1kaaCrhS4i0lB2BnpQfegiIg1lZ6BrlIuIyC6yN9DV5SIiUk92BnowDC4JyWSmKxERaTOyM9AD/iVodD0XEZEdsjPQg2HvWd0uIiI7ZGeg17XQdWJURGSHLA30uha6ulxEROpkZ6AH1YcuItJQdgZ6XQtdXS4iIjtkaaDXtdAV6CIidbIz0OtGueiuRSIiO2RnoGscuojILrIz0DUOXURkF80GupnlmdkHZrbAzBab2S2NrJNrZo+b2Uozm2NmA1ql2jo7xqGrhS4iUqclLfRa4GTn3JHAKGCCmY1rsM5lwBbn3CHAH4DfpLXKhnRSVERkF80GuvNU+i/D/sM1WG0y8KA/PQs4xcwsbVU2FNQXi0REGmpRH7qZBc1sPrAReNU5N6fBKn2AdQDOuTiwDShuZDuXm1mJmZWUlZXtQ9Uahy4i0lCLAt05l3DOjQL6Asea2Yi9+TDn3N3OuTHOuTHdu3ffm014NMpFRGQXezTKxTm3FXgTmNBg0XqgH4CZhYAioDwN9TUuqItziYg01JJRLt3NrLM/nQ+cCixtsNps4GJ/egrwhnOuYT97+ujiXCIiuwi1YJ3ewINmFsQ7ADzhnHvBzG4FSpxzs4F7gYfMbCWwGTi/1SoGjUMXEWlEs4HunPsYGN3I/J+nTEeAc9Nb2m5oHLqIyC6y85uiGocuIrKL7Ax0jUMXEdlFdga6xqGLiOwiSwM96D2rhS4iskN2BnpQLXQRkYayM9A1Dl1EZBfZGeg6KSoisovsDHTzy1aXi4jIDlka6OZ1u2gcuojIDtkZ6OB1u6jLRURkh+wN9EBIX/0XEUmR3YGuLhcRkR2yN9CDYZ0UFRFJkb2BHghDMpHpKkRE2owsDvSgulxERFJkb6Cry0VEpJ7sDXSNQxcRqSd7Az0YUh+6iEiK7A30QEhdLiIiKbI40NXlIiKSKnsDPRjWN0VFRFI0G+hm1s/M3jSzT8xssZld28g6XzOzbWY233/8vHXKTREI6VouIiIpQi1YJw7c4JybZ2Ydgblm9qpz7pMG673jnJuU/hKbEAhBsnK/fZyISFvXbAvdOfe5c26eP10BLAH6tHZhzdI4dBGRevaoD93MBgCjgTmNLD7OzBaY2ctmdngT77/czErMrKSsrGzPq02lLhcRkXpaHOhmVgg8BVznnNveYPE84GDn3JHA/wHPNrYN59zdzrkxzrkx3bt338uSfboeuohIPS0KdDML44X5I865pxsud85td85V+tMvAWEz65bWShvSOHQRkXpaMsrFgHuBJc6525tYp5e/HmZ2rL/d8nQWuouAWugiIqlaMsplPHARsNDM5vvzbgb6Azjn7gKmAFeaWRyoAc53zrn0l5siqBa6iEiqZgPdOfcuYM2s82fgz+kqqkXUQhcRqSd7vymqW9CJiNSTvYGur/6LiNSTvYGuFrqISD3ZG+j6pqiISD3ZG+iBEOAgmcx0JSIibUKWBzrqdhER8WVvoAfD3rO6XUREgGwO9IAf6Gqhi4gAWR3odV0uulG0iAhkc6AH/UBXl4uICJDNga4uFxGRerI40NVCFxFJlb2BXjfKRX3oIiJANge6xqGLiNSTvYGucegiIvVkb6CH8rznaFVm6xARaSOyN9C7DPCet6zOZBUiIm1G9gZ65/5gQdj8aaYrERFpE7I30INh6HIwlCvQRUQgmwMdoOtgtdBFRHzNBrqZ9TOzN83sEzNbbGbXNrKOmdkdZrbSzD42s6Nap9wGigdD+Spwbr98nIhIW9aSFnocuME5dxgwDrjKzA5rsM7pwBD/cTkwM61VNqX4EIhVQeWX++XjRETasmYD3Tn3uXNunj9dASwB+jRYbTLwN+d5H+hsZr3TXm1DXQd5z+pHFxHZsz50MxsAjAbmNFjUB1iX8rqUXUM//YoHe8/lK1v9o0RE2roWB7qZFQJPAdc557bvzYeZ2eVmVmJmJWVlZXuzifqK+kEwRydGRURoYaCbWRgvzB9xzj3dyCrrgX4pr/v68+pxzt3tnBvjnBvTvXv3vam3vkDQ+4KRulxERFo0ysWAe4Elzrnbm1htNvAdf7TLOGCbc+7zNNbZtK6DYfOq/fJRIiJtWagF64wHLgIWmtl8f97NQH8A59xdwEvARGAlUA1ckvZKm1I8GFa9CckkBLJ7WL2IyL5oNtCdc+8C1sw6DrgqXUXtka6DIB6Big1Q1DcjJYiItAXZ36TtNtR7/mJhZusQEcmw7A/0fsdCXhF88lymKxERyajsD/RQLhz6DVjyAsQima5GRCRjsj/QAQ4/G6IVsPK1TFciIpIxWRfoc1aVc/ad77E9knLruYEnQkExLHoqc4WJiGRY1gV6h9wQ89Zu5d53Pts5MxiCwybD8n/olnQicsDKukAf0aeICYf34r53P2NLVTRlwRSIVcPCJzNXnIhIBmVdoANcf+pQKqNx7n4n5RuiB38FDjoK3vlfiEebfrOISDuVlYE+rFdHvnHEQTzw3mo2bvdHtpjB126CrWthwaOZLVBEJAOyMtABfnTqUJLOMf3phbi6OxYNORX6jIG31UoXkQNP1gb6gG4duHHCcN5YupEnSvxLsde10rethQ//mtkCRUT2s6wNdIBpXxnAcYOKufX5T1hT7o9uOeQUGPJ1eONXXveLiMgBIqsDPRAwfnfuEYSCAa54eB410YTXSj/j994KL/5YN5AWkQNGVgc6QN8uBfzx/FEs/WI7Nz/j96d37g8n/xRWvAIfP57pEkVE9ousD3SAk4b14LpThvLMR+t56P013syx34f+x8GLN+iORiJyQGgXgQ7ww5MP4ZThPbj1+U+Yu2azd3u6c+6FYBievFgX7hKRdq/dBHogYNw+dRR9uuRz5cPzvPHpRX3grJnetdJfukH96SLSrrWbQAcoyg9z14VHUxGJc+mDH1JVG4dhp8NX/xs+ehjevzPTJYqItJp2FegAh/buxIwLRvPJhu1c/eg84omkNzb90DPhnz+FFa9mukQRkVbR7gId4OThPbntrJG8uayMnz23CGcG37wLeo6AWZfCxqWZLlFEJO3aZaADfHtsf646aTCPfbCOGW+uhJwO8K3HIJQHj02F6s2ZLlFEJK2aDXQzu8/MNprZoiaWf83MtpnZfP/x8/SXuXd+fNowvjm6D7//53Ke+HAdFPWF8x+F7Z/Do+dBbWWmSxQRSZuWtNAfACY0s847zrlR/uPWfS8rPcyM35xzBCcM6cb0pz9m9oIN0O8YmHIvrJ8Hf/+2hjOKSLvRbKA7594GsrZ/IicU4O6LxjBmQFeuf3w+/1z8hXdT6ckz4LN/wVOXQSKe6TJFRPZZuvrQjzOzBWb2spkd3tRKZna5mZWYWUlZWVmaPrp5+TlB7pt2DCP6FHH1ox/xr+VlMOpbcPpvYekLMPtqSCb3Wz0iIq0hHYE+DzjYOXck8H/As02t6Jy72zk3xjk3pnv37mn46JYrzA3xt0uOZXCPQr7/UAn/+bTcuzzAST+FBY95XzxSqItIFtvnQHfObXfOVfrTLwFhM+u2z5W1gqKCMA9ddix9uxRwyQMf8M6KMvjqj+H466HkPnj+GkgmMl2miMhe2edAN7NeZmb+9LH+Nsv3dbutpVthLn+/fBwDijtw2YMlvL50I5zyCzjxRvjoIXj2SvWpi0hWasmwxceA/wDDzKzUzC4zsyvM7Ap/lSnAIjNbANwBnO9c275oSl2oD+vZkSsenss/Fn8BJ93sXXL348fh6e9CIpbpMkVE9ohlKnvHjBnjSkpKMvLZdbZHYky77wMWlG7jf889krNG94H37oBXfwbDJ8GU+yGUk9EaRURSmdlc59yYxpa122+KtkSnvDB/u2wsxwzownWPz+eed1bB+Gtgwm+80S+PX6hx6iKSNQ7oQAdv9MsDlxzLxJG9uO3FJfzqxU9IHvt9mPQH745Hj54Hke2ZLlNEpFmhTBfQFuSFg/zft46iW+Fi/vrOZ5RV1PLbKdPICeXDc1fBAxPhglnQsVemSxURadIB30KvEwwYt5x5OD/5+jCenb+Bafd/wNah58C3n4DyVXDPqVC2PNNliog0SYGewsy46qRD+N9zj+TD1Zv55p3/ZlXncXDJixCvgftOg7VzMl2miEijFOiNOOfovjz6vXFsq4lx1oz3eK+6H1z2KuR3hQcnwUePZLpEEZFdKNCbcMyArjx31Xh6F+Xznfs+4MGlhrvsVeh/HDz3A3h5ur6AJCJtigJ9N/p1LeCpH3yFk4Z15xezF3P982upnvoEjLsK5syEh7+pG2WISJuhQG9GYW6Iuy8aw49PG8pzCzYw+c45rDzqZjjrLq8//S8nwroPM12miIgCvSUCAePqk4fw0KVjKa+KMvnP7zLbToRLXwYzuO/r8O4fdLVGEckoBfoeOH5IN1685niG9erINY99xI/eC1Ix7U3vhhmv/RIePhsqN2a6TBE5QCnQ91Dvonwe//5xXHPKEJ79aD2n/2UBHx5zO3zjT7D2P3DncbDkhUyXKSIHIAX6XggHA/zo1KE8ecVXCJgx9e73+U3ZOGovfR06HQSPXwBPXw41WzJdqogcQBTo++Dog7vw0rUnMOXovsx861MmPLqJ9095Ek6cDoue8lrry/+Z6TJF5AChQN9HhbkhfjvlSB6+bCyJpOP8e+dyY/kZVFz4D8jrDI+eC09cDNvWZ7pUEWnnFOhpcvyQbrxy3Ve54sTBzJpXytce2crfRz9M8qSfwvJX4M/HwLt/hHg006WKSDulQE+j/Jwg008fzvNXH8/g7oVMn72MiR+NpWTSP2DwSfDaL2DmV7yTpm37pk4ikoUU6K3gsIM68fj3x3HnBUdRFY0z5e/r+W7tdaw9/UFvhccvgHtPg9XvZbZQEWlXFOitxMyYOLI3r15/IjdOGM6czzbz1WfCXN3lTr448bewrdS7zvoj50JpZm/FJyLtwwF9T9H9aVt1jHvfXcV9762mKhrnrMO7cmPXf9Hr45kQ2QoDToATfgSDTvK+fSoi0ojd3VNUgb6fbamKcs+7q3jgvdVURRP816ACbur5PoNWPohVfA69R8FxV8FhkyGUm+lyRaSN2adAN7P7gEnARufciEaWG/AnYCJQDUxzzs1rrqgDNdDrbKuJ8dgHa7n/vc/4cnstI3rk8dN+Czj284cJbP4UCrrB0RfDmEuhqG+myxWRNmJfA/2rQCXwtyYCfSLwQ7xAHwv8yTk3trmiDvRArxONJ3l+wQbuefczlny+ncIc4/rBX3Bu8iU6rXnN63455FQY9S0YejqE8zJdsohk0D53uZjZAOCFJgL9L8BbzrnH/NfLgK855z7f3TYV6PU555i/biuPzFnLCx9vIBJLcmKPGq7r8i5HlP+DYOXn3heVRk6BEedAv7EQCGa6bBHZz3YX6KE0bL8PsC7ldak/b5dAN7PLgcsB+vfvn4aPbj/MjNH9uzC6fxd+dsZhPDt/Pc98tJ5vLjuVoJ3C9/qs5fzwOxz80cPYh/dAhx5w6CTvSo8DToBgONM/gohkWDpa6C8Av3bOveu/fh240Tm32+a3Wugts6qskmfnb+C5+etZU15NodVwaY8VTM6dy8At7xGIVXst92Gnw9AJMOhEyO+S6bJFpJW0dgt9PdAv5XVff56kwaDuhfzo1KFc/19DWPZlBa8s+pJ/ftKTO9YeQS4XcH7XlUwpmMehS14ktOAxsAAcdJT3zdTBJ0PfY9R6FzlApKOFfgZwNTtPit7hnDu2uW2qhb5vSrdU88/FX/LPT75g7potJBNxxoRWcV6XFYy3j+lZsQhzScgp9G5s3X+s1+/e52jI6ZDp8kVkL+3rKJfHgK8B3YAvgV8AYQDn3F3+sMU/AxPwhi1e0lx3CyjQ06mqNs4Hqzfz3opNvLtyE0u/qKATVZyUu4wzC5cyyi2muPozb2ULQq+R0H8c9BkDvY+A4kN0glUkS+iLRQeYjRUR/vNpOR98tpm5a7aw7MsKOrpKxgRXcGrHtYwNLqd/ZCmhRI33hlA+9DzcC/deI6HXEdDjMMgpyOwPIiK7UKAf4CoiMeav28rcNVuYu2YLC9ZtpToS4RDbwIjgGsZ32MDI4Fr6RVeSG68AwFkA6zoYug/zWvDdhkC3od50QdcM/0QiBy4FutTjnKN0Sw2L1m9j0YZtLFy/ncXrt1FeVUtf28RhtpqjckoZnbueAWygW7SUoIvv3EBBNy/giw+BLgdD5wH+88FQ2EPXohFpRa09ykWyjJnRr2sB/boWcPrI3oAX8mUVtazYWMnyLytYsbGS17+sYPmXlVTWROhrZQy2DQwPfcGIeBlDNm6g9+cvUhjbXH/joTzo3N8L9y4He/dY7XgQdOq98zm3YwZ+apH2T4EugBfyPTrl0aNTHuMP6bZjvnOOsspaVn5ZycqySlZvqubpzVWsLq9mbXk1gUQNfa2MflbGgOAmhoU2M7ByEwdVrqHbZ3PIT2zf9cNyOvoB3xsKe0KH7tChm//cvf5r9eOLtJgCXXbLzOjRMY8eHfP4SkrQAySSji+2R1izqYo1m6tZXV7Fv7dGmLW1hs+3RfiiJkI4GaGXbaaXbaEnmzk4vI2BbKdP1RZ6VJVRlFxJh/gWchLVjReQU9gg7P3pgmLIK/IfnVOmiyC3EwR0qX858CjQZa8FA0afzvn06ZzPVxpZnkg6NlZE2LA1woatNXy+rYYNWyO8vLWGjRW1bKqspayqltp4knwiFFsFxWyj2LbTPbCdfjlV9LFKetRW0i26jc6bV1AY/4D82BYCLrGbygzyOjUS+P5zfuf6B4CGB4ScDjoPIFlJgS6tJhgwehfl07son6MPbvxyBM45KmvjlFXUsqky6j/XUlZRS2llLR+lvN5UGSWaSGIk6Ug1nayGTlRRZFV0smp6hmvpkROhWyhCcbCazq6aoppqOlRXUpD8nLxEJeFYBaF41e4LD4S8Vn5uofcXQrjAC/nUR7huusBbJ6eDv16hPy91Hf+hsf7SyhToklFmRse8MB3zwgzqvvt1nXNU1MbZWhVjS3WULdVRtlbXTcfYWh1lWXWMOf6yLVXevKpo/dZ8kASd8A4CnaimOFRDz3CEHuEIxaEaugRq6ByopqPVUhCLkB+NkFe5jRz3BTmJCKFENaF4NYF4NcYejBIL5aWEfodGgn83B4e6A0s4z9tOKA/C+fWf9VfFAU+BLlnDzOiUF6ZTXpj+xS0/WVobT7CtOsb2SIxtNXG2R2Jsr4mxPRL3n2Nsr4mzKhJjvj+/oibG9ipvfjSRbGLLjjyiFFBLgUXoGKilOByjazhG55D3KArW0jEQpWOglsJArbcuEfKoJS9aQ26kknCijHCihmCimmCsGotV7dmBok4oz7vLVSjfD/6Gz00cCBouC+b628mDUM7O7abOdwmo2QLOeaOZ0nlBuGTSuy1jIgYuCTjvc3IK/PMj+kunKQp0afdyQ0F6dArSo9Pe3RwkGk9SVRunsjZOdTRBZW2cqto41dE4lbWJlGVxqmq95WW1cVZHvWVVtXGqIjuXReNNHSDqOHKJ0dEidMuN0zUUo2tOjC6hGJ1CcQpDCToFY3QIxugQSFBgMfIDUfItRh5RcqklnIwSdlFCyVqCiVqC0a0EE7VYIoLFvQexCMRr/NDcR6E8L/CDYQjmpDw3mA75z4GQF9Iu6T2SMajaBJUboarMO2A0Jaejd44kt5P3HAhBMg7JhPcczvdOmrukt81A0LvrV2FP7y+dYAjiUW9+flfvi3IFxd65lXAHiFZ6N26v2giHnul9gzpLKNBFmpETCpATyqFLh5y0bC+WSFJdm6AyGt8Z+LXNHyjWpi6r9N8XjROJ7VkgBwzywkHyw0HycgIUhpMUhRN0CsYpDCfpEEjQMRSnIBCnQyhBh0CcvECcfIt7B41AnHAwiMvvSm4ICmvWU1C7iRBxQi5OyMUIEifoYgSSMSwRg0TUC9HIdm86GfeuDGrmXV8oEPSGsfY+YudQ1mDYWwfz1otWQ2Qb1G73thPZ6k0754V43XZiNbB5lffegmIv6NfNgapyiDVz/qQeg7d/B10HeyfLAWorvO3XdY/VnWfBAL+O1C4y8A9aCf/A5R/ABp3oXfI6zRToIvtZOBigqCBAUUF6LmscTySp8v8aqAv8mmiCSDxBxH+uiSapiSWIpDy81/78aIKaeILN0QSRSLLeOnXrNa3p7pa6g0deOEheKEBeOEhuOEheOEB+yJ8fDpAX8ucHAuTFguRX+fPDQW9ZKEBepyB5xd52csPevNxQgJxQgNxQkNyw/zoYwJo6n5BMegeTYNgL+potUF0ONZuhZivEqr0Wf91VSRfOglVveQchnPeluXCBt160EmoroXodO7qF4jXegSdatfPgYUHv4BLwny3oHSAU6CLSUCgYoCg/QFF+61333jlHbTy5M+SjOw8GtSmhH4nVHUASO9aPpB44/OnauDe9PRLb+b5Yklr//bHEvl2SpF7QhwJ+2AdT5tc/COSGOpIbKtpxkMjdWEtuKEZu6DRyDpmw473hYMD/i81/BOs/1y2vWzcY2L8nqhXoItIsM9vR0u68Hz4vnkgSaXBAqJuujSeJxr2DQm08SW0sZXrHI+HP96ajqctiCSoicTbFo0RT3xfbOZ0uwYDVC/u6g8u3j+3Pd08YlLbPqaNAF5E2JxQMUBgMUJi7/yPKOUcs4eofJPywjyW8g0k0nqQ2kSQWTxJNmRdLeOtHE0licUc0kdixLFq3LJ6kW2Fuq9SuQBcRSWFm5ISMnFCAbLuMnC54ISLSTijQRUTaCQW6iEg7oUAXEWknFOgiIu2EAl1EpJ1QoIuItBMKdBGRdsKc27drJuz1B5uVAWv28u3dgE1pLKc1qMb0UI3poRr3XVup72DnXKO3g8lYoO8LMytxzo3JdB27oxrTQzWmh2rcd229PlCXi4hIu6FAFxFpJ7I10O/OdAEtoBrTQzWmh2rcd229vuzsQxcRkV1lawtdREQaUKCLiLQTWRfoZjbBzJaZ2Uozm57pegDMrJ+ZvWlmn5jZYjO71p/f1cxeNbMV/nPTd9PdP3UGzewjM3vBfz3QzOb4+/JxM0vPbe33vr7OZjbLzJaa2RIzO64N7sPr/X/jRWb2mJnlZXo/mtl9ZrbRzBalzGt0v5nnDr/Wj83sqAzW+Dv/3/pjM3vGzDqnLLvJr3GZmX09UzWmLLvBzJyZdfNfZ2Q/NierAt3MgsAM4HTgMOBbZnZYZqsCIA7c4Jw7DBgHXOXXNR143Tk3BHjdf51J1wJLUl7/BviDc+4QYAtwWUaq2ulPwD+cc8OBI/FqbTP70Mz6ANcAY5xzI4AgcD6Z348PABMazGtqv50ODPEflwMzM1jjq8AI59wRwHLgJgD/d+d84HD/PXf6v/uZqBEz6wecBqxNmZ2p/bh7zrmseQDHAa+kvL4JuCnTdTVS53PAqcAyoLc/rzewLIM19cX7xT4ZeAEwvG+9hRrbtxmorwj4DP9Efcr8trQP+wDrgK54t298Afh6W9iPwABgUXP7DfgL8K3G1tvfNTZY9k3gEX+63u818ApwXKZqBGbhNTBWA90yvR9398iqFjo7f6HqlPrz2gwzGwCMBuYAPZ1zn/uLvgB6Zqou4I/AfwN1tzQvBrY65+L+60zvy4FAGXC/3y10j5l1oA3tQ+fceuD3eC21z4FtwFza1n6s09R+a6u/Q5cCL/vTbaZGM5sMrHfOLWiwqM3UmCrbAr1NM7NC4CngOufc9tRlzjuMZ2SMqJlNAjY65+Zm4vNbKAQcBcx0zo0GqmjQvZLJfQjg90NPxjv4HAR0oJE/0duaTO+35pjZ/+B1Wz6S6VpSmVkBcDPw80zX0lLZFujrgX4pr/v68zLOzMJ4Yf6Ic+5pf/aXZtbbX94b2Jih8sYDZ5rZauDveN0ufwI6m1nIXyfT+7IUKHXOzfFfz8IL+LayDwH+C/jMOVfmnIsBT+Pt27a0H+s0td/a1O+QmU0DJgEX+AceaDs1DsY7eC/wf3f6AvPMrBdtp8Z6si3QPwSG+KMKcvBOnMzOcE2YmQH3Akucc7enLJoNXOxPX4zXt77fOeducs71dc4NwNtnbzjnLgDeBKZkuj4A59wXwDozG+bPOgX4hDayD31rgXFmVuD/m9fV2Gb2Y4qm9tts4Dv+KI1xwLaUrpn9yswm4HUDnumcq05ZNBs438xyzWwg3onHD/Z3fc65hc65Hs65Af7vTilwlP9/tc3sx3oy3Ym/FyctJuKdEf8U+J9M1+PXdDzen7QfA/P9x0S8furXgRXAa0DXNlDr14AX/OlBeL8oK4EngdwM1zYKKPH347NAl7a2D4FbgKXAIuAhIDfT+xF4DK9PP4YXOpc1td/wTobP8H9/FuKN2MlUjSvx+qHrfmfuSln/f/walwGnZ6rGBstXs/OkaEb2Y3MPffVfRKSdyLYuFxERaYICXUSknVCgi4i0Ewp0EZF2QoEuItJOKNBFRNoJBbqISDvx/3UukybCORooAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train[:150], label = 'train_loss')\n",
    "plt.plot(losses_val[:150], label = 'val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Gene Autoencoder Taining\n",
    "\n",
    "data_full = MyDataset(X_full_sc)\n",
    "full_loader = DataLoader(data_full, batch_size=50)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE_T(input_shape = len(X_full_sc[0])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/150, train_loss = 2.371810\n",
      "epoch : 2/150, train_loss = 2.278350\n",
      "epoch : 3/150, train_loss = 2.208492\n",
      "epoch : 4/150, train_loss = 2.145116\n",
      "epoch : 5/150, train_loss = 2.085704\n",
      "epoch : 6/150, train_loss = 2.029379\n",
      "epoch : 7/150, train_loss = 1.976472\n",
      "epoch : 8/150, train_loss = 1.926855\n",
      "epoch : 9/150, train_loss = 1.878786\n",
      "epoch : 10/150, train_loss = 1.833050\n",
      "epoch : 11/150, train_loss = 1.789027\n",
      "epoch : 12/150, train_loss = 1.747618\n",
      "epoch : 13/150, train_loss = 1.708049\n",
      "epoch : 14/150, train_loss = 1.670141\n",
      "epoch : 15/150, train_loss = 1.634714\n",
      "epoch : 16/150, train_loss = 1.600824\n",
      "epoch : 17/150, train_loss = 1.568573\n",
      "epoch : 18/150, train_loss = 1.538861\n",
      "epoch : 19/150, train_loss = 1.510168\n",
      "epoch : 20/150, train_loss = 1.482938\n",
      "epoch : 21/150, train_loss = 1.458832\n",
      "epoch : 22/150, train_loss = 1.435410\n",
      "epoch : 23/150, train_loss = 1.411999\n",
      "epoch : 24/150, train_loss = 1.390832\n",
      "epoch : 25/150, train_loss = 1.371119\n",
      "epoch : 26/150, train_loss = 1.353508\n",
      "epoch : 27/150, train_loss = 1.336036\n",
      "epoch : 28/150, train_loss = 1.319810\n",
      "epoch : 29/150, train_loss = 1.304592\n",
      "epoch : 30/150, train_loss = 1.290401\n",
      "epoch : 31/150, train_loss = 1.277153\n",
      "epoch : 32/150, train_loss = 1.264738\n",
      "epoch : 33/150, train_loss = 1.253081\n",
      "epoch : 34/150, train_loss = 1.242131\n",
      "epoch : 35/150, train_loss = 1.231823\n",
      "epoch : 36/150, train_loss = 1.222191\n",
      "epoch : 37/150, train_loss = 1.213084\n",
      "epoch : 38/150, train_loss = 1.204650\n",
      "epoch : 39/150, train_loss = 1.196487\n",
      "epoch : 40/150, train_loss = 1.188906\n",
      "epoch : 41/150, train_loss = 1.181854\n",
      "epoch : 42/150, train_loss = 1.174952\n",
      "epoch : 43/150, train_loss = 1.168507\n",
      "epoch : 44/150, train_loss = 1.162441\n",
      "epoch : 45/150, train_loss = 1.156673\n",
      "epoch : 46/150, train_loss = 1.151200\n",
      "epoch : 47/150, train_loss = 1.146000\n",
      "epoch : 48/150, train_loss = 1.141094\n",
      "epoch : 49/150, train_loss = 1.136326\n",
      "epoch : 50/150, train_loss = 1.131814\n",
      "epoch : 51/150, train_loss = 1.128227\n",
      "epoch : 52/150, train_loss = 1.123961\n",
      "epoch : 53/150, train_loss = 1.119952\n",
      "epoch : 54/150, train_loss = 1.115999\n",
      "epoch : 55/150, train_loss = 1.113063\n",
      "epoch : 56/150, train_loss = 1.109531\n",
      "epoch : 57/150, train_loss = 1.106015\n",
      "epoch : 58/150, train_loss = 1.102673\n",
      "epoch : 59/150, train_loss = 1.099513\n",
      "epoch : 60/150, train_loss = 1.096458\n",
      "epoch : 61/150, train_loss = 1.093461\n",
      "epoch : 62/150, train_loss = 1.090625\n",
      "epoch : 63/150, train_loss = 1.087794\n",
      "epoch : 64/150, train_loss = 1.085056\n",
      "epoch : 65/150, train_loss = 1.082394\n",
      "epoch : 66/150, train_loss = 1.079777\n",
      "epoch : 67/150, train_loss = 1.077231\n",
      "epoch : 68/150, train_loss = 1.074681\n",
      "epoch : 69/150, train_loss = 1.072166\n",
      "epoch : 70/150, train_loss = 1.069703\n",
      "epoch : 71/150, train_loss = 1.067292\n",
      "epoch : 72/150, train_loss = 1.064922\n",
      "epoch : 73/150, train_loss = 1.062577\n",
      "epoch : 74/150, train_loss = 1.060224\n",
      "epoch : 75/150, train_loss = 1.057896\n",
      "epoch : 76/150, train_loss = 1.055603\n",
      "epoch : 77/150, train_loss = 1.053334\n",
      "epoch : 78/150, train_loss = 1.051052\n",
      "epoch : 79/150, train_loss = 1.048789\n",
      "epoch : 80/150, train_loss = 1.046596\n",
      "epoch : 81/150, train_loss = 1.044392\n",
      "epoch : 82/150, train_loss = 1.042131\n",
      "epoch : 83/150, train_loss = 1.039913\n",
      "epoch : 84/150, train_loss = 1.037704\n",
      "epoch : 85/150, train_loss = 1.035512\n",
      "epoch : 86/150, train_loss = 1.033334\n",
      "epoch : 87/150, train_loss = 1.031216\n",
      "epoch : 88/150, train_loss = 1.029089\n",
      "epoch : 89/150, train_loss = 1.026903\n",
      "epoch : 90/150, train_loss = 1.024910\n",
      "epoch : 91/150, train_loss = 1.022919\n",
      "epoch : 92/150, train_loss = 1.020762\n",
      "epoch : 93/150, train_loss = 1.018569\n",
      "epoch : 94/150, train_loss = 1.016428\n",
      "epoch : 95/150, train_loss = 1.014314\n",
      "epoch : 96/150, train_loss = 1.012224\n",
      "epoch : 97/150, train_loss = 1.010303\n",
      "epoch : 98/150, train_loss = 1.008262\n",
      "epoch : 99/150, train_loss = 1.006153\n",
      "epoch : 100/150, train_loss = 1.004200\n",
      "epoch : 101/150, train_loss = 1.002260\n",
      "epoch : 102/150, train_loss = 1.000145\n",
      "epoch : 103/150, train_loss = 0.998125\n",
      "epoch : 104/150, train_loss = 0.996147\n",
      "epoch : 105/150, train_loss = 0.994167\n",
      "epoch : 106/150, train_loss = 0.992285\n",
      "epoch : 107/150, train_loss = 0.990398\n",
      "epoch : 108/150, train_loss = 0.988371\n",
      "epoch : 109/150, train_loss = 0.987805\n",
      "epoch : 110/150, train_loss = 0.985897\n",
      "epoch : 111/150, train_loss = 0.983933\n",
      "epoch : 112/150, train_loss = 0.982202\n",
      "epoch : 113/150, train_loss = 0.980193\n",
      "epoch : 114/150, train_loss = 0.978382\n",
      "epoch : 115/150, train_loss = 0.976507\n",
      "epoch : 116/150, train_loss = 0.974599\n",
      "epoch : 117/150, train_loss = 0.972771\n",
      "epoch : 118/150, train_loss = 0.971044\n",
      "epoch : 119/150, train_loss = 0.969298\n",
      "epoch : 120/150, train_loss = 0.967502\n",
      "epoch : 121/150, train_loss = 0.965805\n",
      "epoch : 122/150, train_loss = 0.964090\n",
      "epoch : 123/150, train_loss = 0.962411\n",
      "epoch : 124/150, train_loss = 0.960894\n",
      "epoch : 125/150, train_loss = 0.959309\n",
      "epoch : 126/150, train_loss = 0.957700\n",
      "epoch : 127/150, train_loss = 0.956071\n",
      "epoch : 128/150, train_loss = 0.954439\n",
      "epoch : 129/150, train_loss = 0.952788\n",
      "epoch : 130/150, train_loss = 0.951233\n",
      "epoch : 131/150, train_loss = 0.949928\n",
      "epoch : 132/150, train_loss = 0.948806\n",
      "epoch : 133/150, train_loss = 0.947249\n",
      "epoch : 134/150, train_loss = 0.945605\n",
      "epoch : 135/150, train_loss = 0.944255\n",
      "epoch : 136/150, train_loss = 0.942898\n",
      "epoch : 137/150, train_loss = 0.941307\n",
      "epoch : 138/150, train_loss = 0.939749\n",
      "epoch : 139/150, train_loss = 0.938300\n",
      "epoch : 140/150, train_loss = 0.937236\n",
      "epoch : 141/150, train_loss = 0.935834\n",
      "epoch : 142/150, train_loss = 0.934276\n",
      "epoch : 143/150, train_loss = 0.932873\n",
      "epoch : 144/150, train_loss = 0.931735\n",
      "epoch : 145/150, train_loss = 0.930408\n",
      "epoch : 146/150, train_loss = 0.929207\n",
      "epoch : 147/150, train_loss = 0.928142\n",
      "epoch : 148/150, train_loss = 0.926730\n",
      "epoch : 149/150, train_loss = 0.925264\n",
      "epoch : 150/150, train_loss = 0.923844\n"
     ]
    }
   ],
   "source": [
    "losses_train_final = []\n",
    "\n",
    "epochs=150\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_features in full_loader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        code, outputs = model(batch_features)\n",
    "        \n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    loss = loss / len(train_loader)\n",
    "    losses_train_final.append(loss)\n",
    "\n",
    "    print(\"epoch : {}/{}, train_loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(X_full_sc,dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out,out2 = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5faf8a12-a2aa-44f1-b099-02106766ec94</td>\n",
       "      <td>-0.723040</td>\n",
       "      <td>-0.727643</td>\n",
       "      <td>0.481543</td>\n",
       "      <td>0.097189</td>\n",
       "      <td>-0.044624</td>\n",
       "      <td>-0.156007</td>\n",
       "      <td>-0.154831</td>\n",
       "      <td>0.664276</td>\n",
       "      <td>-0.689005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183494</td>\n",
       "      <td>1.492558</td>\n",
       "      <td>-0.319100</td>\n",
       "      <td>-0.367510</td>\n",
       "      <td>0.127676</td>\n",
       "      <td>-0.419394</td>\n",
       "      <td>0.314201</td>\n",
       "      <td>0.629479</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.024343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6855a406-c085-45c7-b789-981786f0c775</td>\n",
       "      <td>-0.723040</td>\n",
       "      <td>-0.160989</td>\n",
       "      <td>0.200158</td>\n",
       "      <td>-1.077208</td>\n",
       "      <td>-0.199147</td>\n",
       "      <td>0.287531</td>\n",
       "      <td>-0.081362</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>-0.230718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660066</td>\n",
       "      <td>0.090072</td>\n",
       "      <td>-0.068541</td>\n",
       "      <td>-1.042671</td>\n",
       "      <td>0.230443</td>\n",
       "      <td>-0.795689</td>\n",
       "      <td>0.197465</td>\n",
       "      <td>-0.606612</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.478020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2445ad6-2a6a-4ec3-84d8-93cc3c180a58</td>\n",
       "      <td>0.718881</td>\n",
       "      <td>-0.624347</td>\n",
       "      <td>-0.336003</td>\n",
       "      <td>-2.066164</td>\n",
       "      <td>0.825467</td>\n",
       "      <td>0.951794</td>\n",
       "      <td>-0.547016</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029410</td>\n",
       "      <td>-0.583218</td>\n",
       "      <td>-1.108014</td>\n",
       "      <td>0.338714</td>\n",
       "      <td>0.632016</td>\n",
       "      <td>-0.585889</td>\n",
       "      <td>-0.267355</td>\n",
       "      <td>-0.606612</td>\n",
       "      <td>0.605120</td>\n",
       "      <td>0.032335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cded92df-9367-4ee5-930c-6c87bf2c8eb0</td>\n",
       "      <td>1.438180</td>\n",
       "      <td>-0.557001</td>\n",
       "      <td>-0.336003</td>\n",
       "      <td>-2.066164</td>\n",
       "      <td>2.620411</td>\n",
       "      <td>2.022248</td>\n",
       "      <td>-0.547016</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.599585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312819</td>\n",
       "      <td>-0.583218</td>\n",
       "      <td>-1.087689</td>\n",
       "      <td>0.193324</td>\n",
       "      <td>0.576623</td>\n",
       "      <td>0.221532</td>\n",
       "      <td>-0.435843</td>\n",
       "      <td>1.303435</td>\n",
       "      <td>1.688595</td>\n",
       "      <td>-0.274273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b167e70-e4e6-47f7-9fe9-11cf20f0d442</td>\n",
       "      <td>-0.521697</td>\n",
       "      <td>-0.107451</td>\n",
       "      <td>-0.336003</td>\n",
       "      <td>-1.552637</td>\n",
       "      <td>-0.199147</td>\n",
       "      <td>-0.156007</td>\n",
       "      <td>-0.442018</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.148820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095978</td>\n",
       "      <td>-0.095162</td>\n",
       "      <td>-0.765949</td>\n",
       "      <td>-0.027248</td>\n",
       "      <td>0.209361</td>\n",
       "      <td>-0.867946</td>\n",
       "      <td>-0.212317</td>\n",
       "      <td>-0.440243</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.348813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>1120d776-47df-4f72-862e-427d3c44dd43</td>\n",
       "      <td>0.761489</td>\n",
       "      <td>1.552966</td>\n",
       "      <td>0.894864</td>\n",
       "      <td>-2.066164</td>\n",
       "      <td>0.818666</td>\n",
       "      <td>0.833590</td>\n",
       "      <td>0.470497</td>\n",
       "      <td>1.263454</td>\n",
       "      <td>-1.091603</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533095</td>\n",
       "      <td>0.444737</td>\n",
       "      <td>-0.959472</td>\n",
       "      <td>-0.085191</td>\n",
       "      <td>0.179598</td>\n",
       "      <td>1.321581</td>\n",
       "      <td>2.732470</td>\n",
       "      <td>2.351571</td>\n",
       "      <td>2.095917</td>\n",
       "      <td>-0.348813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>f9cd1c16-3be3-415c-8d7b-9249c3b1c7fa</td>\n",
       "      <td>1.115692</td>\n",
       "      <td>-0.810143</td>\n",
       "      <td>-0.155077</td>\n",
       "      <td>-2.066164</td>\n",
       "      <td>0.275015</td>\n",
       "      <td>0.345515</td>\n",
       "      <td>-0.547016</td>\n",
       "      <td>0.249808</td>\n",
       "      <td>-1.091603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249223</td>\n",
       "      <td>-0.583218</td>\n",
       "      <td>-1.108014</td>\n",
       "      <td>-1.170032</td>\n",
       "      <td>0.391803</td>\n",
       "      <td>-0.889050</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>-0.606612</td>\n",
       "      <td>1.162846</td>\n",
       "      <td>2.433268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>218eb2f2-24b1-4e41-9071-26ed97a2edc1</td>\n",
       "      <td>-0.162710</td>\n",
       "      <td>1.692431</td>\n",
       "      <td>0.869310</td>\n",
       "      <td>0.065778</td>\n",
       "      <td>-0.199147</td>\n",
       "      <td>-0.156007</td>\n",
       "      <td>2.086387</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>-0.936617</td>\n",
       "      <td>...</td>\n",
       "      <td>1.477265</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.128918</td>\n",
       "      <td>-0.728673</td>\n",
       "      <td>2.263087</td>\n",
       "      <td>1.171138</td>\n",
       "      <td>-0.435843</td>\n",
       "      <td>-0.606612</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.100294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>978939fa-a27f-46ff-9120-7a43e3588b28</td>\n",
       "      <td>-0.723040</td>\n",
       "      <td>-1.070742</td>\n",
       "      <td>3.538204</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>2.680707</td>\n",
       "      <td>2.650743</td>\n",
       "      <td>1.702279</td>\n",
       "      <td>5.178845</td>\n",
       "      <td>3.526742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.591628</td>\n",
       "      <td>1.556053</td>\n",
       "      <td>-1.108014</td>\n",
       "      <td>3.250179</td>\n",
       "      <td>0.490337</td>\n",
       "      <td>-0.889050</td>\n",
       "      <td>3.118964</td>\n",
       "      <td>2.401170</td>\n",
       "      <td>1.250437</td>\n",
       "      <td>-0.348813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>a1b28726-ff2b-4a60-a48a-6e4ce62fa853</td>\n",
       "      <td>0.209637</td>\n",
       "      <td>-0.305205</td>\n",
       "      <td>-0.336003</td>\n",
       "      <td>-2.066164</td>\n",
       "      <td>0.745462</td>\n",
       "      <td>0.583011</td>\n",
       "      <td>-0.547016</td>\n",
       "      <td>0.142628</td>\n",
       "      <td>0.464460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057695</td>\n",
       "      <td>-0.162607</td>\n",
       "      <td>0.059107</td>\n",
       "      <td>-0.645353</td>\n",
       "      <td>0.127676</td>\n",
       "      <td>-0.498374</td>\n",
       "      <td>-0.318450</td>\n",
       "      <td>-0.606612</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.395244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file_name         0         1         2  \\\n",
       "0    5faf8a12-a2aa-44f1-b099-02106766ec94 -0.723040 -0.727643  0.481543   \n",
       "1    6855a406-c085-45c7-b789-981786f0c775 -0.723040 -0.160989  0.200158   \n",
       "2    e2445ad6-2a6a-4ec3-84d8-93cc3c180a58  0.718881 -0.624347 -0.336003   \n",
       "3    cded92df-9367-4ee5-930c-6c87bf2c8eb0  1.438180 -0.557001 -0.336003   \n",
       "4    4b167e70-e4e6-47f7-9fe9-11cf20f0d442 -0.521697 -0.107451 -0.336003   \n",
       "..                                    ...       ...       ...       ...   \n",
       "949  1120d776-47df-4f72-862e-427d3c44dd43  0.761489  1.552966  0.894864   \n",
       "950  f9cd1c16-3be3-415c-8d7b-9249c3b1c7fa  1.115692 -0.810143 -0.155077   \n",
       "951  218eb2f2-24b1-4e41-9071-26ed97a2edc1 -0.162710  1.692431  0.869310   \n",
       "952  978939fa-a27f-46ff-9120-7a43e3588b28 -0.723040 -1.070742  3.538204   \n",
       "953  a1b28726-ff2b-4a60-a48a-6e4ce62fa853  0.209637 -0.305205 -0.336003   \n",
       "\n",
       "            3         4         5         6         7         8  ...  \\\n",
       "0    0.097189 -0.044624 -0.156007 -0.154831  0.664276 -0.689005  ...   \n",
       "1   -1.077208 -0.199147  0.287531 -0.081362  0.142628 -0.230718  ...   \n",
       "2   -2.066164  0.825467  0.951794 -0.547016  0.142628  0.017885  ...   \n",
       "3   -2.066164  2.620411  2.022248 -0.547016  0.142628  0.599585  ...   \n",
       "4   -1.552637 -0.199147 -0.156007 -0.442018  0.142628  0.148820  ...   \n",
       "..        ...       ...       ...       ...       ...       ...  ...   \n",
       "949 -2.066164  0.818666  0.833590  0.470497  1.263454 -1.091603  ...   \n",
       "950 -2.066164  0.275015  0.345515 -0.547016  0.249808 -1.091603  ...   \n",
       "951  0.065778 -0.199147 -0.156007  2.086387  0.142628 -0.936617  ...   \n",
       "952 -0.009267  2.680707  2.650743  1.702279  5.178845  3.526742  ...   \n",
       "953 -2.066164  0.745462  0.583011 -0.547016  0.142628  0.464460  ...   \n",
       "\n",
       "          118       119       120       121       122       123       124  \\\n",
       "0   -0.183494  1.492558 -0.319100 -0.367510  0.127676 -0.419394  0.314201   \n",
       "1    0.660066  0.090072 -0.068541 -1.042671  0.230443 -0.795689  0.197465   \n",
       "2   -0.029410 -0.583218 -1.108014  0.338714  0.632016 -0.585889 -0.267355   \n",
       "3   -0.312819 -0.583218 -1.087689  0.193324  0.576623  0.221532 -0.435843   \n",
       "4    0.095978 -0.095162 -0.765949 -0.027248  0.209361 -0.867946 -0.212317   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "949  1.533095  0.444737 -0.959472 -0.085191  0.179598  1.321581  2.732470   \n",
       "950  0.249223 -0.583218 -1.108014 -1.170032  0.391803 -0.889050  0.003858   \n",
       "951  1.477265  0.032020  0.128918 -0.728673  2.263087  1.171138 -0.435843   \n",
       "952 -0.591628  1.556053 -1.108014  3.250179  0.490337 -0.889050  3.118964   \n",
       "953  0.057695 -0.162607  0.059107 -0.645353  0.127676 -0.498374 -0.318450   \n",
       "\n",
       "          125       126       127  \n",
       "0    0.629479 -0.005811  0.024343  \n",
       "1   -0.606612 -0.005811  0.478020  \n",
       "2   -0.606612  0.605120  0.032335  \n",
       "3    1.303435  1.688595 -0.274273  \n",
       "4   -0.440243 -0.005811 -0.348813  \n",
       "..        ...       ...       ...  \n",
       "949  2.351571  2.095917 -0.348813  \n",
       "950 -0.606612  1.162846  2.433268  \n",
       "951 -0.606612 -0.005811 -0.100294  \n",
       "952  2.401170  1.250437 -0.348813  \n",
       "953 -0.606612 -0.005811  0.395244  \n",
       "\n",
       "[954 rows x 129 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df = pd.DataFrame(out)\n",
    "latent_df.insert(0,'file_name',gene_df['file_name'])\n",
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"../../data/gene_df_128_tw.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ef9037651b8fb300183737a1adf54e758a8413bef4becc8f06877b013d9a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
